#+title: [WIP] 10x faster exact global alignment using A*PA-v2
#+HUGO_SECTION: posts
#+HUGO_TAGS: paper-draft
#+HUGO_LEVEL_OFFSET: 1
#+OPTIONS: ^:{}
#+hugo_front_matter_key_replace: author>authors
#+toc: headlines 3
#+date: <2023-08-08>
#+author: Ragnar Groot Koerkamp

\begin{equation*}
\newcommand{\g}{g^*}
\newcommand{\h}{h^*}
\newcommand{\cgap}{c_{\texttt{gap}}}
\end{equation*}


* Abstract

* Summary

This paper improves Edlib and A*PA by
- reducing overhead by using larger block sizes,
- avoiding recomputing states where possible,
- using SIMD with a novel bit-encoding,
- using a novel /optimistic/ diagonal-transition based traceback,
- applying and improving the A* heuristic of A*PA,
leading to a $10\times$ overall speedup on the alignment of $500$kbp ONT reads.

* Introduction

The problem of /global pairwise alignment/ is to find the shortest sequence of
edit operations (insertions, deletions, substitutions) to convert a string $A$
into a second string $B$ [cite:@nw;@vintsyuk68], where the number of such
operations is called the /Levenshtein distance/ or /edit distance/
[cite:@levenshtein;@wagner74].

Per tradition, as [cite/t:@rognes00] write:
#+begin_quote
The rapidly increasing amounts of genetic-sequence
information available represent a constant challenge to
developers of hardware and software database searching
and handling.
#+end_quote

In this work we introduce a new exact algorithm for global pairwise alignment
building on the recently introduced A* heuristics of A*PA [cite:@astarpa], but
instead of running A* we use a much more efficient DP-based algorithm. Thus, as
[cite/t:@fickett84] states,
#+begin_quote
at present one must choose between an algorithm which gives the best alignment
but is expensive, and an algorithm which is fast but may not give the best
alignment. In this paper we narrow the gap between these choices by showing how
to get the optimal alignment much more quickly than before.
#+end_quote

** Contributions

In A*PA2, we combine multiple existing techniques and introduce a number of new
ideas to obtain a $10\times$ speedup over existing single-threaded exact aligners.
As a starting point, we take the band doubling algorithm as efficiently
implemented by Edlib [cite:@edlib] that uses bitpacking [cite:@myers99].
First, we speed up the implementation without modifying the algorithm
(points 1., 2., and 3.), then, we reduce the amount of work done (points 4. and 5.),
and lastly we apply A*.

1. *Block-based computation.* Edlib computes one column of the DP matrix at a time,
   and for each column decides which range (subset of rows) of cells to compute.  We
   significantly reduce this overhead by processing blocks of $256$ columns at a
   time, taking inspiration from Block aligner [cite:@block-aligner].
   Correspondingly, we only store cells of the DP-matrix at block boundaries,
   reducing memory usage.
2. *SIMD.* We speed up the implementation using SIMD to compute each block, allowing
   the processing of $4$ computer words in parallel.
3. *Novel encoding.* We introduce a novel bit-encoding of the input sequence to
   speed up SIMD operations for size-$4$ alphabets.
4. *Incremental doubling.* We further note that both the original band doubling method of
   [cite/t:@ukkonen85] and Edlib recompute states in the doubled region. Reusing
   the theory behind the A* algorithm, we prove and apply a novel theorem stating that some of
   this recomputation can be avoided.
5. *Traceback.* For the traceback, we optimistically use the diagonal transition method within each
   block with a strong Z-drop [TODO] heuristic, falling back to a full recomputation of the block when needed.
6. *A** *.* We improve the A* seed heuristics of [cite/t:@astarpa] in two ways. First,
   instead of updating contours each time a match is pruned, we now do this in
   batches once the band is doubled. Secondly, we introduce a new /pre-pruning/ technique
   that discards most of the /spurious/ (off-path) matches ahead of time.

** Previous work

In the following, we give a brief recap of developments that this work builds
on, in chronological order per approach.  See also e.g. the reviews by
[cite/t:@kruskal83] and [cite/t:@navarro01].

*Needleman-Wunsch.* This problem has classically been approached as a dynamic
programming (DP) problem. For string lengths $n$ and $m$, [cite/t:@nw]
introduced the first $O(n^2m)$ algorithm.  This was improved to what is now
known as the $O(nm)$ /Needleman-Wunsch algorithm/ by [cite/t:@sellers] and
[cite/t:@wagner74], building on the quadratic algorithm for /longest common
subsequence/ by [cite/t:@sankoff].

*Graph algorithms.* It was already realized early on that this problem
corresponds to finding the shortest path from $v_s$ to $v_t$ in the /alignment
graph/ (also called /edit graph/ or /dependency graph/)
[cite:@vintsyuk68;@ukkonen85]. Both [cite/t:@ukkonen85] and [cite/t:@myers86]
remarked that this can be solved using Dijkstra's algorithm [cite:@dijkstra59],
taking $O(ns)$ time[fn::Although Ukkonen didn't realize this faster runtime and
only gave a bound of $O(nm \log (nm))$.], where $s$ is the edit distance between
the two strings.  However, [cite/t:@myers86] observes that
#+begin_quote
the resulting  algorithm involves a relatively complex discrete priority queue
and this queue  may contain as many as O(ND) entries even in the case where just
the length  of the [...] shortest edit script is being computed.
#+end_quote
[cite/t:@hadlock88detour] realized that Dijkstra's algorithm can be improved
upon by using A* [cite:@astar-hart67], a more /informed/ algorithm that uses a
/heuristic/ function $h(u)$ that gives a lower bound on the edit distance
$\h(u)$ between the suffixes following DP state $u$. He uses two heuristics, the widely
used /gap cost/ heuristic $h(u)=\cgap(u, v_t)$
[cite:@ukkonen85;@hadlock88detour;@wu90-O-np;@spouge89;@spouge91;@papamichail2009;]
that simply uses the difference between the lengths of the suffixes as lower
bound, and a new improved heuristic based on character frequencies in the two
suffixes. A*PA [cite:@astarpa] applies the /gap-chaining seed heuristic/ with /pruning/
[cite:@astarix-2] to obtain near-linear runtime when errors are uniform random.
Nevertheless, as [cite/t:@spouge91] states:
#+begin_quote
Many algorithms for finding optimal paths in non-lattice graphs also exist
[cite:@dijkstra59; @astar-hart67; @rubin74], but algorithms exploiting the
lattice structure of an alignment graph are usually faster. In molecular
biology, speed is important, ...
#+end_quote
and further [cite:@spouge89]:
#+begin_quote
This suggests a radical approach to A* search complexities: dispense with the
lists [of open states] if there is a natural order for vertex expansion.
#+end_quote
Indeed, a lot of work has gone into speeding up DP-based algorithms.

*Computational volumes.* [cite/t:@wilbur-lipman-83] is (to our knowledge) the
first paper that speeds up the $O(nm)$ DP algorithm, by only considering states
near diagonals with many /k-mer matches/, but at the cost of giving up the exactness
of the method.  [cite/t:@fickett84] notes that for $t\geq s$ only those DP-states with cost $\g(u)$ at
most $t$ need to be computed:
#+begin_quote
However it is possible to fill the matrix in many different orders, the only
restriction being that the calculation of any given $d_{ij}$ depends on already
having the values of the three element up and to the left of it.

[...]

But the only alignments of subsequences which are relevant are ones at least as
good (distance at least as small) as the overall one. I.e. one really only needs
those $d_{ij}$ which are below a fixed bound.
#+end_quote
This only requires $O(nt)$ time, which is fast when $t$ is an accurate bound on
the distance $s$, which for example can be set as a known upper bound for the
data being aligned, or as the length of a known suboptimal alignment.  When
$t=t_0$ turns out too small a larger new bound $t_1$ can be chosen, and only
states with distance in between $t_0$ and $t_1$ have to be computed.  This is
implemented by keeping for each row the index of the first and last state with
value at most $t_0$, and skipping over already computed states.  In the limit,
one could choose $t_i = i$ and compute states by increasing distance,
closely mirroring Dijkstra's algorithm.

[cite/t:@ukkonen85] introduces a very similar idea, statically bounding the
computation to only those states that can be on a path of length at most $t$
through the graph. When the sequences have the same length ($n=m$), this only
considers diagonals $-t/2$ to $t/2$, where diagonal $0$ is the main diagonal of
the DP-matrix.

On top of this, [cite/t:@ukkonen85] introduces /band doubling/: $t_0=1$ can be /doubled/ ($t_i
= 2^i$) until $t_k \geq s > t_{k-1}$. Since each test requires $O(n \cdot t_i)$ time, the
total time is
\begin{equation}
n\cdot t_0 + \dots + n\cdot t_k
= n\cdot (2^0 + \dots + 2^k)
< n\cdot 2^{k+1} = 4\cdot n\cdot 2^{k-1} < 4\cdot n\cdot s = O(ns).
\end{equation}
Note that this method does not (and indeed can not) reuse values from previous
iterations, resulting in roughly a factor $2$ overhead.

[cite/t:@spouge89] unifies the methods of
[cite/t:@fickett84] and [cite/t:@ukkonen85], and generalizes them to accept any
A* heuristic. In particular, a /computational volume/ is a subgraph of the
alignment graph that contains /every/ shortest path. Given a bound $t\geq s$, some examples of
computational volumes are:
1. $\{u\}$, the entire $(n+1)\times (m+1)$ graph.
2. $\{u: \g(u) + \h(u)=s\}$, the vertices on a shortest paths.
3. $\{u: \g(u)\leq t\}$, the states at distance $\leq t$ [cite:@fickett84].
4. $\{u: \cgap(v_s, u) + \cgap(u, v_t) \leq t\}$ the static set of states possibly on a path
   of length $\leq t$ [cite:@ukkonen85].
5. $\{u: \g(u) + \cgap(u, v_t) \leq t\}$, as used by Edlib [cite:@spouge91;@edlib].
6. $\{u: \g(u) + h(u) \leq t\}$, for any admissible heuristic $h$.

As [cite/t:@spouge89] notes:
#+begin_quote
The order of computation (row major, column major or antidiagonal) is just a
minor detail in most algorithms.
#+end_quote
But this is exactly what was investigated a lot in the search for faster implementations.

*Implementation and parallelism.* Since roughly $1995$, the focus shifted from
reducing the number of computed states to computing states faster through
advancements in implementation and hardware (SIMD, GPUs).  These speedups are
often applied to the Smith-Waterman-(Gotoh) [cite:@sw;@gotoh] algorithm for
(affine-cost) local alignment, where algorithmic improvements beyond
$\Theta(nm)$ are unknown.

The first technique in this direction is /microparallelism/ [cite:@alpern95],
where each (64 bit) computer word is divided into multiple (e.g. 16 bit) parts,
and word-size operations modifying all (4) parts in parallel.
[cite/t:@alpern95] applied this with /inter-sequence parallelism/ to align a
given query to $4$ reference sequences in parallel (see also
[cite/t:@rognes11]).  [cite/t:@hughey96] was the first to note that
/antidiagonals/ of the DP matrix can be computed in parallel, and
[cite/t:@wozniak97] applied SIMD for this purpose.

[cite/t:@rognes00] split 64bit words into 8 8-bit values, capping all
computations at $255$ but doubling the speed.  Further, it uses /vertical/
instead of antidiagonal vectors.
#+begin_quote
The advantage of this approach is the much-simplified and faster loading of the
vector of substitution scores from memory. The disadvantage is that data
dependencies within the vector must be handled.
#+end_quote
In particular, [cite/t:@rognes00] introduce the /query profile/: Instead of
looking up the substitution score $S[A[i]][B[j]]$ for the $i$'th and $j$'th
character of $A$ and $B$ respectively, it is more efficient to precompute the
/profile/ $P[c][j] := S[c][B[j]]$ for each character $c$ in the alphabet. Then,
adjacent scores are simply found as adjacent values $P[A[i]][j \dots j']$.

Similarly, [cite/t:@myers99] introduces a /bitpacking/ algorithm specifically
for edit distance that stores the differences between adjacent DP-states
bit-encoded in two 64-words $P$ and $M$, with $P_i$ and $M_i$ indicating whether
the $i$'th difference is $+1$ resp. $-1$.  It then gives an efficient algorithm
using bitwise operations on these words.

TODO
- [cite:@farrar] Farrar's striped; uses query profile; conditional prefix scan
  is moved outside inner loop. $2-8\times$ faster than Wozniak and Rognes.
- Wu Manber 1992
- Baeza-Yates Gonnet 1992
- Hyyro and Navarro, 2005; Hyyro et al., 2005
- Benson 2013
- navarro 2004
- bergeron hamel 2002

*Tools.*
There are multiple semi-global aligners that implement $O(nm)$ global
alignment using numerous of the aforementioned implementation
techniques, such as SeqAn [cite:@seqan], Parasail [cite:@parasail], Opal
(https://github.com/martinsos/opal), libssa
(https://github.com/RonnySoak/libssa), SWIPE [cite:@rognes11], SWPS3
[cite:@swps3], SSW library [cite:@ssw-library] ([[https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library][link]]), and KSW2 [cite:@minimap2].

Dedicated global alignment implementations are much rarer.
Edlib [cite:@edlib] implements the band doubling of [cite/t:@ukkonen85] using
the $\g(u)+\cgap(u, v_t)\leq t$ computational volume of [cite/t:@spouge91] and
the bitpacking of [cite/t:@myers99].
WFA and BiWFA [cite:@wfa;@biwfa] implement the $O(n+s^2)$ expected time /diagonal transition/
algorithm [cite:@ukkonen85;@myers86].
Block aligner [cite:@block-aligner] is an approximate aligner that can handle
position-specific scoring matrices whose main novelty is to divide the
computation into blocks.
Lastly, A*PA [cite:@astarpa] directly implements A* on the alignment graph using
the gap-chaining seed heuristic.


---

TODO:

- Opal: Šošic M. An simd dynamic programming c/c++ library: Thesis, University
  of Zagreb; 2015. https://bib.irb.hr/datoteka/758607.diplomski_Martin_
  Sosic.pdf.

- libssa: Frielingsdorf JT. Improving optimal sequence alignments through a
  simd-accelerated library: Thesis, University of Oslo; 2015. http://urn.nb.no/
  URN:NBN:no-49935. Accessed 10 Dec 2015.
- [cite:@suzuki-kasahara] libgaba: SIMD with difference recurrence relation for
  affine cost alignment
- [cite:@bitpal] BitPAl


* Methods
First, we explain in detail the algorithm and implementation used by Edlib and
reduce the overhead in the implementation by using blocks and SIMD.
Then, we improve the algorithm by avoiding recomputing states and
speeding up the traceback algorithm.
On top of that, we apply the A*PA heuristics for further speed gains on large/complex
alignments, at the cost of larger precomputation time to build the heuristic.

** Band-doubling and bitpacking in Edlib
As a baseline, we first outline the band-doubling method used by Edlib.

1. Start with edit-distance threshold $t=1$.
2. Iterate over columns $i$ from $0$ to $n$.
3. For each column, determine the range of rows $R=(r_{start}, r_{end})$ to compute by finding the top-
   and bottommost cell that can possibly have cost at most $t$, taking into
   account the gap-cost to the end.
   a. If the range is empty, double $t$ and go back to step 2.
   b. Otherwise, compute the range in blocks of $w=64$ rows at a time using
     bitpacking and the standard /profile/ of sequence $B$.

     Only the last and current column are kept in memory.
4. Once the last column has been reached, recursively repeat the algorithm using
   Hirschberg's /meet-in-the-middle/ approach to find the alignment.

** Blocks
Our first improvement is to process $B=256$ columns at a time. Instead of
computing the range of rows $R$ for each column individually, we compute it once and
then use this one range for a block of $B$ consecutive columns. While the extra
computed state in each column introduce some overhead, the time saved by not
computing $R$ for each column is larger.

See [[*Range-of-rows computation]] for details on the computation of the range of
rows.

** SIMD
While it is tempting to use a SIMD vector as a
single $w=256$-bit word, the four $64$-bit words are dependent on each other and
require manual work to shift bits between the lanes.
Instead, we let each $256$-bit AVX2 SIMD vector represent four $64$-bit words that are anti-diagonally
staggered (TODO FIG). This is similar to the original anti-diagonal tiling
introduced by [cite/t:@wozniak97], but using units of $64$-bit words instead of
single characters. This idea was already introduced in 2014 by the author of
Edlib[fn::See https://github.com/Martinsos/edlib/issues/5.], but to our
knowledge has never been implemented either in Edlib or elsewhere.

We achieve further speedup by improving instruction-level-parallelism.
Modern CPUs can execute up to 4 instructions per cycle (IPC) and use execution
pipelines that look ahead tens of instructions. The dependencies
between the instructions for computing each SIMD vector do not allow such high
parallelism. We improve this by processing two SIMD vectors in parallel, spanning a total of
eight antidiagonally-aligned $64$-bit words covering $512$ rows (TODO FIG).

** SIMD-friendly sequence profile
- New encoding scheme
- TODO: Tried BitPAl's bitpacking method which is one less than Myers 99's, but
  without success so far.
** Incremental doubling
- States with $f \leq f_{max}$ have /fixed/ values (assuming the heuristic is consistent).
- We can store deltas around the edge of this region and incrementally start the
  computation from there.
** Traceback
- Sparse memory
  - Param: sparsity, same as block size, but could in theory be a multiple.
- DT Trace
  - Param: x-drop
** A*
*** Pre-pruning
- Param: length of lookahead
*** Bulk-contours update
*** Range-of-rows computation
  - Sparse heuristic
* Results
Compare
- Edlib
- WFA
- A*PA
- A*PA-v2 without heuristics
- A*PA-v2 with heuristics
on
- synthetic data
- human ONT reads
- human ONT reads with genetic variation

Important:
- Find threshold where heuristics become worth the overhead
- Show benefit of each of the optimizations
- Show sensitivity to parameter tuning

* Acknowledgements

I am grateful to Daniel Liu for regular discussions, and suggesting additional
papers that have been added to the introduction.

#+print_bibliography:
