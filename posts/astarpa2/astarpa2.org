#+title: [WIP] A*PA2: 10x faster exact global alignment
#+HUGO_SECTION: posts
#+HUGO_TAGS: paper-draft
#+HUGO_LEVEL_OFFSET: 1
#+OPTIONS: ^:{} num:t
#+hugo_front_matter_key_replace: author>authors
#+toc: headlines 3
#+date: <2024-02-29>
#+author: Ragnar Groot Koerkamp

\begin{equation*}
\newcommand{\g}{g^*}
\newcommand{\h}{h^*}
\newcommand{\cgap}{c_{\texttt{gap}}}
\newcommand{\xor}{\ \mathrm{xor}\ }
\newcommand{\and}{\ \mathrm{and}\ }
\newcommand{\st}[2]{\langle #1, #2\rangle}
\newcommand{\matches}{\mathcal M}
\end{equation*}

Preface: This is my work-in-progress draft for my paper on version 2 of A*PA.
Code is done and works, but needs some cleanup and polishing. The API also needs
work.

* Abstract
:PROPERTIES:
:UNNUMBERED: t
:END:

*Methods.* We introduce A*PA2, an exact global pairwise aligner with respect to
edit distance. A*PA2 builds on Edlib and incorporates ideas from Block Aligner,
WFA, and A*PA. A*PA2 1) uses Myers' bitpacking with SIMD, 2) significantly
reduces overhead by using large block sizes, 3) avoids recomputation of states
where possible, 4) introduces a new technique for traceback, and 5) improves and
applies the heuristics developed in A*PA.

*Results.*
The average runtime of A*PA2 is $20\times$ faster than BiWFA and Edlib on $>500$kbp long ONT reads of a
human genome having $6\%$ divergence on average. On shorter ONT reads of
 $10\%-12\%$ average divergence the speedup is $5\times$ (length $11$kbp),
$2.5\times$ (length $4$kbp), and $0.66\times$ (length $800$bp).

*Availability.* [[https://github.com/RagnarGrootKoerkamp/astar-pairwise-aligner]]

*Contact.* [[https://twitter.com/curious_coding]]

* Introduction

The problem of /global pairwise alignment/ is to find the shortest sequence of
edit operations (insertions, deletions, substitutions) to convert a string $A$
into a second string $B$ [cite:@nw;@vintsyuk68], where the number of such
operations is called the /Levenshtein distance/ or /edit distance/
[cite:@levenshtein;@wagner74].

Over time, the length of genomic reads has increased from hundreds of basepairs
to hundreds of thousands basepairs now. Meanwhile, the complexity of exact
algorithms has not been improved by more than a constant factor since the
introduction of the O(s^2) diagonal transition algorithm [cite:@ukkonen85].

In this work we introduce a new exact algorithm for global pairwise alignment
building on the recently introduced A* heuristics of A*PA [cite:@astarpa], but
instead of running A* we use a much more efficient DP-based algorithm.
As [cite/t:@fickett84 p. 1] stated already 40 years ago,
#+begin_quote
at present one must choose between an algorithm which gives the best alignment
but is expensive, and an algorithm which is fast but may not give the best
alignment. In this paper we narrow the gap between these choices by showing how
to get the optimal alignment much more quickly than before.
#+end_quote

** Contributions

In A*PA2, we combine multiple existing techniques and introduce a number of new
ideas to obtain a $10\times$ speedup over existing single-threaded exact aligners.
As a starting point, we take the band doubling algorithm
implemented by Edlib [cite:@edlib] using bitpacking [cite:@myers99].
First, we speed up the implementation (points 1., 2., and 3.).
Then, we reduce the amount of work done (points 4. and 5.).
Lastly, we apply A*.

1. *Block-based computation.* Edlib computes one column of the DP matrix at a time,
   and for each column decides which range (subset of rows) of cells to compute.  We
   significantly reduce this overhead by processing blocks of $256$ columns at a
   time, taking inspiration from Block aligner [cite:@block-aligner].
   Correspondingly, we only store cells of the DP-matrix at block boundaries,
   reducing memory usage.
2. *SIMD.* We speed up the implementation using SIMD to compute each block, allowing
   the processing of $4$ computer words in parallel.
3. *Novel encoding.* We introduce a novel bit-encoding of the input sequence to
   speed up SIMD operations for size-$4$ alphabets.
4. *Incremental doubling.* Both the band doubling methods of [cite/t:@ukkonen85]
   and Edlib recompute states after doubling the threshold.  We avoid this by
   using the theory behind the A* algorithm, extending the incremental doubling
   of [cite/t:@fickett84] to blocks and arbitrary heuristics.
5. *Traceback.* For the traceback, we optimistically use the diagonal transition method within each
   block with a strong Z-drop [TODO] heuristic, falling back to a full recomputation of the block when needed.
6. *A** *.* We improve the A* seed heuristics of [cite/t:@astarpa] in two ways. First,
   instead of updating contours each time a match is pruned, we now do this in
   batches once the band is doubled. Secondly, we introduce a new /pre-pruning/ technique
   that discards most of the /spurious/ (off-path) matches ahead of time.

#+name: domains
#+caption: Alignment of two sequences of length 8000 (CHECK) with 20% errors introduced.
| [[file:imgs/intro/0_full.png]] Full grid (NW) | [[file:imgs/intro/0_gap-start.png]] gap heuristic                       | [[file:imgs/intro/0_gap-gap.png]] gap-gap (ukkonen)    |                                           |
|                                      | [[file:imgs/intro/0_g.png]] g*                                          | [[file:imgs/intro/0_g-gap.png]] g*+gap (edlib)         | [[file:imgs/intro/5_astarpa.png]] g*+h (A*PA)      |
|                                      | [[file:imgs/intro/2_dijkstra.png]] Dijkstra                             | [[file:imgs/intro/6_astarpa2_simple.png]] A*PA2-simple | [[file:imgs/intro/7_astarpa2_full.png]] A*PA2-full |
|                                      | [[file:imgs/intro/3_diagonal-transition.png]] Diagonal transition (WFA) |                                               |                                           |

** Previous work

In the following, we give a brief recap of developments that this work builds
on, in chronological order per approach.  See also e.g. the reviews by
[cite/t:@kruskal83] and [cite/t:@navarro01].

*** Needleman-Wunsch
This problem has classically been approached as a dynamic
programming (DP) problem. For string lengths $n$ and $m$, [cite/t:@nw]
introduced the first $O(n^2m)$ algorithm. [cite/t:@sellers] and
[cite/t:@wagner74] improved this to what is now
known as the $O(nm)$ /Needleman-Wunsch algorithm/, building on the quadratic algorithm for /longest common
subsequence/ by [cite/t:@sankoff].

*** Graph algorithms
It was already realized early on that this problem
corresponds to finding the shortest path from $v_s$ to $v_t$ in the /alignment
graph/, which is also called /edit graph/ or /dependency graph/
[cite:@vintsyuk68;@ukkonen85]. Both [cite/t:@ukkonen85] and [cite/t:@myers86]
remarked that this can be solved using Dijkstra's algorithm [cite:@dijkstra59],
taking $O(ns)$ time[fn::Although Ukkonen didn't realize this faster runtime and
only gave a bound of $O(nm \log (nm))$.], where $s$ is the edit distance between
the two strings.  However, [cite/t:@myers86 p. 2] observes that
#+begin_quote
the resulting  algorithm involves a relatively complex discrete priority queue
and this queue  may contain as many as O(ND) entries even in the case where just
the length  of the [...] shortest edit script is being computed.
#+end_quote
[cite/t:@hadlock88detour] realized that Dijkstra's algorithm can be improved
upon by using A* [cite:@astar-hart67], a more /informed/ algorithm that uses a
/heuristic/ function $h(u)$ that gives a lower bound on the edit distance
$\h(u)$ between the suffixes following DP state $u$. He uses two heuristics, the widely
used /gap cost/ heuristic $h(u)=\cgap(u, v_t)$
[cite:@ukkonen85;@hadlock88detour;@wu90-O-np;@spouge89;@spouge91;@papamichail2009;]
that simply uses the difference between the lengths of the suffixes as lower
bound, and a new improved heuristic based on character frequencies in the two
suffixes. A*PA [cite:@astarpa] applies the /gap-chaining seed heuristic/ with /pruning/
[cite:@astarix-2] to obtain near-linear runtime when errors are uniform random.
Nevertheless, as [cite/t:@spouge91 p. 3] states:
#+begin_quote
Many algorithms for finding optimal paths in non-lattice graphs also exist
[cite:@dijkstra59; @astar-hart67; @rubin74], but algorithms exploiting the
lattice structure of an alignment graph are usually faster. In molecular
biology, speed is important, ...
#+end_quote
and further [cite:@spouge89 p. 4]:
#+begin_quote
This suggests a radical approach to A* search complexities: dispense with the
lists [of open states] if there is a natural order for vertex expansion.
#+end_quote
In this work we follow this advice and replace the A* search in A*PA with a much
more efficient DP based approach based on /computational volumes/.

*** Computational volumes
[cite/t:@wilbur-lipman-83] is, to our knowledge, the first paper that speeds up
the $O(nm)$ DP algorithm, by only considering states near diagonals with many
/k-mer matches/, but at the cost of giving up the exactness of the method.
[cite/t:@fickett84 p. 177] notes that for $t\geq s$ only those DP-states with
cost $\g(u)$ at most $t$ need to be computed:
#+begin_quote
However it is possible to fill the matrix in many different orders, the only
restriction being that the calculation of any given $d_{ij}$ depends on already
having the values of the three elements up and to the left of it.

[...]

But the only alignments of subsequences which are relevant are ones at least as
good (distance at least as small) as the overall one. I.e. one really only needs
those $d_{ij}$ which are below a fixed bound.
#+end_quote
This only requires $O(nt)$ time, which is fast when $t$ is an accurate bound on
the distance $s$. For example $t$ can be set as an upper bound for the
data being aligned, or as the length of a suboptimal alignment.  When
$t=t_0$ turns out too small, a larger new bound $t_1$ can be chosen, and only
states with distance in between $t_0$ and $t_1$ have to be computed.  This is
implemented by keeping for each row the index of the first and last state with
value at most $t_0$, and skipping over already computed states.  In the limit
where $t$ increases by $1$ in each iteration, this closely mirrors Dijkstra's algorithm.

[cite/t:@ukkonen85] introduces a very similar idea, statically bounding the
computation to only those states that can be on a path of length at most $t$
through the graph. When the sequences have the same length ($n=m$), this only
considers diagonals $-t/2$ to $t/2$, where diagonal $0$ is the main diagonal of
the DP-matrix.

On top of this, [cite/t:@ukkonen85] introduces /band doubling/: $t_0=1$ can be
/doubled/ ($t_i = 2^i$) until $t_k$ is at least the actual distance $s$, i.e.
$t_k \geq s > t_{k-1}$. Since each test requires $O(n \cdot t_i)$ time, the
total time is
\begin{equation}
n\cdot t_0 + \dots + n\cdot t_k
= n\cdot (2^0 + \dots + 2^k)
< n\cdot 2^{k+1} = 4\cdot n\cdot 2^{k-1} < 4\cdot n\cdot s = O(ns).
\end{equation}
Note that this method does not (and indeed can not) reuse values from previous
iterations, resulting in roughly a factor $2$ overhead.

[cite/t:@spouge89] unifies the methods of
[cite/t:@fickett84] and [cite/t:@ukkonen85], and generalizes them to accept any
A* heuristic. In particular, Spouge defines a /computational volume/ as a subgraph of the
alignment graph that contains /every/ shortest path. Given a bound $t\geq s$, some examples of
computational volumes are:
1. $\{u\}$, the entire $(n+1)\times (m+1)$ graph.
2. $\{u: \g(u) + \h(u)=s\}$, the vertices on a shortest paths.
3. $\{u: \g(u)\leq t\}$, the states at distance $\leq t$ [cite:@fickett84].
4. $\{u: \cgap(v_s, u) + \cgap(u, v_t) \leq t\}$ the static set of states possibly on a path
   of length $\leq t$ [cite:@ukkonen85].
5. $\{u: \g(u) + \cgap(u, v_t) \leq t\}$, as used by Edlib [cite:@spouge91;@edlib].
6. $\{u: \g(u) + h(u) \leq t\}$, for any admissible heuristic $h$.

As [cite/t:@spouge89 p. 1559] notes:
#+begin_quote
The order of computation (row major, column major or antidiagonal) is just a
minor detail in most algorithms.
#+end_quote
But this is exactly what was investigated a lot in the search for faster implementations.

*** Parallelism
In the 1990s, the focus shifted from
reducing the number of computed states to computing states faster through
advancements in implementation and hardware (SIMD, GPUs).  These speedups are
often applied to the Smith-Waterman-(Gotoh) [cite:@sw;@gotoh] algorithm for
(affine-cost) local alignment, where algorithmic improvements beyond
$\Theta(nm)$ are unknown.

The first technique in this direction is /microparallelism/ [cite:@alpern95],
where each (64 bit) computer word is divided into multiple (e.g. 16 bit) parts,
and word-size operations modify all (4) parts in parallel.
[cite/t:@alpern95] applied this with /inter-sequence parallelism/ to align a
given query to four reference sequences in parallel, see also
[cite/t:@rognes11].  [cite/t:@hughey96] was the first to note that
/anti-diagonals/ of the DP matrix are independent and can be computed in parallel, and
[cite/t:@wozniak97] applied SIMD for this purpose.

[cite/t:@rognes00 p. 702] also use microparallelism, but use /vertical/
instead of anti-diagonal vectors:
#+begin_quote
The advantage of this approach is the much-simplified and faster loading of the
vector of substitution scores from memory. The disadvantage is that data
dependencies within the vector must be handled.
#+end_quote
In particular, [cite/t:@rognes00] introduce the /query profile/: Instead of
looking up the substitution score $S[A_i][B_j]$ for the $i$'th and $j$'th
character of $A$ and $B$ respectively, it is more efficient to precompute the
/profile/ $Eq[c][j] := S[c][B_j]$ for each character $c$ in the alphabet. Then,
adjacent scores are simply found as adjacent values $Eq[A_i][j \dots j']$.

Similarly, [cite/t:@myers99] introduces a /bitpacking/ algorithm specifically
for edit distance. This method stores the differences between adjacent DP-states
bit-encoded in two $w{=}64$-bit words $P$ and $M$, with $P_i$ and $M_i$ indicating whether
the $i$'th difference is $+1$ resp. $-1$. It then gives an efficient algorithm
using bit operations on these words ([[myers]]).

---

TODO
- [cite:@farrar] Farrar's striped; uses query profile; conditional prefix scan
  is moved outside inner loop. $2-8\times$ faster than Wozniak and Rognes.
- Wu Manber 1992
- Baeza-Yates Gonnet 1992
- Hyyro and Navarro, 2005; Hyyro et al., 2005
- Benson 2013
- navarro 2004
- bergeron hamel 2002

*** Tools
There are multiple semi-global aligners that implement $O(nm)$ global
alignment using numerous of the aforementioned implementation
techniques, such as SeqAn [cite:@seqan], Parasail [cite:@parasail], Opal
(https://github.com/martinsos/opal), libssa
(https://github.com/RonnySoak/libssa), SWIPE [cite:@rognes11], SWPS3
[cite:@swps3], SSW library [cite:@ssw-library] ([[https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library][link]]), and KSW2 [cite:@minimap2].

Dedicated global alignment implementations are much rarer.
Edlib [cite:@edlib] implements the band doubling of [cite/t:@ukkonen85] using
the $\g(u)+\cgap(u, v_t)\leq t$ computational volume of [cite/t:@spouge91] and
the bitpacking of [cite/t:@myers99].
WFA and BiWFA [cite:@wfa;@biwfa] implement the $O(n+s^2)$ expected time /diagonal transition/
algorithm [cite:@ukkonen85;@myers86].
Block aligner [cite:@block-aligner] is an approximate aligner that can handle
position-specific scoring matrices whose main novelty is to divide the
computation into blocks.
Lastly, A*PA [cite:@astarpa] directly implements A* on the alignment graph using
the gap-chaining seed heuristic.

---

- Opal: Šošic M. An simd dynamic programming c/c++ library: Thesis, University
  of Zagreb; 2015. https://bib.irb.hr/datoteka/758607.diplomski_Martin_
  Sosic.pdf.

- libssa: Frielingsdorf JT. Improving optimal sequence alignments through a
  simd-accelerated library: Thesis, University of Oslo; 2015. http://urn.nb.no/
  URN:NBN:no-49935. Accessed 10 Dec 2015.
- [cite:@suzuki-kasahara] libgaba: SIMD with difference recurrence relation for
  affine cost alignment
- [cite:@bitpal] BitPAl


* Methods
First, we explain in detail the algorithm and implementation used by Edlib and
reduce the overhead in the implementation by using blocks and SIMD.
Then, we improve the algorithm by avoiding recomputing states and
speeding up the traceback algorithm.
On top of that, we apply the A*PA heuristics for further speed gains on large/complex
alignments, at the cost of larger precomputation time to build the heuristic.

** Band-doubling and bitpacking in Edlib
As a baseline, we first outline the band-doubling method used by Edlib.

1. Start with edit-distance threshold $t=1$.
2. Iterate over columns $i$ from $0$ to $n$.
3. For each column, determine the range of rows $R=(r_{start}, r_{end})$ to compute by finding the top-
   and bottommost state that can possibly have cost at most $t$, taking into
   account the gap-cost to the end. Both $r_{start}$ and $r_{end}$ are rounded
   /out/ to the previous/next multiple of $w$.
   a. If the range is empty, double $t$ and go back to step 2.
   b. Otherwise, compute the range in blocks of $w=64$ rows at a time using
     bitpacking and the standard /profile/ of sequence $B$.

     Only the last and current column are kept in memory.
4. *Traceback.* Once the last column has been reached, recursively repeat the algorithm using
   Hirschberg's /meet-in-the-middle/ approach to find the alignment. Continue
   until the sequences are of length $\leq 1000$. For these small sequences all
   vertical differences can be stored and a backtrace is done to find the alignment.


TODO: Talk about exact details of growth factor and start growth.

#+name: blocks
#+caption: introducing blocks
| [[file:imgs/intro/0_g-gap.png]] g-gap | [[file:imgs/intro/0_bitpacking.png]] myers bitpacking (Edlib) | [[file:imgs/intro/0_blocks.png]] Blocks (Block aligner) |

** Bitpacking

#+name: myers
#+caption: Rust code for SIMD version of Myers' bitpacking. Computes four antidiagonal words in parallel.
#+begin_src rust
pub fn compute_block_simd(
    hp0: &mut Simd<u64, 4>,  // 0 or 1. Indicates +1 difference on top.
    hm0: &mut Simd<u64, 4>,  // 0 or 1. Indicates -1 difference on top.
    vp: &mut Simd<u64, 4>,  // 64-bit indicator of +1 differences on left.
    vm: &mut Simd<u64, 4>,  // 64-bit indicator of -1 differences on left.
    eq: Simd<u64, 4>,  // 64-bit indicator which characters equal the top char.
) {
    let vx = eq | *vm;
    let eq = eq | *hm0;
    // The add here is the only operation that carries information between rows.
    let hx = (((eq & *vp) + *vp) ^ *vp) | eq;
    let hp = *vm | !(hx | *vp);
    let hm = *vp & hx;
    // Extract the high bit as bottom horizontal difference.
    let right_shift = Simd<u64,4>::splat(63);   // Shift each lane by 63.
    let hpw = hp >> right_shift;
    let hmw = hm >> right_shift;
    // Insert the top horizontal difference.
    let left_shift = Simd<u64,4>::splat(1);     // Shift each lane by 1.
    let hp = (hp << left_shift) | *hp0;
    let hm = (hm << left_shift) | *hm0;
    // Update the input-output parameters.
    *hp0 = hpw;
    *hm0 = hmw;
    *vp = hm | !(vx | hp);
    *vm = hp & vx;
}
#+end_src


** Blocks
Our first improvement is to process $B=256$ columns at a time. Instead of
computing the range of rows $R$ for each column individually, we compute it once and
then use this one range for a block of $B$ consecutive columns. While this
computes some extra states in most columns, the time saved by not
having to compute $R$ for each column is larger.

Within each block, we iterate over the rows in /lanes/ of $w$ rows at a time, and for each
lane compute all $B$ columns before moving on to the next lane.

See [[*Computed range]] for details on the computation of $R$.

** Memory

Where Edlib does not initially store intermediate values and uses
meet-in-the-middle to find the alignment, we /always/ store an offset and vertical differences
at the end of each block[fn::Even sparser memory usage is possible by only
storing vertical differences every $B'$ columns for $B'$ a multiple of $B$, but in
practice memory is not a bottleneck.]. This simplifies the implementation, and has sufficiently small
memory usage to be practical. See [[*Traceback]] for details on recovering the
alignment.

** SIMD

#+name: simd
#+caption: SIMD+ILP processing of 8 lanes in parallel. The example uses 4-bit (instead of 64-bit) lanes.
[[file:imgs/simd.png]]

While it is tempting to use a SIMD vector as a single $W=256$-bit word, the four
$w=64$-bit words (SIMD lanes) are dependent on each other and require manual
work to shift bits between the lanes.
Instead, we let each $256$-bit AVX2 SIMD vector represent four $64$-bit words
(lanes) that are anti-diagonally
staggered ([[simd]]). This is similar to the original anti-diagonal tiling
introduced by [cite/t:@wozniak97], but using units of $w$-bit words instead of
single characters. This idea was already introduced in 2014 by the author of
Edlib[fn::See https://github.com/Martinsos/edlib/issues/5.], but to our
knowledge has never been implemented either in Edlib or elsewhere.

We achieve further speedup by improving instruction-level-parallelism.
Modern CPUs can execute up to 4 instructions per cycle (IPC) and use execution
pipelines that look ahead tens of instructions. The dependencies
between the instructions for computing each SIMD vector do not allow such high
parallelism. We improve this by processing two SIMD vectors in parallel, spanning a total of
$8$ anti-diagonally-aligned $64$-bit lanes covering $2W = 512$ rows ([[simd]]).

When the number of lanes of rows to be computed is $c=(r_{end}-r_{start})/64$, we
process $8$ lanes in parallel as long as $c\geq 8$. If there are remaining
rows, we end with another $8$-lane ($5\leq c<8$) or $4$-lane ($1\leq c\leq 4$)
iteration that optionally includes some padding rows at the bottom.
In case the horizontal differences along the original bottom row are needed (as
is the case for incremental doubling [[*Incremental doubling]]), we
do not use padding and instead fall back to trying a $4$-lane SIMD ($c\geq 4$),
a $2$-lane SIMD ($c\geq 2$), and lastly a scalar iteration ($c\geq 1$).

TODO: How about padding upwards?

** SIMD-friendly sequence profile

#+name: profile
#+caption: equality check with the new profile
#+begin_src rust
/// `pa`: Exploded bit-encoding of single char `c` of `a`.
/// c=0:  (00...00, 00...00)
/// c=1:  (11...11, 00...00)
/// c=2:  (00...00, 11...11)
/// c=3:  (11...11, 11...11)
/// 64-char packed *negated* bit-encoding of 64 chars of `b`.
/// bi=0: (...1..., ...1...)
/// bi=1: (...0..., ...1...)
/// bi=2: (...1..., ...0...)
/// bi=3: (...0..., ...0...)
///
/// Returns a mask which chars of `b` equal the char of `a`.
fn eq(pa: &(u64, u64), pb: &(u64, u64)) -> u64 {
    (pa.0 ^ pb.0) & (pa.1 ^ pb.1)
}
#+end_src

Myers' bitpacking algorithm precomputes a /profile/ $P_{eq}[c][j]$ containing
$\sigma \times m$ bits. For each character $c$, it contains a bitvector of
$w$-bit words indicating the positions where $c$ occurs in $B$. We improve
memory locality by instead storing the profile as an array of blocks of $\sigma$
words: $P_{eq}[j/w][c]$ containing $\lceil m/w\rceil \times \sigma$ $w$-bit
words (FIG?).

A drawback of anti-diagonal tiling is that each column contains its own
character $a_i$ that needs to be looked up. While SIMD offers =gather=
instructions to do multiple of these lookups in parallel, these instructions are
not always efficient. Thus, we introduce the following alternative scheme.

Let $b = \lceil \log_2(\sigma)\rceil$ be the number of bits needed to encode
each character, with $b=2$ for DNA.
The new profile $P'$ contains $b$ bitvectors, each indicating the negation of one bit of each
character, stored as an $\lceil m/w\rceil \times b$ array $P'[j/w][p]$ of
$w$-bit words.

To check whether row $j$ contains character $c$ with bit representation
$\overline{c_{b-1}\dots c_{0}}$, we compute
$$(c_0 \xor P'[j/w][0][j\bmod w]) \and \dots \and (c_{b-1} \xor P'[j/w][b-1][j\bmod w]).$$
This naturally extends to an efficient computation for $w$-bit words and larger
SIMD vectors.

TODO Remark only alphabet size 4 works

TODO: Tried BitPAl's bitpacking method which is one less than Myers 99's, but
without success so far.

TODO: No specific benchmark.

** Traceback

#+name: trace
#+caption: traceback shown in blue, both DT trace states and compute blocks as fallback.
[[file:imgs/trace/trace.png]]

The traceback stage takes as input the computed vertical differences at
the end of each block of columns. We iteratively work backwards through the
blocks of columns. In each step, we are given the distances $D_i[j]$ to
the states in column $i$ ($B|i$) and the state $u=\st{i+B}j$ in column $i+B$
that is on the optimal path and has distance $d_u$.
The goal is to find an optimal path from column $i$ to $u$.

A naive approach is to simply recompute the entire block of columns for their
entire range $R$ while storing distances to all cells, but we introduce to
faster methods.


*** Optimistic block computation
Instead of computing the full range $R=(r_{start}, r_{end})$ for this column, a
first insight is that only rows up to $j$ are needed, since the optimal path to
$u=\st{i+B}j$ can never go below row $j$.

Secondly, the path crosses $B=256$ rows, and so we optimistically assume that it
will be contained in rows $j-256-64=j-320$ to $j$. Thus, we first compute the
states in this range of rows (rounded out to multiples of $w$). If the distance
to $u$ computed this way agrees with the known distance, the path must lie
within these rows. Otherwise, we repeatedly try again with double the number of lanes, until
success. The exponential search ensures low overhead and good average case performance.
*** Optimistic diagonal transition
A further improvement uses the /diagonal transition/ algorithm backwards from
$u$. We simply run the unmodified algorithm on the reverse graph covering
columns $i$ to $i+B$ and rows $0$ to $j$. When the distance $d_j$ from $u$ to a
state $\st ij$ in column $i$ is found, we check whether $D_i[j] + d_j = d_u$.
If this is not the case, we continue until a suitable $j$ is found.  We then
infer the optimal path by a traceback on the diagonal transition algorithm.
When no path has been found of distance $\leq 40$, we fall back to the block
doubling described above.

Another optimization is the WF-adaptive heuristic introduced by WFA: all states
that lag more than $10$ behind the furthest reaching diagonal are dropped.

TODO: Stop halfway

** Incremental doubling

TODO: Rephrase $g(u)\leq t$ to $f(u) \leq t$.

TODO: The range-end only matters for the last columns of the block. Intermediate
columns that go further down can be disregarded.

#+name: doubling
#+caption: Doubling detail
| [[file:imgs/doubling-0.png]] | [[file:imgs/doubling-1.png]] |

#+name: doubling-large
#+caption: Due to incremental doubling, states 'in the middle' do not have to be recomputed.
[[file:imgs/doubling-large/doubling-large.png]]

The original band doubling algorithm doubles the threshold from $t$ to $t'=2t$
in each iteration and simply recomputes the distance to all states.  On the
other hand, BFS, Dijkstra, and A*[fn::A* with a /consistent/ heuristic.] visit
states in increasing order of distance ($g(u)$ for BFS and Dijkstra, $f(u) =
g(u) + h(u)$ for A*), and the distance to a state is known to be correct
(/fixed/) as soon as it is expanded. This way a state is never expanded twice.

Indeed, the band-doubling algorithm can also avoid recomputations. After
completing the iteration for $t$, it is guaranteed that the distance is fixed
for all states that are indeed at distance $\leq t$.  In fact a stronger result holds:
in any column the distance is fixed for /all/ states between the topmost
and bottommost state with distance $\leq t$.
Note that due to the
word-based computations, there will also be states whose computed distance is
$>t$. These are /not/ guaranteed to be correct.

After a range $R=(r_{start}, r_{end})$ of rows for a block of $B$ columns has
been computed, we determine the first row $r'_{start} \geq r_{start}$ and last
row $r'_{end}\leq r_{send}$ that are a multiple of $w$ and for which all
computed distances in this block are at most $t$[fn::More precisely, such that
in each column there is a state of distance $\leq t$ above (below) with distance
$\leq t$.], if such rows exists. (See
[[*Fixed range]] for details.) We then store these values $(r'_{start}, r'_{end})$
and the horizontal difference along row $r'_{end}$. The next iteration for
$t'=2t$ then skips the rows in this interval, and uses the stored differences as
input to compute rows $r'_{end}$ to the new $r_{end}$.

TODO: Explain 1/2/3-way split cases. With pseudocode?

** A*
Edlib already uses a simple /gap-cost/ heuristic that gives a lower bound on the
number of insertions and deletions on a path from each state to the end.
We simply replace this by the stronger heuristics introduced in A*PA.
We use three variants:
1. *No heuristic.* Only use the gap heuristic. No initialization needed.
2. *Seed heuristic (SH).* This requires relatively simple precomputation, and
   little bookkeeping, but works well for low uniform error rate.
3. *Gap-chaining seed heuristic (GCSH).* The strongest heuristic that requires
   more time to initialize and update, but is better able to penalize long indels.

The details of how these changes affects the ranges of rows being computed are
in [[*Appendix: Range-of-rows computations]].

We make two modifications the previous version of the A*PA algorithm.

*** Pre-pruning
#+caption: Effect of prepruning. Top: no prepruning, Bot: with prepruning.
| CSH, no pruning            | CSH, pruning                 |
| [[file:imgs/prepruning/csh.png]]    | [[file:imgs/prepruning/csh-p.png]]    |
| [[file:imgs/prepruning/csh-lp.png]] | [[file:imgs/prepruning/csh-lp-p.png]] |

#+caption: Appendix: full table. Effect of prepruning. Top: no prepruning, Bot: with prepruning.
| SH, no pruning | SH, pruning | CSH, no pruning | CSH, pruning | GCSH, no pruning | GCSH, pruning|
| [[file:imgs/prepruning/sh.png]]    | [[file:imgs/prepruning/sh-p.png]]    | [[file:imgs/prepruning/csh.png]]    | [[file:imgs/prepruning/csh-p.png]]    | [[file:imgs/prepruning/gcsh.png]]    | [[file:imgs/prepruning/gcsh-p.png]]    |
| [[file:imgs/prepruning/sh-lp.png]] | [[file:imgs/prepruning/sh-lp-p.png]] | [[file:imgs/prepruning/csh-lp.png]] | [[file:imgs/prepruning/csh-lp-p.png]] | [[file:imgs/prepruning/gcsh-lp.png]] | [[file:imgs/prepruning/gcsh-lp-p.png]] |

Here we introduce an independent optimization that also applies to the original
A*PA method.

Each of the heuristics $h$ introduced in A*PA depends on the set of matches
$\matches$. Given that $\matches$ contains /all/ matches, $h$ was shown to be an
admissible [TODO] heuristic. Even after pruning some matches, $h$ was shown to
still be a lower bound on the length of a path not going through already visited states.

Now consider a situation where there are two seeds and there is an exact match
$m$ from $u=v_s$ to $v$ for seed $s_0$, but going from $v$ to the end of the
next seed $s_1$ takes cost at least $2$ (TODO FIG).  The existence of the match
is a 'promise' that $s_0$ can be crossed for free.  In this case, this leads to
a seed heuristic value of $1$ is $u$, namely $0$ for $s_1$ plus $1$ for $s_1$. But we
already know that match $m$ can /never/ lead to a path of cost $<2$ to the end
of $s_1$. Thus, we may as well ignore $m$! This increases the value of the
seed heuristic in $u$ to $2$, which is indeed a lower bound on the actual distance.

More generally, consider a situation where there is a match $m$ from $u$ to $v$
in seed $s_i$, and the lowest cost path from $s_i$ to the /start/ of $s_{i+p}$
has cost $\geq p$.  The seed heuristic penalizes the path from $u$ (at the start
of $s_i$) to the start of $s_{i+p}$ by at most $p-1$, since there are at most
$p-1$ seeds in $\{s_{i+1}, \dots, s_{i+p-1}\}$ without match. Since in fact we
know that this path has cost at least $p$, we can /pre-prune/ the match $m$ and
increase the value of the heuristic while keeping it /admissible/.

TODO: Generally applicable in seed-and-extend settings.

TODO: Properly introduce $p$

*** Bulk-contours update
In A*PA, matches are /pruned/ as soon as a shortest path to their start has been
found. This helps to penalize states /before/ (left of) the match. Each
iteration of our new algorithm works left-to-right only, and thus pruning of
matches does not affect the current iteration. Instead of pruning on the fly, we
now collect all matches to be pruned at the end of each iteration, and prune
them in one right-to-left sweep.

To ensure pruning is a valid optimization, we never allow the range of rows for
each block to shrink after increasing $t$.

** Appendix: Range-of-rows computations
*** Computed range
Here we determine the range of rows that can possibly contain cells at distance
$\leq t$ in a block of $B$ columns from $i$ to $i+B$. We assume that the
heuristic $h$ being used is consistent, i.e. that for any states $u\preceq v$ we
have $h(u) \leq d(u,v) +h(v)$.

Let $R=(r_{start}, r_{end})$ be the range of states in column $i$ to which the
distance has been computed. From this we can find the topmost and
bottommost states $r^t_{start}$ and $r^t_{end}$ that are at distance $\leq t$,
see [[*Fixed range]].

*Start of range.* Since row $j=r^t_{start}$ is the first row in column $i$ with distance $\leq t$,
this means that states in columns $i$ to $i+B$ at rows $<j$ can not be at
distance $\leq t$. Thus, the first row that needs to be computed is row $r^t_{start}$.
[TODO: Add a $+1$ to this?]

*End of range.* We will now determine the bottommost row $j$ that can contain a
state at distance $\leq t$ in the block. Let $u=\st{i}{r^t_{end}}$ be the bottommost state in
column $i$ with distance $\leq t$. Let $v = \st{i'}{j'}$ be a state in the
current block ($i\leq i'\leq i+B$) that is below the diagonal of $u$ ($j'-i' \geq r^t_{end}-i$).
Then, the distance to $v$ is at least $\g(v) \geq \g(u) + \cgap(u,v)$, and hence
$$
f(v) = g(v) + h(v) \geq \g(v) + h(v) \geq \g(u) + \cgap(u,v) + h(v) =: f_l(v).
$$
The end of the range can now be computed by finding the bottommost state $v$ in each
column for which this lower bound $f_l$ on $f$ is $\leq t$, using the following
algorithm[fn::Bound checks omitted.].

*Algorithm (bottommost row computation).*
1. Start with $v = \st{i'}{j'} = u = \st{i}{r^t_{end}}$.
2. While the below-neighbour $v' = \st{i'}{j'+1}$ of $v$ has $f_l(v)\leq t$, increment $j'$.
3. Go to the next column by incrementing $i'$ and $j'$ by $1$ and repeat step 2, until $i'=i+B$.

A drawback of this approach is that $h$ is evaluated at least once per column,
which is slow in practice.

We improve this using the following lemma.

*Lemma 1.* When $h$ is a consistent heuristic and $v\preceq v'$, then $f_l(v')
\geq f(v) - 2\cdot d(v, v')$.

*Proof.* By consistency, $h(v) \leq d(v, v') + h(v')$, so $h(v') \geq
h(v)-d(v,v')$. Furthermore, $\cgap(u,v) \leq \cgap(u,v') + \cgap(v,v')\leq
\cgap(u,v) + d(v,v')$, and hence $\cgap(u,v') \geq \cgap(u,v) - d(v,v')$.
Putting these together we obtain
\begin{align*}
f_l(v') &= \g(u) + \cgap(u,v') + h(v') \\
&\geq \g(u) + \cgap(u,v) - d(v,v') + h(v) - d(v,v') \\
&= f_l(v) - 2\cdot d(v,v'). \square % TODO
\end{align*}

When we have $f_l(v) > t+2x$, the lemma implies that $f_l(v')>t$ for any $v'$
with $d(v,v')\leq x$. This inspires the following algorithm[fn::Bound checks omitted.], that first takes
just over $B$ steps down, and then makes jumps to the right.

*Algorithm (sparse bottommost row computation).*
1. Start with $v = \st{i'}{j'} = u+\st{0}{B+8} = \st{i}{r^t_{end} + B + 8}$.
2. If $f_l(v) \leq t$, increase $j'$ (go down) by $8$.
3. If $f_l(v) > t$, increase $i'$ (go right) by $\min(\lceil(f_l(v)-t)/2\rceil, i+B-i')$.
4. Repeat from step 2, until $i' = i+B$.
5. While $f_l(v) > t$, decrease $j'$ (go up) by $\lceil(f_l(v)-t)/2\rceil$, but
   do not go above the diagonal of $u$.

The resulting $v$ is the bottommost state in column $i+B$ with $f_l(v) \leq t$,
and its row is the last row that will be computed.

#+name: ranges
#+caption: Computed ranges
| [[./imgs/ranges/full.png]] Full boundary | [[./imgs/ranges/sparse.png]] Sparse h calls |

*** Fixed range
In a column, the /fixed/ range is the range of rows between the topmost and
bottommost states with $f(v)\leq t$, in rows $r'_{start}$ and $r'_{end}$
respectively.  Given a range $R=(r_{start}, r_{end})$ of computed values, one
way to find $r'_{start}$ and $r'_{end}$ is by simply iterating from the
start/end of the range and dropping all states $v$ with $f(v)>t$.
Like before, this has the drawback that the heuristic must be invoked many
times.

Instead, we have the following lemma, somewhat analogous to Lemma 1:

*Lemma 2.* When $h$ is a consistent heuristic we have
$$|f(v) - f(v')| \leq 2 d(v, v').$$

*Proof.*
The triangle inequality gives $\g(v) \leq \g(v') + d(v, v')$, and consistency
gives $h(v) \leq h(v') + d(v,v')$ TODO WHAT IF $v$ and $v'$ ARE IN THE OPPOSITE ORIENTATION??
Expanding the definitions of $f$, we have
\begin{align*}
f(v) - f(v')
&= (g(v) + h(v)) - (g(v') + h(v'))\\
&= (\g(v) + h(v)) - (\g(v') + h(v'))\\
&= (\g(v) - \g(v')) - (h(v) + h(v'))\\
&\leq d(v,v') + d(v,v') = 2\cdot d(v,v'). \square
\end{align*}

Now we can use a similar approach as before. To find the first row $j'$ with
$f(\st ij)\leq t$, start with
$v=\st{i'}{j'}=\st{i}{r_{start}}$, and increment $j'$ by
$\lceil(f(v)-t)/2\rceil$ as long as $f(v)>t$. The last row is found in the same way.


TODO: Run this algorithm directly on 64-row lanes.
* Results

Compare
- Edlib
- WFA
- A*PA
- A*PA2 no h
- A*PA2 SH+pruning
- A*PA2 GCSH+pruning
on
- synthetic data
- human ONT reads
- human ONT reads with genetic variation

Important:
- Find threshold where heuristics become worth the overhead
- Show benefit of each of the optimizations
- Show sensitivity to parameter tuning


** Real data
*** Dataset statistics
#+name: statistics
#+caption: Real dataset statistics. Lengths in kbp, divergence in %.
| dataset      |   cnt | len min | len mean | len max | div min | div mean | div max | max gap mean | max gap max |
| ONT >500k    |    50 |     500 |      594 |     849 |     2.7 |      6.1 |    16.7 |          0.1 |         1.3 |
| ONT+gen.var. |    48 |     502 |      632 |    1053 |     4.3 |      7.2 |    18.2 |          1.9 |          42 |
| SARS-CoV-2   | 10000 |      27 |       30 |      30 |     0.0 |      1.5 |    12.8 |          0.1 |         1.0 |
| ONT < 1k     | 12477 |    0.04 |      0.8 |     1.1 |     0.0 |     10.4 |    22.5 |         0.01 |         0.1 |
| ONT < 10k    |  5000 |     0.2 |      3.6 |      10 |     3.0 |     12.1 |    20.1 |         0.04 |         0.5 |
| ONT < 50k    | 10000 |     0.2 |       11 |      50 |     3.0 |     11.6 |    19.2 |         0.07 |         3.4 |


*** Real data summary

#+name: real
#+caption: real data summary
#+attr_html: :class inset large
[[file:plots/real-summary.svg]]

*** Appendix: Real data by divergence

#+name: real
#+caption: real data scaling with divergence
#+attr_html: :class inset large
[[file:plots/real-summary-scatter.svg]]

*** Incremental improvements

#+name: real
#+caption: real data improvements
#+attr_html: :class inset large
[[file:plots/real-incremental.svg]]

*** Real data ablation

#+name: ablation
#+caption: Ablation: how things slow down when removing 1 feature.
#+attr_html: :class inset large
[[file:plots/real-ablation.svg]]
*** Real data parameters
#+name: params
#+caption: Varying parameters.
#+attr_html: :class inset large
[[file:plots/real-params.svg]]
*** Memory usage

#+name: memory
#+caption: Memory usage
#+attr_html:
|              | >500kbp ONT reads Median | >500kbp ONT reads Max | >500kbp ONT reads + genetic variation Median | >500kbp ONT reads + genetic variation Max | SARS-CoV-2 pairs Median | SARS-CoV-2 pairs Max | <1kbp ONT reads Median | <1kbp ONT reads Max | <10kbp ONT reads Median | <10kbp ONT reads Max | <50kbp ONT reads Median | <50kbp ONT reads Max |
|--------------+--------------------------+-----------------------+----------------------------------------------+-------------------------------------------+-------------------------+----------------------+------------------------+---------------------+-------------------------+----------------------+-------------------------+----------------------|
| Edlib        |                        0 |                     3 |                                            1 |                                         4 |                       2 |                    5 |                      0 |                   0 |                       0 |                    0 |                       3 |                    5 |
| BiWFA        |                       10 |                    19 |                                           12 |                                        17 |                      11 |                   14 |                      2 |                   2 |                       2 |                    2 |                       8 |                   11 |
| A*PA         |                       90 |                   467 |                                          160 |                                       585 |                       3 |                   46 |                      0 |                   0 |                       9 |                   67 |                      63 |                  150 |
| A*PA2 simple |                       22 |                    86 |                                           31 |                                       197 |                       2 |                    5 |                      0 |                   0 |                       0 |                    0 |                       4 |                    6 |
| A*PA2 full   |                       28 |                    84 |                                           32 |                                       166 |                       3 |                    7 |                      0 |                   0 |                       0 |                    0 |                       5 |                    7 |
*** Runtime distribution
#+name: runtime
#+caption: Runtime distribution
#+attr_html: :class inset large
[[file:plots/real-timing.svg]]
** Synthetic data
*** Scaling with divergence
#+name: scaling-e
#+caption: Scaling with error rate
#+attr_html: :class inset large
[[file:plots/scaling_e.svg]]

* Conclusion
** Summary
** Limitations
- Initialization takes time
- WFA is better when edit distance is /very/ low.
** Future work
- Pre-pruning for seed&extend methods?
- Semi-global alignment
- Affine alignment
- Local doubling
- DT Blocks
- TALCO: tiling alignment using convergence of traceback pointers: https://turakhia.ucsd.edu/research/


* Acknowledgements
:PROPERTIES:
:UNNUMBERED: t
:END:

I am grateful to Daniel Liu for regular discussions, and suggesting additional
papers that have been added to the introduction.

#+print_bibliography:
