#+title: [WIP] A*PA2: 20x faster exact global alignment of long sequences
#+HUGO_SECTION: posts
#+HUGO_TAGS: paper-draft
#+HUGO_LEVEL_OFFSET: 1
#+OPTIONS: ^:{} num:t
#+hugo_front_matter_key_replace: author>authors
#+toc: headlines 3
#+date: <2024-02-29>
#+author: Ragnar Groot Koerkamp

\begin{equation*}
\newcommand{\g}{g^*}
\newcommand{\h}{h^*}
\newcommand{\f}{f^*}
\newcommand{\cgap}{c_{\texttt{gap}}}
\newcommand{\xor}{\ \mathrm{xor}\ }
\newcommand{\and}{\ \mathrm{and}\ }
\newcommand{\st}[2]{\langle #1, #2\rangle}
\newcommand{\matches}{\mathcal M}
\end{equation*}

Preface: This is my work-in-progress draft for my paper on version 2 of A*PA.
Code is done and works, but needs some cleanup and polishing. The API also needs
work.

* Abstract
:PROPERTIES:
:UNNUMBERED: t
:END:

*Methods.* We introduce A*PA2, an exact global pairwise aligner with respect to
e Edlib, A*PA2 uses Ukkonen's band doubling in combination
with Myers' bitpacking. A*PA2 1) extends this with SIMD, 2) uses large block
sizes inspired by Block Aligner, 3) avoids recomputation of states where
possible as suggested before by Fickett, 4) introduces a new optimistic technique for
traceback based on diagonal transition (WFA), and 5) applies the heuristics
developed in A*PA and improves them using /pre-pruning/.

*Results.*
The average runtime of A*PA2 is  $20\times$ faster than BiWFA and Edlib on $>500$kbp long ONT reads of a
human genome having $6\%$ divergence on average. On shorter ONT reads of
 $10\%-12\%$ average divergence the speedup is $6.1\times$ (length $11$kbp),
$3.7\times$ (length $4$kbp), and $0.96\times$ (length $800$bp).

*Availability.* [[https://github.com/RagnarGrootKoerkamp/astar-pairwise-aligner]]

*Contact.* [[https://twitter.com/curious_coding]]

* Introduction

The problem of /global pairwise alignment/ is to find the shortest sequence of
edit operations (insertions, deletions, substitutions) to convert a string $A$
into a second string $B$ [cite:@nw;@vintsyuk68], where the number of such
operations is called the /Levenshtein distance/ or /edit distance/
[cite:@levenshtein;@wagner74].

Over time, the length of genomic reads has increased from hundreds of basepairs
to hundreds of thousands basepairs now. Meanwhile, the complexity of exact
algorithms has not been improved by more than a constant factor since the
introduction of the $O(s^2+n)$ diagonal transition algorithm [cite:@ukkonen85].

In this work we introduce a new exact algorithm for global pairwise alignment
building on the recently introduced A* heuristics of A*PA [cite:@astarpa], but
instead of running A* we use a much more efficient DP-based algorithm.
As [cite/t:@fickett84 p. 1] stated already 40 years ago,
#+begin_quote
at present one must choose between an algorithm which gives the best alignment
but is expensive, and an algorithm which is fast but may not give the best
alignment. In this paper we narrow the gap between these choices by showing how
to get the optimal alignment much more quickly than before.
#+end_quote

** Contributions
We introduce A*PA2, an exactly pairwise global sequence aligner with respect to
edit distance.

In A*PA2 we combine multiple existing techniques and introduce a number of new
ideas to obtain a $20\times$ speedup over existing single-threaded exact
aligners.  As a starting point, we take the band doubling algorithm implemented
by Edlib [cite:@edlib] using bitpacking [cite:@myers99].  First, we speed up the
implementation (points 1., 2., and 3.).  Then, we reduce the amount of work done
(points 4. and 5.).  Lastly, we apply A*.

1. *Block-based computation.* Edlib computes one column of the DP matrix at a
   time, and for each column decides which range (subset of rows) of states to
   compute.  We significantly reduce this overhead by processing blocks of $256$
   columns at a time, taking inspiration from Block aligner
   [cite:@block-aligner].  Correspondingly, we only store states of the
   DP-matrix at block boundaries, reducing memory usage.
2. *SIMD.* We speed up the implementation using SIMD to compute each block,
   allowing the processing of $4$ computer words in parallel.
3. *Novel encoding.* We introduce a novel encoding of the input sequence to
   speed up SIMD operations. This limits the current implementation to size-$4$
   alphabets.
4. *Incremental doubling.* Both the band doubling methods of [cite/t:@ukkonen85]
   and Edlib recompute states after doubling the threshold.  We avoid this by
   using the theory behind the A* algorithm, extending the incremental doubling
   of [cite/t:@fickett84] to blocks and arbitrary heuristics.
5. *Traceback.* For the traceback, we optimistically use the diagonal transition
   method within each block with a strong adaptive heuristic, falling back to a
   full recomputation of the block when needed.
6. *A** *.* We improve the A* seed heuristics of [cite/t:@astarpa] in two ways.
   First, instead of updating contours each time a match is pruned, we now do
   this in batches once the band is doubled. Secondly, we introduce a new
   /pre-pruning/ technique that discards most of the /spurious/ (off-path)
   matches ahead of time.

#+name: domains
#+caption: Alignment of two sequences of length 4000 with 20% errors introduced.
| [[file:imgs/intro/0_full.png]] Full grid (NW) | [[file:imgs/intro/0_gap-start.png]] gap heuristic                       | [[file:imgs/intro/0_gap-gap.png]] gap-gap (ukkonen)    |                                           |
|                                      | [[file:imgs/intro/0_g.png]] g*                                          | [[file:imgs/intro/0_g-gap.png]] g*+gap (edlib)         | [[file:imgs/intro/5_astarpa.png]] g*+h (A*PA)      |
|                                      | [[file:imgs/intro/2_dijkstra.png]] Dijkstra                             | [[file:imgs/intro/6_astarpa2_simple.png]] A*PA2-simple | [[file:imgs/intro/7_astarpa2_full.png]] A*PA2-full |
|                                      | [[file:imgs/intro/3_diagonal-transition.png]] Diagonal transition (WFA) |                                               |                                           |

** Previous work

In the following, we give a brief recap of developments that this work builds
on, in chronological order per approach.  See also e.g. the reviews by
[cite/t:@kruskal83] and [cite/t:@navarro01].

*** Needleman-Wunsch
This problem has classically been approached as a dynamic
programming (DP) problem. For string lengths $n$ and $m$, [cite/t:@nw]
introduced the first $O(n^2m)$ algorithm. [cite/t:@sellers] and
[cite/t:@wagner74] improved this to what is now
known as the $O(nm)$ /Needleman-Wunsch algorithm/, building on the quadratic algorithm for /longest common
subsequence/ by [cite/t:@sankoff].

*** Graph algorithms
It was already realized early on that this problem
corresponds to finding the shortest path from $v_s$ to $v_t$ in the /alignment
graph/, which is also called /edit graph/ or /dependency graph/
[cite:@vintsyuk68;@ukkonen85]. Both [cite/t:@ukkonen85] and [cite/t:@myers86]
remarked that this can be solved using Dijkstra's algorithm [cite:@dijkstra59],
taking $O(ns)$ time[fn::Although Ukkonen didn't realize this faster runtime and
only gave a bound of $O(nm \log (nm))$.], where $s$ is the edit distance between
the two strings.  However, [cite/t:@myers86 p. 2] observes that
#+begin_quote
the resulting  algorithm involves a relatively complex discrete priority queue
and this queue  may contain as many as O(ND) entries even in the case where just
the length  of the [...] shortest edit script is being computed.
#+end_quote
[cite/t:@hadlock88detour] realized that Dijkstra's algorithm can be improved
upon by using A* [cite:@astar-hart67], a more /informed/ algorithm that uses a
/heuristic/ function $h(u)$ that gives a lower bound on the edit distance
$\h(u)$ between the suffixes following DP state $u$. He uses two heuristics, the widely
used /gap cost/ heuristic $h(u)=\cgap(u, v_t)$
[cite:@ukkonen85;@hadlock88detour;@wu90-O-np;@spouge89;@spouge91;@papamichail2009;]
that simply uses the difference between the lengths of the suffixes as lower
bound, and a new improved heuristic based on character frequencies in the two
suffixes. A*PA [cite:@astarpa] applies the /gap-chaining seed heuristic/ with /pruning/
[cite:@astarix-2] to obtain near-linear runtime when errors are uniform random.
Nevertheless, as [cite/t:@spouge91 p. 3] states:
#+begin_quote
Many algorithms for finding optimal paths in non-lattice graphs also exist
[cite:@dijkstra59; @astar-hart67; @rubin74], but algorithms exploiting the
lattice structure of an alignment graph are usually faster. In molecular
biology, speed is important, ...
#+end_quote
and further [cite:@spouge89 p. 4]:
#+begin_quote
This suggests a radical approach to A* search complexities: dispense with the
lists [of open states] if there is a natural order for vertex expansion.
#+end_quote
In this work we follow this advice and replace the A* search in A*PA with a much
more efficient DP based approach based on /computational volumes/.

*** Computational volumes
[cite/t:@wilbur-lipman-83] is, to our knowledge, the first paper that speeds up
the $O(nm)$ DP algorithm, by only considering states near diagonals with many
/k-mer matches/, but at the cost of giving up the exactness of the method.
[cite/t:@fickett84 p. 177] notes that for $t\geq s$ only those DP-states with
cost $\g(u)$ at most $t$ need to be computed:
#+begin_quote
However it is possible to fill the matrix in many different orders, the only
restriction being that the calculation of any given $d_{ij}$ depends on already
having the values of the three elements up and to the left of it.

[...]

But the only alignments of subsequences which are relevant are ones at least as
good (distance at least as small) as the overall one. I.e. one really only needs
those $d_{ij}$ which are below a fixed bound.
#+end_quote
This only requires $O(nt)$ time, which is fast when $t$ is an accurate bound on
the distance $s$. For example $t$ can be set as an upper bound for the
data being aligned, or as the length of a suboptimal alignment.  When
$t=t_0$ turns out too small, a larger new bound $t_1$ can be chosen, and only
states with distance in between $t_0$ and $t_1$ have to be computed.  This is
implemented by keeping for each row the index of the first and last state with
value at most $t_0$, and skipping over already computed states.  In the limit
where $t$ increases by $1$ in each iteration, this closely mirrors Dijkstra's algorithm.

[cite/t:@ukkonen85] introduces a very similar idea, statically bounding the
computation to only those states that can be on a path of length at most $t$
through the graph. When the sequences have the same length ($n=m$), this only
considers diagonals $-t/2$ to $t/2$, where diagonal $0$ is the main diagonal of
the DP-matrix.

On top of this, [cite/t:@ukkonen85] introduces /band doubling/: $t_0=1$ can be
/doubled/ ($t_i = 2^i$) until $t_k$ is at least the actual distance $s$, so that
the alignment can be found in $O(ns)$ time.

[cite/t:@spouge89] unifies the methods of
[cite/t:@fickett84] and [cite/t:@ukkonen85] in /computational volumes/
(see [[*Preliminaries]]): subgraphs of the full edit graph that are guaranteed to
contain the shortest paths that are much smaller and can hence be computed faster.
As [cite/t:@spouge89 p. 1559] notes:
#+begin_quote
The order of computation (row major, column major or antidiagonal) is just a
minor detail in most algorithms.
#+end_quote
But this is exactly what was investigated a lot in the search for more efficient implementations.

*** Parallelism
In the 1990s, the focus shifted from
reducing the number of computed states to computing states faster through
advancements in implementation and hardware (SIMD, GPUs).  These speedups are
often applied to the Smith-Waterman-(Gotoh) [cite:@sw;@gotoh] algorithm for
(affine-cost) local alignment, where algorithmic improvements beyond
$\Theta(nm)$ are unknown.

The first technique in this direction is /microparallelism/ [cite:@alpern95],
where each (64 bit) computer word is divided into multiple (e.g. 16 bit) parts,
and word-size operations modify all (4) parts in parallel.
[cite/t:@alpern95] applied this with /inter-sequence parallelism/ to align a
given query to four reference sequences in parallel, see also
[cite/t:@rognes11].  [cite/t:@hughey96] was the first to note that
/anti-diagonals/ of the DP matrix are independent and can be computed in parallel, and
[cite/t:@wozniak97] applied SIMD for this purpose.

[cite/t:@rognes00 p. 702] also use microparallelism, but use /vertical/
instead of anti-diagonal vectors:
#+begin_quote
The advantage of this approach is the much-simplified and faster loading of the
vector of substitution scores from memory. The disadvantage is that data
dependencies within the vector must be handled.
#+end_quote
They also introduce the /query profile/
to efficiently compute which characters equal each other, see [[*Preliminaries]].
Similarly, [cite/t:@myers99] introduces a /bitpacking/ algorithm specifically
for edit distance. It bit-encodes the differences between $w=64$ states in a
column into two computer words, and an efficient algorithm to operate on them ([[myers]]).
BitPAl [cite:@bitpal;@bitpal-cpm] introduces an alternative bitpacking scheme based on a
slightly different bit-encoding, but as both methods end up using $20$
instructions (see [[*BitPAl]]) we did not pursue this further.

---

TODO
- [cite:@farrar] Farrar's striped; uses query profile; conditional prefix scan
  is moved outside inner loop. $2-8\times$ faster than Wozniak and Rognes.
- Wu Manber 1992
- Baeza-Yates Gonnet 1992
- Hyyro and Navarro, 2005; Hyyro et al., 2005
- Benson 2013
- navarro 2004
- bergeron hamel 2002

*** Tools
There are multiple semi-global aligners that implement $O(nm)$ global
alignment using numerous of the aforementioned implementation
techniques, such as SeqAn [cite:@seqan], Parasail [cite:@parasail], Opal
(https://github.com/martinsos/opal), libssa
(https://github.com/RonnySoak/libssa), SWIPE [cite:@rognes11], SWPS3
[cite:@swps3], SSW library [cite:@ssw-library] ([[https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library][link]]), and KSW2 [cite:@minimap2].

Dedicated global alignment implementations are much rarer.
Edlib [cite:@edlib] implements the band doubling of [cite/t:@ukkonen85] using
the $\g(u)+\cgap(u, v_t)\leq t$ computational volume of [cite/t:@spouge91] and
the bitpacking of [cite/t:@myers99].
WFA and BiWFA [cite:@wfa;@biwfa] implement the $O(n+s^2)$ expected time /diagonal transition/
algorithm [cite:@ukkonen85;@myers86].
Block aligner [cite:@block-aligner] is an approximate aligner that can handle
position-specific scoring matrices whose main novelty is to divide the
computation into blocks.
Lastly, A*PA [cite:@astarpa] directly implements A* on the alignment graph using
the gap-chaining seed heuristic.

---

TODO
- Opal: Šošic M. An simd dynamic programming c/c++ library: Thesis, University
  of Zagreb; 2015. https://bib.irb.hr/datoteka/758607.diplomski_Martin_
  Sosic.pdf.

- libssa: Frielingsdorf JT. Improving optimal sequence alignments through a
  simd-accelerated library: Thesis, University of Oslo; 2015. http://urn.nb.no/
  URN:NBN:no-49935. Accessed 10 Dec 2015.
- [cite:@suzuki-kasahara] libgaba: SIMD with difference recurrence relation for
  affine cost alignment

* Preliminaries
*Edit graph.* We take as input two zero-indexed sequences $A$ and $B$ over an alphabet of size
$4$ of lengths $n$ and $m$.  The /edit graph/ contains /states/ $\st ij$ ($0\leq
i\leq n$, $0\leq j\leq m$) as vertices. It further contains directed insertion and
deletion edges $\st ij \to \st i{j+1}$ and $\st ij \to \st {i+1}j$ of cost $1$,
and diagonal edges $\st ij\to \st{i+1}{j+1}$ of cost $0$ when $A_i = B_i$ and
substitution cost $1$ otherwise.  The shortest path from $v_s:=\st 00$ to $v_t :=
\st nm$ in the edit graph corresponds to an alignment of $A$ and $B$.
We write $d(u,v)$ for the distance from $u$ to
$v$, $\g(u) := d(v_s, u)$ for the distance from the start, $\h(u) := d(u, v_t)$
for the distance to the end, and $\f(u) := \g(u) + \h(u)$ for the length of the
shortest path through $u$.

*A** is a shortest path algorithm based on a /heuristic/ function $h(u)$ [cite:@astar-hart67]. A
heuristic is called /admissible/ when $h(u)$ underestimates the distance to the
end, i.e., $h(u) \leq \h(u)$, and admissible $h$ guarantee that A* finds a
shortest path. A* /expands/ states in order of increasing $f(u) :=
g(u) + h(u)$, where $g(u)$ is the best distance to $u$ found so far. We say that
$u$ is /fixed/ when the distance to $u$ has been found, i.e., $g(u) = \g(u)$.

*Computational volumes.* [cite/t:@spouge89] defines a /computational volume/ as a subgraph of the
alignment graph that contains all shortest paths . Given a bound $t\geq s$, some examples of
computational volumes are:
1. $\{u\}$, the entire $(n+1)\times (m+1)$ graph [cite:@nw].
2. $\{u: \g(u)\leq t\}$, the states at distance $\leq t$, introduced by
   [cite/t:@fickett84] and similar to Dijkstra's algorithm [cite:@dijkstra59].
3. $\{u: \cgap(v_s, u) + \cgap(u, v_t) \leq t\}$ the static set of states possibly on a path
   of length $\leq t$ [cite:@ukkonen85].
4. $\{u: \g(u) + \cgap(u, v_t) \leq t\}$, as used by Edlib [cite:@edlib;@spouge91;@papamichail2009].
5. $\{u: \g(u) + h(u) \leq t\}$, for any admissible heuristic $h$, which we will
   use as is similar to A*.


*Band-doubling* is the following algorithm by [cite/t:@ukkonen85], that depends on the choice of
computational volume being used.
1. Start with edit-distance threshold $t=1$.
2. Loop over columns $i$ from $0$ to $n$.
3. For each column, determine the range of rows $[j_{start}, j_{end}]$ to be
   computed according the computational volume that's being used.
   a. If this range is empty or does not contain a state at distance $\leq t$, double $t$ and go back to step 1.
   b. Otherwise, compute the distance to the states in the range, and continue
      with the next column.
The algorithm stops when $t_k \geq s > t_{k-1}$. For the
$\cgap(v_s,u)+\cgap(u,v_t)\leq t$ computational volume used by Ukkonen, each
test requires $O(n \cdot t_i)$ time, and hence the total time is
\begin{equation}
n\cdot t_0 + \dots + n\cdot t_k
= n\cdot (2^0 + \dots + 2^k)
< n\cdot 2^{k+1} = 4\cdot n\cdot 2^{k-1} < 4\cdot n\cdot s = O(ns).
\end{equation}
Note that this method does not (and indeed can not) reuse values from previous
iterations, resulting in roughly a factor $2$ overhead.

*Myers' bitpacking* exploits that the difference in distance to adjacent states
is always in $\{-1,0,+1\}$ [cite:@myers99]. The method bit-encodes $w=64$ differences between
adjacent states in a columns in two
indicator words, indicating positions where the difference is $+1$ and $-1$ respectively.
Given also the similarly encoded difference along the top, a $1\times w$
rectangle can be computed in only $20$ bit operations ([[myers]]).
[[*BitPAl]] contains similar code for BitPAL, which also uses $20$ instructions.
We call each consecutive non-overlapping chunk of $64$ rows a /lane/, so that
there are $\lceil m/64\rceil$ lanes, where the last lane may be padded.
Note that this method originally only uses $17$ instructions, but some additional
instructions are needed to support multiple lanes when $m>w$.

*Profile.* Instead of computing each substitution score $S[A_i][B_j] = [A_i\neq
B_j]$ for the $64$ states in a word one by one, Myers' algorithm first builds a
/profile/ [cite:@rognes00].  For each character $c$, $Eq[c][j]$ stores a bitvector indicating
which characters of $B$ equal $c$.  This way, adjacent equalities in a columns
are simply found as $Eq[A_i][j \dots j']$.

*Edlib* implements band doubling using the $\g(u) + \cgap(u, v_t)\leq t$ computational
volume and bitpacking [cite:@edlib]. For traceback, it uses Hirschberg's /meet-in-the-middle/
approach: once the distance is found, the alignment is started over from both
sides towards the middle column, where a state on the shortest path is
determined. This is recursively applied to the left and right halves until the
sequences are short enough that $O(tn)$ memory can be used.

#+name: blocks
#+caption: TODO introducing blocks
| [[file:imgs/intro/0_g-gap.png]] g-gap | [[file:imgs/intro/0_bitpacking.png]] myers bitpacking (Edlib) | [[file:imgs/intro/0_blocks.png]] Blocks (Block aligner) |


* Methods
Conceptually, A*PA2 builds on Edlib.
First we describe how we make the implementation more efficient
using SIMD and blocks.
Then, we modify the algorithm itself by using a new traceback method and
avoiding unnecessary recomputation of states.
On top of that, we apply the A*PA heuristics for further speed gains on large/complex
alignments, at the cost of larger precomputation time to build the heuristic.

** Band-doubling
A*PA2 uses band-doubling with the $\g(u) + h(u) \leq t$ computational volume.
That is, in each iteration of $t$ we compute the distance to all states with
$\g(u) + h(u) \leq t$.  In its simple form, we use $h(u) =\cgap(u, v_t)$ like
Edlib does.  As initial guess $t_0$ for $t$ we use $h(v_s)=h\st 00$.  Instead of
doubling $t$ itself, we double the increment, so that initially we stay close to
$t_0$. Additionally we start with a small offset to compensate for the block
sizes and bitpacking (see below), so that the $t_i$ we try are
$$
t_i := h\st 00 + B \cdot 2^i.
$$

** Blocks
Instead of determining the range of rows to be computed for each column
individually, we determine it once per /block/ and then reuse it for $B=256$
consecutive columns.  This computes some extra states, but reduces the overhead
by a lot.

Within each block, we iterate over the /lanes/ of $w=64$ rows at a time, and for
each lane compute all $B$ columns before moving on to the next lane.

[[*Determining the rows to compute]] explains in detail how the range of rows to be
computed is determined.

** Memory

Where Edlib does not initially store intermediate values and uses
meet-in-the-middle to find the alignment, A*PA2 /always/ stores the distance to
all states at the end of a block, encoded as the distance to the top-right state
of the block and the bit-encoded vertical differences along the right-most
column.  This simplifies the traceback method (see [[*Traceback]]), and has
sufficiently small memory usage to be practical.

** SIMD

#+name: myers
#+caption: *Myers' bitpacking.* Rust code for SIMD version of Myers' bitpacking algorithm. Computes four independent words on an antidiagonal in parallel in $20$ instructions.
#+begin_src rust
pub fn compute_block_simd_myers(
    hp0: &mut Simd<u64, 4>,  // 0 or 1. Indicates +1 difference on top.
    hm0: &mut Simd<u64, 4>,  // 0 or 1. Indicates -1 difference on top.
    vp: &mut Simd<u64, 4>,  // 64-bit indicator of +1 differences on left.
    vm: &mut Simd<u64, 4>,  // 64-bit indicator of -1 differences on left.
    eq: Simd<u64, 4>,  // 64-bit indicator which characters equal the top char.
) {
    let vx = eq | *vm;
    let eq = eq | *hm0;
    // The addition carries information between rows.
    let hx = (((eq & *vp) + *vp) ^ *vp) | eq;
    let hp = *vm | !(hx | *vp);
    let hm = *vp & hx;
    // Extract the high bit as bottom horizontal difference.
    let right_shift = Simd::<u64,4>::splat(63);   // Shift each lane by 63.
    let hpw = hp >> right_shift;
    let hmw = hm >> right_shift;
    // Insert the top horizontal difference.
    let left_shift = Simd::<u64,4>::splat(1);     // Shift each lane by 1.
    let hp = (hp << left_shift) | *hp0;
    let hm = (hm << left_shift) | *hm0;
    // Update the input-output parameters.
    ,*hp0 = hpw;
    ,*hm0 = hmw;
    ,*vp = hm | !(vx | hp);
    ,*vm = hp & vx;
}
#+end_src

#+name: simd
#+caption: *SIMD* processing of two times 4 lanes in parallel. This example uses 4-row (instead of 64-row) lanes. First the top-left triangle is computed lane by lane, and then 8-lane diagonals are computed by using two 4-lane SIMD vectors in parallel.
[[file:imgs/simd.png]]

While it is tempting to use a SIMD vector as a single $W=256$-bit word, the four
$w=64$-bit words (SIMD lanes) are dependent on each other and require manual
work to shift bits between the lanes.
Instead, we let each $256$-bit AVX2 SIMD vector represent four $64$-bit words
(lanes) that are anti-diagonally
staggered as in [[simd]]. This is similar to the original anti-diagonal tiling
introduced by [cite/t:@wozniak97], but using units of $w$-bit words instead of
single characters. This idea was already introduced in 2014 by the author of
Edlib in a GitHub issue (https://github.com/Martinsos/edlib/issues/5), but to our
knowledge has never been implemented either in Edlib or elsewhere.

We further improve instruction-level-parallelism (ILP) by processing $8$ lanes
at a time using two SIMD vectors in parallel, spanning a total of $512$ rows ([[simd]]).

When the number of remaining lanes to be computed is $\ell$, we
process $8$ lanes in parallel as long as $\ell\geq 8$. If there are remaining
lanes, we end with another $8$-lane ($5\leq \ell<8$) or $4$-lane ($1\leq \ell\leq 4$)
iteration that optionally includes some padding lanes at the bottom.
In case the horizontal differences along the original bottom row are needed (as
required by incremental doubling [[*Incremental doubling]]), we
can not use padding and instead fall back to trying a $4$-lane SIMD ($\ell\geq 4$),
a $2$-lane SIMD ($\ell\geq 2$), and lastly a scalar iteration ($\ell\geq 1$).

# TODO: How about padding upwards?

** SIMD-friendly sequence profile

#+name: profile
#+caption: *Sequence profile.* Character equality check for the SIMD-friendly sequence profile.
#+begin_src rust
/// `ca`: Exploded bit-encoding of single char `c` of `a`.
///   e.g.  c=1: (11...11, 00...00)
/// 64-char packed *negated* bit-encoding of 64 chars of `b`.
///   e.g. bj=1: (...0..., ...1...)
/// Returns a mask which chars of `b` equal the char of `a`.
fn eq(ca: &(u64, u64), profile_b: &(u64, u64)) -> u64 {
    (ca.0 ^ profile_b.0) & (ca.1 ^ profile_b.1)
}
#+end_src

A drawback of anti-diagonal tiling is that each column contains its own
character $a_i$ that needs to be looked up in the profile $Eq[a_i][j]$. While SIMD can do multiple
lookups in parallel using =gather= instructions, these instructions are
not always efficient. Thus, we introduce the following alternative scheme.

Let $b = \lceil \log_2(\sigma)\rceil$ be the number of bits needed to encode
each character, with $b=2$ for DNA.
For each lane, the new profile $Eq'$ stores $b$ words as an $\lceil
m/w\rceil\times b$ array $Eq'[\ell][p]$. Each word $0\leq p< b$
stores the negation of the $p$th bit of each character.
To check which characters in lane $\ell$ contain character $c$ with bit representation
$\overline{c_{b-1}\dots c_{0}}$, we precompute $b$ words $C_0 =
\overline{c_0\dots c_0}$ to
$C_{b-1}=\overline{c_{b-1}\dots c_{b-1}}$ and then compute
$$(C_0 \xor Eq'[\ell][0]) \and \dots \and (C_{b-1} \xor Eq'[\ell][b-1]),$$
see [[profile]].
This naturally extends to SIMD vectors, where each lane is initialized with its
own constants $C_i$.

** Traceback

#+name: trace
#+caption: *Traceback method.* States expanded by the diagonal transition traceback in each block are shown in green. When the distance in a block is too large, a part of the block is fully recomputed as fallback, as shown in blue.
[[file:imgs/trace/trace.png]]

The traceback stage takes as input the computed vertical differences at
the end of each block of columns. We iteratively work backwards through the
blocks. In each step, we know the distances $g\st ij$ to
the states in column $i$ and the state $u=\st{i+B}j$ in column $i+B$
that is on the optimal path and has distance $\g(u)$.
The goal is to find an optimal path from column $i$ to $u$.

A naive approach is to simply recompute the entire block of columns while
storing distances to all states. Here we consider two more efficient methods.

*Optimistic block computation.*
Instead of computing the full range for this column, a
first insight is that only rows up to $j$ are needed, since the optimal path to
$u=\st{i+B}j$ can never go below row $j$.

Secondly, the path crosses $B=256$ columns, and so we optimistically assume that
it will be contained in rows $j-256-64=j-320$ to $j$. Thus, we first compute the
distance to all states in this range of rows (rounded out to multiples of
$w=64$). If the distance to $u$ computed this way agrees with the known
distance, there is a shortest path contained within the computed rows and we
trace it one state at a time. Otherwise, we repeatedly try again with double the
number of lanes, until success. The exponential search ensures low overhead and
good average case performance.

*Optimistic diagonal transition traceback (DTT).*
A second improvement uses the /diagonal transition/ algorithm backwards from
$u$. We simply run the unmodified algorithm on the reverse graph covering
columns $i$ to $i+B$ and rows $0$ to $j$.
Whenever a state $v$ in column $i$ is reached, with distance $d$ from $u$, we check
whether $g(v) + d=\g(u)$, and continue until a $v$ is found for which this holds.
We then know that $v$ lies on a shortest path and can find the path from $v$ to
$u$ by a usual traceback on the diagonal transition algorithm.

As an optimization, when no suitable $v$ is found after trying all states at
distance $\leq 40$, we abort the DT trace fall back to the block doubling described above.
Another optimization is the WF-adaptive heuristic introduced by WFA: all states
that lag more than $10$ behind the furthest reaching diagonal are dropped.
Lastly, we abort early when after reaching distance $20=40/2$, less than half
the columns were reached.

[[trace]] shows that in regions with low divergence, the DTT is sufficient to trace
the path, and only in noisy regions the algorithm falls back to recomputing full blocks.

** A*
Edlib already uses a simple /gap-cost/ heuristic that gives a lower bound on the
number of insertions and deletions on a path from each state to the end.
We replace this by the much stronger gap-chaining seed heuristic (GCSH) introduced in A*PA.

Compared to A*PA, we make two modifications.

*** Bulk-contours update
In A*PA, matches are /pruned/ as soon as a shortest path to their start has been
found. This helps to penalize states /before/ (left of) the match. Each
iteration of our new algorithm works left-to-right only, and thus pruning of
matches does not affect the current iteration. Instead of pruning on the fly, we
collect all matches to be pruned at the end of each iteration, and update the
contours in one right-to-left sweep.

To ensure the band doubling approach remains valid after pruning, we ensure that
the range of computed rows never shrinks after an increase of $t$ and subsequent
pruning.

*** Pre-pruning
#+name: pre-pruning
#+caption: *Effect of pre-pruning* on chaining seed heuristic (CSH) contours. The left shows contours and layers of the heuristic at the end of an A*PA alignment, after matches (black diagonals) on the path have been pruned (red). The right shows pre-pruned matches in purple and the states visited during pre-pruning in green. After pre-pruning, almost no off-path matches remain. This decreases the number of contours, making the heuristic stronger, and simplifies contours, making the heuristic faster to evaluate.
#+attr_html: :class small
| [[file:imgs/prepruning/csh-p.png]] | [[file:imgs/prepruning/csh-lp-p.png]] |

Here we introduce an independent optimization that also applies to the original
A*PA method.

Each of the heuristics $h$ introduced in A*PA depends on the set of matches
$\matches$. Given that $\matches$ contains /all/ matches, $h$ is an
admissible heuristic that never overestimates the true distance. Even after
pruning some matches, $h$ is still a lower bound on the length of a
path not going through already visited states.

Now consider an exact match $m$ from $u$ to $v$ for seed $s_i$.  The existence
of the match is a 'promise' that seed $s_i$ can be crossed for free.  When $m$
is a match outside the optimal alignment,
it is likely that $m$ can not be extended into a longer alignment.  When indeed
$m$ can not be extended into an alignment of $s_i$ and $s_{i+1}$ of cost less
than $2$, the existence of $m$ was a 'false promise', since crossing the two
seeds takes cost at least $2$. Thus, we can ignore $m$ and remove $m$ from the
heuristic, making the heuristic more accurate.

More generally, we try to extend each match $m$ into an alignment covering seeds
$s_i$ up to (but excluding) $s_{i+q}$ for all $q\leq p=14$. If any of these
extensions has cost at least $q$, that means $m$ falsely promised that $s_i$ to
$s_{i+q}$ can be crossed for cost $<q$, and we /pre-prune/ (remove) $m$.

We try to extend each match by running the diagonal transition algorithm
from the end of each match, and dropping any furthest reaching points that are
at distance $\geq q$ while at most $q$ seeds have been covered.

As shown in [[pre-pruning]]b, the effect is that the number of off-path matches is
significantly reduced.  This makes contours faster to initialize, update, and
query, and increases the value of the heuristic

** Determining the rows to compute
For each block spanning columns $i$ to $i+B$, only a subset of rows is computed in each iteration.
Namely, we only compute those rows that can possibly contain states on a
path/alignment of cost at most $t$.
Intuitively, we try to 'trap' the alignment inside a wall of states that can not lie
on a path of length at most $t$ (i.e. have $\f(u) \geq t$), as can be seen in [[ranges]]a.
We determine this range of rows in two steps:
1. First, we determine the /fixed range/ at the end of the preceding block.
   I.e., we find the topmost and
   bottom-most states $u=\st i{j_{start}}$ and $u=\st i{j_{end}}$ with $f(u) = g(u) + h(u)
   \leq t$. All in-between states $u=\st ij$ with $j_{start}\leq j\leq j_{end}$
   are then /fixed/, meaning that the correct distance has been found and $g(u) = \g(u)$.
2. Then, we use the heuristic to find the bottom-most state $v=\st{i+B}{j_{end}'}$ at the
   end of the to-be-computed block that can possibly have $f(v) \leq t$.
   We then compute rows $j_{start}$ to $j_{end}'$ in columns $i$ to $i+B$,
   rounding $[j_{start}, j_{end}']$ /out/ to the previous/next multiple of the word size $w=64$.

Before describing these two steps in detail, we state the following lemma.

*Step 1: Fixed range.*
In a given column $i$, the /fixed/ range is the range of rows between the topmost and
bottom-most states with $f(u)\leq t$, in rows $j_{start}$ and $j_{end}$
respectively. Suppose that states in rows $[r_{start}, r_{end}]$ were computed.
One way to find $j_{start}$ and $j_{end}$ is by simply iterating inward from the
start/end of the range and dropping all states $v=\st ij$ with
$f(v)=g(v)+h(v)>t$, as indicated by the red columns in [[ranges]]a.

*Step 2: End of computed range.*
We will now determine the bottom-most row $j$ that can contain a state at
distance $\leq t$ at the end of the block. Let $u=\st{i}{j_{end}}$ be the
bottom-most fixed state in column $i$ with distance $\leq t$.  Let $v =
\st{i'}{j'}$ be a state in the current block ($i\leq i'\leq i+B$) that is below
the diagonal of $u$.
Then, the distance to $v$ is at least $g(v) \geq \g(u) + \cgap(u,v)$ (TODO PROOF), and hence
$$
f(v) = g(v) + h(v) \geq \g(u) + \cgap(u,v) + h(v) =: f_l(v).
$$
The end of the range is now computed by finding the bottom-most state $v$ in each
column for which this lower bound $f_l$ on $f$ is at most $t$, using the following
algorithm (omitting boundary checks).
1. Start with $v = \st{i'}{j'} = u = \st{i}{r^t_{end}}$.
2. While the below-neighbour $v' = \st{i'}{j'+1}$ of $v$ has $f_l(v)\leq t$, increment $j'$.
3. Go to the next column by incrementing $i'$ and $j'$ by $1$ and repeat step 2, until $i'=i+B$.
The row $j'_{end}$ of the last $v$ we find in this way is the bottom-most state
in column $i+B$ that can possibly have $f(v)\leq t$, and hence this is end of
the range we compute.

In [[ranges]]a, we see that $f(v)$ is computed at a diagonal of states just below
the bottommost green (fixed) state $u$, and that the to-be-computed range
(indicated in blue) includes exactly all states above the diagonal.

#+name: ranges
#+caption: *Detail of computed ranges.* Coloured states are invocations of $f(u) = g(u) + h(u)$. Red: $f(u) > t$, green: $f(u) \leq t$ and $u$ is fixed, and blue: $f(u)\leq t$, but only tentatively. Vertical black rectangles indicated fixed states, and blue rectangles indicate the range of rows $[j_{start}, j'_{end}]$ that must be computed for each block. The third block has no fixed states in its right column, indicating that $t$ must be increased.
#+attr_html: :class small
| [[./imgs/ranges/full.png]] Simple | [[./imgs/ranges/sparse.png]] Sparse |

*** Sparse heuristic invocation
A drawback of the previous method is that it require a large number of
calls to $f$ and hence the heuristic $h$: roughly one per column and one per row.
Here we present a /sparse/ version that uses fewer calls to $f$, based on the
following lemma:

*Lemma 1.* When $f_l(v) > t + 2d$, then $\f(v') > t$ for any $v$ with
$d(v,v')\leq d$.

*Proof TODO.*

*Sparse fixed range.* To find the first row $j_{start}$ with $f(\st
i{j_{start}})\leq t$, start with $j=r_{start}$, and increment $j$ by
$\lceil(f(v)-t)/2\rceil$ as long as $f(v)>t$, since none of the intermediate
states can lie on a path of length $\leq t$ by Lemma 1. The last row is found in the same
way.  As seen in [[ranges]]b, this sparse variant significantly reduces the number
of evaluations of the heuristic in the right-most columns of each block.

*Sparse end of computed range.*
Lemma 1 inspires the following algorithm ([[ranges]]b). Instead of considering
one column at a time, we now first make a big just down and then jump to the right.
1. Start with $v = \st{i'}{j'} = u+\st{1}{B+1} = \st{i+1}{j_{end} + B+1}$.
2. If $f_l(v) \leq t$, increase $j'$ (go down) by $8$.
3. If $f_l(v) > t$, increase $i'$ (go right) by $\lceil(f_l(v)-t)/2\rceil$, but do not exceed column $i+B$.
4. Repeat from step 2, until $i' = i+B$.
5. While $f_l(v) > t$, decrease $j'$ (go up) by $\lceil(f_l(v)-t)/2\rceil$, but
   do not go above the diagonal of $u$.
The resulting $v$ is again the bottom-most state in column $i+B$ that can
potentially have $f(t)\leq t$, and its row is the last row that will be computed.


** Incremental doubling

#+name: doubling
#+caption: *Incremental doubling detail.* Blue rectangles show the ranges required to be computed, and grey the computed blocks. Vertical green rectangles show the fixed range at the end of each block, and horizontal rectangles a fixed row of states inside some blocks. In both figures the third column was just computed, in the first (left) and second (right) iteration of trying a threshold. The black horizontal rectangle indicates the new candidate for fixed horizontal region.
| [[file:imgs/doubling-0.png]] | [[file:imgs/doubling-1.png]] |

When the original band doubling algorithm doubles the threshold from $t$ to $2t$,
it simply recomputes the distance to all states.  On the
other hand, BFS, Dijkstra, and A* with a consistent heuristic visit
states in increasing order of distance ($g(u)$ for BFS and Dijkstra, $f(u) =
g(u) + h(u)$ for A*), and the distance to a state is known to be correct
(/fixed/) as soon as it is expanded. This way a state is never expanded twice.

Indeed, our band-doubling algorithm can also avoid recomputations. After
completing the iteration for $t$, it is guaranteed that the distance is fixed
to all states that indeed satisfy $f(u)\leq t$.  In fact a stronger result holds:
in any column the distance is fixed for /all/ states between the topmost
and bottom-most state in that column with $f(u)\leq t$.

To be able to skip rows, we must store horizontal differences along
a row so we can continue from there. We choose this row $j_f$ (for /fixed/)
as the last row at a lane boundary before the end of the fixed states
in the last column of the preceding block, as indicated in [[doubling]] by a
horizontal black rectangle. In the first iteration, reusing values is not
possible, so we split the computation of the block into two parts ([[doubling]]a): one above
$j_h$, to extract and store the horizontal differences at $j_h$, and the remainder below $j_h$.

In the second and further iterations, the values at $j_h$ may be
reused and the block is split into three parts. The first part computes all
lanes covering states before the start of the already-fixed range at the end of the block (the
green column at the end of the third column in [[doubling]]b). Then we skip the
lanes up to the previous $j_h$, since the values at both the bottom and right of this
region are already fixed. Then, we compute the lanes between the old $j_h$ and its new
value $j'_h$. Lastly we compute
the lanes from $j'_h$ to the end.

* Results
Our implementation A*PA is written in Rust and available at
[[https://github.com/RagnarGrootKoerkamp/astar-pairwise-aligner][github.com/RagnarGrootKoerkamp/astar-pairwise-aligner]]. We compare it against
other aligners on real datasets, report the impact of the individual
techniques we introduced, and measure time and memory usage.

** Setup
*Datasets.* We benchmark on six datasets containing real sequences of varying
length and divergence, as listed in detail in [[*Comparison with other aligners]].
They can be downloaded from
[[https://github.com/pairwise-alignment/pa-bench/releases/tag/datasets][github.com/pairwise-alignment/pa-bench/releases/tag/datasets]].

Five datasets containing ONT reads are reused from the WFA, BiWFA, and A*PA
evaluations [cite:@wfa;@biwfa;@astarpa]. Of these, the '>500kbp' and '>500kbp with
genetic variation' datasets have divergence $6-7\%$, while three datasets
filtered for sequences of length <1kbp, <10kbp, and <50kbp have divergence $10-12\%$ and
average sequence length $800$bp, $4$kbp, and $11$kbp.

A SARS-CoV-2 dataset was newly generated by downloading 500MB of viral sequences
from the COVID-19 Data Portal, [[https://www.covid19dataportal.org/][covid19dataportal.org]] [cite:@covid19portal],
filtering out non-ACTG characters, and selecting 10000 random pairs. This
dataset has average divergence $1.5\%$ and length $30$kbp.

For each set, we sorted all sequence pairs by edit distance and split them
into $50$ files each containing multiple pairs, with the first file containing the
$2\%$ of pairs with the lowest divergence. Reported results are averaged over
the sequences in each file.

*Algorithms and aligners.*
We benchmark two versions of A*PA2 against state-of-the-art aligners Edlib,
BiWFA, and A*PA.
Version /A*PA2-simple/ uses all engineering optimizations (bitpacking, SIMD,
blocks, new traceback) and uses the simple gap-heuristic.
/A*PA2-full/ additionally uses more complicated techniques:
incremental-doubling, and the gap-chaining seed heuristic introduced by
A*PA with pre-pruning.

*Parameters.*
For A*PA2, we fix block size $B=256$. For A*PA2-full, we use the gap-chaining seed
heuristic (GCSH) of A*PA with exact matches ($r=1$) and seed length $k=12$. We
pre-prune matches by looking ahead up to $p=14$ seeds.
A detailed parameter comparison can be found in [[*Comparison with other aligners]].
For A*PA, we use inexact matches ($r=2$) with seed length $k=15$ by default, and
only change this for the low-divergence SARS-CoV-2 dataset, where we use exact
matches ($r=1$) intead.

*Execution.*
We ran all benchmarks using PaBench ([[https://github.com/pairwise-alignment/pa-bench][github.com/pairwise-alignment/pa-bench]]) on
Arch Linux on an =Intel Core i7-10750H= with $64$GB of memory and $6$ cores,
with hyper-threading disabled, frequency boost disabled, and CPU power saving
features disabled.  The CPU frequency is fixed to $3.3$GHz and we run $1$
single-threaded job at a time with niceness $-20$. Reported running times are
the average wall-clock time per alignment and do not include the time to read
data from disk. For A*PA2-full, reported times do include the time to find matches and
initialize the heuristic.

** Comparison with other aligners
*Speedup on real data.*
[[real-summary]] compares the running time of aligners on real datasets, and
[[real-table]] shows average running times more precisely.
For long ONT reads, with $6\%-7\%$ divergence, A*PA2-full is $20\times$ faster
than Edlib, BiWFA, and A*PA in average running time, and using the gap-chaining
seed heuristic in A*PA2-full provides speedup over A*PA2-simple.

On shorter sequences, the overhead of initializing the heuristic in A*PA2-full is large, and
A*PA2-simple is faster. For the <10kbp and <50kbp datasets ($4$kbp and $11$kbp
average length), A*PA2-simple is $3.7\times$ and $6.1\times$ faster than other methods.
For the shortest (<1kbp ONT reads) and most similar sequences (SARS-CoV-2
with $1\%$ divergence), BiWFA is usually faster than Edlib and A*PA2-simple. In these cases,
the overhead of using $256$ wide blocks is relatively large compared to the
edit distance $s\leq 500$ in combination with BiWFAs $O(s^2+n)$ expected running time.

#+name: real-summary
#+caption: *Runtime comparison (log).* Each dot shows the running time of a single alignment (right two plots) or the average runtime over $2\%$ of the input pairs (left four plots). Box plots show the three quartiles, and the red circled dot shows the average running time over all alignments. Dots on the red line indicate timeouts. On the >500kbp reads, A*PA2-full is $20\times$ faster than other methods.
#+attr_html: :class inset large
[[file:plots/real-summary.svg]]

#+name: real-table
#+caption: *Average runtime per sequence* of each aligner on each dataset. Cells marked with $>$ are a lower bound due to timeouts. Speedup is reported as the fastest A*PA2 variant compared to the fastest of Edlib, BiWFA, and A*PA.
#+attr_html: :class small
|              | SARS-CoV-2 pairs (ms) | <1kbp ONT reads  (ms) | <10kbp ONT reads  (ms) | <50kbp ONT reads  (ms) | >500kbp ONT reads (s) | >500kbp ONT reads + gen.var. (s) |
|--------------+-----------------------+-----------------------+------------------------+------------------------+-----------------------+-------------------------------------------|
| Edlib        |                 11.56 |                 0.122 |                   1.17 |                    8.2 |                  3.72 |                                      5.17 |
| BiWFA        |                  1.51 |                 0.049 |                   1.02 |                   12.9 |                  6.29 |                                      9.71 |
| A*PA         |                  6.84 |                 0.546 |                  15.22 |                  203.2 |                 14.14 |                                     13.53 |
| A*PA2 simple |                  0.85 |                 0.051 |                   0.27 |                    1.3 |                  0.52 |                                      0.69 |
| A*PA2 full   |                  1.98 |                 0.082 |                   0.40 |                    1.6 |                  0.19 |                                      0.25 |
| Speedup      |             1.8\times |            0.96\times |              3.7\times |              6.1\times |              20\times |                                  21\times |

*Scaling with divergence.*
[[scaling-e]] compares the runtime of aligners on synthetic sequences of increasing
divergence. BiWFA's runtime grows quadratically, while Edlib grows
linearly and jumps up each time another doubling of the threshold is required.
A*PA is fast until the maximum potential is reached at $6\%$ resp. $12\%$ and
then becomes very slow. A*PA2 behaves similar to Edlib and jumps up each time
another doubling of the threshold is needed, but is much faster.
It outperforms BiWFA for divergence $\geq 2\%$ and A*PA for divergence
$\geq 4\%$.
The runtime of A*PA2-full is near-constant up to divergence $7\%$ due to the
gap-chaining seed heuristic which can correct for up to $1/k=1/12=8.3\%$ of divergence, while
A*PA2-simple starts to slow down because of doubling at lower divergence.
For a fixed number of doublings of the threshold, A*PA2 is faster for higher
divergence because too low thresholds are rejected more quickly.


On real data, A*PA and BiWFA slow down as divergence goes up, while Edlib and
A*PA2 are much less sensitive to this for sequences of length <50kbp, as shown
in [[*Comparison with other aligners]].

#+name: scaling-e
#+caption: *Runtime scaling with divergence.* Average running time of aligners over $10$ sequences of length $100$kbp with varying uniform divergence. The right plot is the same but zoomed in.
#+attr_html: :class inset large
| [[file:plots/scaling_e.svg]] | [[file:plots/scaling_e_zoom.svg]] |


*Memory usage* of A*PA2 is around $30$MB on average and at most
$200$MB when aligning >500kbp sequences, and always less than $10$MB for all
shorter sequences ([[*Comparison with other aligners]]).

** Effects of methods

*Incremental improvements.*
[[real-incremental]] shows the effect of one-by-one adding improvements to A*PA2 on
>500kbp long sequences, starting with Ukkonens band-doubling method using Myers'
bitpacking. We first change to the $\g(u) + \cgap(u, v_t)$ domain, making it
comparable to Edlib. Then we process blocks of $256$ columns at a time and only
store differences at block boundaries giving $\approx 2\times$ speedup. Adding
SIMD gives another $\approx 3\times$ speedup, and instruction level parallelism
(ILP) provides a further small improvement. The diagonal transition traceback
(DTT) and sparse heuristic computation do not improve performance of
A*PA2-simple much on long sequences, but their removal can be seen to slow it
down for shorter sequences in [[real-ablation]].

Incremental doubling (ID), the gap-chaining seed heuristic (GCSH), pre-pruning
(PP), and the pruning of A*PA give another $2\times$ speedup on average and
$3\times$ speedup in the first quantile.

#+name: real-incremental
#+caption: *Effect of adding features.* Box plots showing the performance improvements of A*PA2 when incrementally adding new methods one-by-one. A*PA2-simple corresponds to teh rightmost red columns, and A*PA2-full corresponds to the rightmost blue column.
#+attr_html: :class inset large
[[file:plots/real-incremental.svg]]


*Runtime profile.* In [[real-timing]] we see that for >500kbp long sequences,
A*PA2-full spends most of its time computing blocks, followed by the
initialization of the heuristic. For shorter sequences the heuristic is not
used, and for very short sequences <10kbp, up to half the time is spent on
tracing the optimal alignment.

#+name: real-timing
#+caption: *Runtime distribution per stage of A*PA2,* using A*PA2-full for >500kbp sequences in the left two plots and A*PA2-simple in the remaining four plots. Each column corresponds to a (set of) alignment(s), which are sorted by total runtime. /Overhead/ is the part of the runtime not measured in one of the other parts and includes the time to build the profile.
#+attr_html: :class inset large
[[file:plots/real-timing.svg]]
* Discussion
We have shown that A*PA2 is over $20\times$ faster than other methods when aligning $>500$kbp
ONT reads with $6\%$ divergence, $2$ to $6\times$ faster for sequences of length
$4$ kbp to $50$ kbp, and only slightly slower than BiWFA for very short ($<1000$ bp) and
very similar ($<2\%$ divergence) sequences.
A*PA2 achieves this by building on Edlib, using bitpacking, blocks, SIMD, the
gap-chaining seed heuristic, and pre-pruning.

*Limitations.*
1. The main limitation of A*PA2-full is that the heuristic requires finding all
   matches between the two input sequences, which can take long compared to the
   alignment itself.
2. For sequences with divergence $<2\%$, BiWFA exploits the
   sparse structure of the diagonal transition algorithm. In comparison, computing full
   blocks of size around $256\times 256$ in A*PA2 has considerable overhead.
3. Only sequences over alphabet size $4$ are currently supported, so DNA
   sequences containing e.g. =N= characters must be cleaned first.
*Future work.*
1. When divergence is low, performance could be improved by applying A* to the
   diagonal transition algorithm directly, instead of using DP. As a middle
   ground, it may be possible to compute individual blocks using DT when the
   divergence is low.
2. Currently A*PA2 is completely unaware of the type of sequences it aligns.
   Using an upper bound on the edit distance, either known or found using a
   non-exact method, could avoid trying overly large thresholds and smoothen the
   curve in [[scaling-e]].
3. It should be possible to extend A*PA2 to open-ended and semi-global
   alignment, just like Edlib and WFA support these modes.
4. Extending A*PA2 to affine cost models should also be possible. This will
   require adjusting the gap-chaining seed heuristic, and changing the
   computation of the blocks from a bitpacking approach to one of the
   SIMD-based methods for affine costs.
5. Lastly, TALCO (Tiling ALignment using COnvergence of traceback pointers,
   https://turakhia.ucsd.edu/research/) provides an interesting idea: it may be
   possible start traceback while still computing blocks, saving memory.

* Acknowledgements
:PROPERTIES:
:UNNUMBERED: t
:END:

I am grateful to Daniel Liu for discussions, feedback, and suggesting additional
papers relating to this topic, to Andrea Guarracino and Santiago Marco-Sola for
sharing the WFA and BiWFA benchmark datasets, and to Gary Benson for help with
debugging the BitPAl bitpacking code.  RGK is financed by ETH Research Grant
ETH-1721-1 to Gunnar Rätsch.

* Conflict of interest
None declared.

* Appendix
** BitPAl
[[bitpal]] shows a SIMD version of the edit distance bitpacking scheme explained
in the supplement of [cite/t:@bitpal]. Like Myers method it requires $20$ instructions.

We note that the method in BitPAl is reported as requiring $15$ instructions,
but those exclude the shifting out of the bottom horizontal difference (four
instructions) and the initialization of the carry (one operation). We require
these additional outputs/inputs since we want to align multiple $64$bit lanes
below each other, and the horizontal difference in between must be carried
through.

#+name: bitpal
#+caption: Rust code for SIMD version of BitPAl's bitpacking. Computes four independent words on an antidiagonal in parallel in $20$ instructions.
#+begin_src rust
pub fn compute_block_simd_bitpal(
    hz0: &mut Simd<u64, 4>,  // 0 or 1. Indicates 0 difference on top.
    hp0: &mut Simd<u64, 4>,  // 0 or 1. Indicates -1 difference on top.
    vm:  &mut Simd<u64, 4>,  // 64-bit indicator of -1 differences on left.
    vmz: &mut Simd<u64, 4>,  // 64-bit indicator of -1 and 0 differences on left.
    eq: Simd<u64, 4>,  // 64-bit indicator which characters equal the top char.
) {
    let eq = eq | *vm;
    let ris = !eq;
    let notmi = ris | *vmz;
    let carry = *hp0 | *hz0;
    // The addition carries information between rows.
    let masksum = (notmi + *vmz + carry) & ris;
    let hz = masksum ^ notmi ^ *vm;
    let hp = *vm | (masksum & *vmz);
    // Extract the high bit as bottom horizontal difference.
    let right_shift = Simd::<u64,4>::splat(63);
    let hzw = hz >> right_shift;
    let hpw = hp >> right_shift;
    // Insert the top horizontal difference.
    let left_shift = Simd::<u64,4>::splat(1);
    let hz = (hz << left_shift) | *hz0;
    let hp = (hp << left_shift) | *hp0;
    // Update the input-output parameters.
    *hz0 = hzw;
    *hp0 = hpw;
    *vm = eq & hp;
    *vmz = hp | (eq & hz);
}
#+end_src

** Comparison with other aligners
Here we provide further results on the comparison of aligners.

*Dataset statistics.* Detailed statistics on the datasets are provided in [[statistics]].
The ONT (Oxford Nanopore Technologies) read sets all have high $6\%-12\%$ divergence, and
the set with genetic variation (gen.var.) contains long gaps.
The SARS-CoV-2 dataset stands out for having only $1.5\%$ divergence.

#+name: statistics
#+caption: Statistics of the real datasets. Lengths are in kbp, divergence in %. Max gap indicates the average length of the largest gap in each alignment.
| Dataset              | Source | #Pairs | len min | len mean | len max | div min | div mean | div max | max gap mean | max gap max |
| SARS-CoV-2           | A*PA2  |  10000 |      27 |       30 |      30 |     0.0 |      1.5 |    12.8 |          0.1 |         1.0 |
| ONT <1k              | WFA    |  12477 |    0.04 |      0.8 |     1.1 |     0.0 |     10.4 |    22.5 |         0.01 |         0.1 |
| ONT <10k             | BiWFA  |   5000 |     0.2 |      3.6 |      10 |     3.0 |     12.1 |    20.1 |         0.04 |         0.5 |
| ONT <50k             | BiWFA  |  10000 |     0.2 |       11 |      50 |     3.0 |     11.6 |    19.2 |         0.07 |         3.4 |
| ONT >500k            | A*PA   |     50 |     500 |      594 |     849 |     2.7 |      6.1 |    16.7 |          0.1 |         1.3 |
| ONT >500k + gen.var. | BiWFA  |     48 |     502 |      632 |    1053 |     4.3 |      7.2 |    18.2 |        *1.9* |          42 |

*Real data scaling by divergence.* [[real-summary-scatter]] shows the same data as [[real-summary]], but split out by divergence.

#+name: real-summary-scatter
#+caption: Scatter plot of runtime of aligners. Each dot shows the average divergence and runtime of the corresponding set of sequences.
#+attr_html: :class inset large
[[file:plots/real-summary-scatter.svg]]

*Memory usage.* [[real-memory]] shows the memory usage of all compared aligners.

#+name: real-memory
#+caption: Memory usage of aligners, measured as the increase in =max_rss= before and after aligning a pair of sequences. These measurements are rather unstable, so should only be taken as an indication of the order of magnitude.
#+attr_html:
| Memory [MB]  |   SARS-CoV-2 pairs Median |   SARS-CoV-2 pairs Max |   <1kbp ONT reads Median |   <1kbp ONT reads Max |   <10kbp ONT reads Median |   <10kbp ONT reads Max |   <50kbp ONT reads Median |   <50kbp ONT reads Max |   >500kbp ONT reads Median |   >500kbp ONT reads Max |   >500kbp ONT reads + gen.var. Median |   >500kbp ONT reads + gen.var. Max |
|--------------+---------------------------+------------------------+--------------------------+-----------------------+---------------------------+------------------------+---------------------------+------------------------+----------------------------+-------------------------+---------------------------------------+------------------------------------|
| Edlib        |                         0 |                      0 |                        0 |                     0 |                         0 |                      0 |                         0 |                      0 |                          0 |                       0 |                                     0 |                                  0 |
| BiWFA        |                         0 |                      0 |                        0 |                     0 |                         0 |                      0 |                         0 |                      0 |                          5 |                      12 |                                     0 |                                  0 |
| A*PA         |                         0 |                    237 |                        0 |                     0 |                         0 |                     43 |                       181 |                    873 |                         85 |                    3454 |                                   160 |                               6870 |
| A*PA2 simple |                         2 |                      5 |                        0 |                     0 |                         0 |                      0 |                         4 |                      6 |                          0 |                      57 |                                     4 |                                166 |
| A*PA2 full   |                         0 |                      0 |                        0 |                     0 |                         0 |                      0 |                         0 |                      0 |                         31 |                      83 |                                     7 |                                143 |

** Effects of methods
*Ablation.* [[real-ablation]] shows how the performance of A*PA2 changes as individual features are removed.
#+name: real-ablation
#+caption: *Ablation.* Box plots showing how the performance of A*PA2-simple and A*PA2-full changes when removing features.
#+attr_html: :class inset large
[[file:plots/real-ablation.svg]]

*Parameters.* [[real-params]] compares A*PA2 with default parameters against versions where one of the
parameters is modified. As can be seen, running time is not very sensitive with
regards to most parameters. Of note are using inexact matches ($r=2$) for the
heuristic, which take significantly longer to find, larger seed length $k$, which decreases the strength of the heuristic, and
smaller block sizes ($B=128$ and $B=64$), which induce more overhead.

#+name: real-params
#+caption: *Changing parameters.* Running time of A*PA2-simple (left, middle) and A*PA2-full (right) with one parameter modified. Default parameters are
#+caption: seed length $k=12$, pre-pruning look-ahead $p=14$, growth factor $f=2$, block size
#+caption: $b=256$, max traceback cost $g=40$, and dropping diagonals that lag $fd=10$ behind during traceback.
#+attr_html: :class inset large
[[file:plots/real-params.svg]]

* TODO
- Rerun with =debug = false=, =strip = true=
- update tables
- no 'cell'
- subcaptions
- range proofs
- intro papers
- integrate intro figs

#+print_bibliography:
