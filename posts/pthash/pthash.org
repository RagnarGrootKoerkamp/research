#+title: PTHash: Notes on implementing PTHash in Rust [ongoing]
#+HUGO_SECTION: notes
#+hugo_tags: mphf
#+HUGO_LEVEL_OFFSET: 1
#+OPTIONS: ^:{}
#+hugo_front_matter_key_replace: author>authors
#+toc: headlines 3
#+date: <2023-09-21>
#+author: Ragnar Groot Koerkamp

Daniel got me excited about minimal perfect hashing functions (MPHFs), and then
[[https://twitter.com/nomad421/status/1701593870734336290][+twitter+ Rob]] asked for rust a implementation of PTHash [cite:@pthash], and also
I have some ideas related to memory prefetching I want to play with, so here we
are: I'm working on a Rust implementation ~pthash-rs~ at [[https://github.com/ragnargrootkoerkamp/pthash-rs]].

This post is just to collect random thoughts and questions that come up while
writing the code.

Twitter discussion is [[https://twitter.com/curious_coding/status/1704989305158979656][here]].

(See also my [[file:bbhash.org][note on BBHash]] [cite:@bbhash].)

* Questions and remarks on PTHash paper
Probably some of these questions are answered by reading into the original FCH
paper [cite:@fch] -- that'll come at some later time.
- FCH uses $cm/\log_2 n$ buckets because each bucket needs $\log_2 n$ bits
  storage, but for PTHash the bits per bucket is lower so this isn't really the
  right formula maybe.
  - [[https://twitter.com/giulio_pibiri/status/1705114424787308718][Tweet]]: another option is to fix $c = \alpha * \log(n) / d$ for some constant
    $d$.
- FCH uses $\lceil cn / (\log_2n+1)\rceil$ buckets, but PTHash uses $\lceil
  cn/\log_2 n\rceil$ buckets. To me, $\lceil cn/\lceil \log_2n\rceil\rceil$
  actually sounds most intuitive.
- $p_1=0.6n$ and $p_2=0.3m$ seem somewhat arbitrary. Can this be tuned better?
  Could partitioning into more chunks help?

  Anyway, this puts $60\%$ of data in the first $30\%$ of buckets (i.e. double
  the average size), and $40\%$ of data in $70\%$ of buckets (i.e. roughly half
  the average size).

  I suppose the goal is to stitch two binomial distributions of bucket sizes
  together in a nice way to get a more even distribution over sizes.
  - This is an open question ([[https://twitter.com/giulio_pibiri/status/1705112904779915662][tweet]])
    - Proof in appendix of [[https://jermp.github.io/assets/pdf/papers/TKDE2023.pdf][paper]] could possibly be extended.

* Ideas for improvement
** Parameters
- Better tuning of parameters may work?
- Partitioning into three or more chunks may speed up construction?

** Align packed vectors to cachelines
Instead of packing $r$ bit values throughout the entire vector, only pack them
inside cachelines.

** Prefetching
The query performance of this and other MPHFs seem to be limited by the latency
of memory lookups. For this reason, most MPHFs minimize the number of memory
lookups, and in particular PTHash uses one random memory access followed by one
lookup in a small (cached) dictionary.

When processing a number of lookups sequentially, each lookup currently incurs a
cache miss. This could be made much more efficient by doing $k$ (e.g. $k=16$) cache
lookups in parallel:
1. first compute the array location to lookup for $k$ keys (possibly using SIMD),
2. then prefetch the $k$ memory locations,
3. then do the rest of the computation in parallel.
4. If needed, do lookups in the $free$ table in parallel.

This should mostly hide the memory latency and could give significant speedup.
I'm not quite sure yet whether this would work best when done in batches (as
described), or in streaming fashion, where we iterate over elements and prefetch
memory for the element $k$ iterations ahead. [[https://en.algorithmica.org/hpc/cpu-cache/prefetching/][Algorithmica.org]] has a nice article
on the streaming approach.

** Faster modulo operations
There are a lot of ~% n~, ~% p1~ and ~% (m-p1)~ operations throughout the code.
I didn't look into it yet, but possibly these are the bottleneck on the CPU
latency.

First note that
$$
a\, \%\, b = a - \lfloor a/b\rfloor * b.
$$
This division by a constant can be computed efficiently using a trick which
replaces division by multiplication with the inversion.
Using the formula of the [[https://en.wikipedia.org/wiki/Division_algorithm#Division_by_a_constant][wikipedia article]] we can precompute some constants to
evaluate $\lfloor a/b\rfloor$ in $6$ operations and ~a % b~ in $8$ operations.

+(Note that it might be possible compilers already do this, but I don't expect so.)+

Some blogposts by Daniel Lemire ([[https://twitter.com/daniel_c0deb0t/status/1704999240802636051][Thanks Daniel Liu]] ;)
- [[https://lemire.me/blog/2016/06/27/a-fast-alternative-to-the-modulo-reduction/][A fast alternative to the modulo reduction]]

  Instead of ~a % b~, compute ~a * b >> 64~, assuming that $a$ is uniform in
  $[2^{64}-1]$.

  This doesn't seem to work well in practice though for PTHash, probably since
  this only uses the entropy in the high-order bits of $a$.
- [[https://lemire.me/blog/2019/02/08/faster-remainders-when-the-divisor-is-a-constant-beating-compilers-and-libdivide/][Faster remainders when the divisor is a constant: beating compilers and libdivide]]

  *Indeed, the C++ PTHash implementation [[https://twitter.com/giulio_pibiri/status/1705104355270037980][already uses]] the =fastmod= library.*

- [[https://lemire.me/blog/2019/02/20/more-fun-with-fast-remainders-when-the-divisor-is-a-constant/][More fun with fast remainders when the divisor is a constant]]
- [cite/t:@fast-remainder]

* Implementation log
A somewhat chronological list of notes and remarks.
** Hashing function
For now I use =murmur64a=.
** Bitpacking crates
There are *a lot* of bitvector and bitpacking crates!
- [[https://crates.io/search?q=bitvec][bitvectors]] :: All of the below seem to do the same
  - =bitvec=: $30M$ downloads
  - =bit-vec=: $30M$ downloads
  - =fixedbitset=: $55M$ downloads
  No idea which is best; probably I'll settle for the one below in =sucds=.
- [[https://crates.io/crates/sucds][sucds]] :: only $60K$ downloads, but contains
  - [[https://docs.rs/sucds/latest/sucds/bit_vectors/bit_vector/struct.BitVector.html][BitVector]]
  - fixed-width integer packing: [[https://docs.rs/sucds/latest/sucds/int_vectors/compact_vector/struct.CompactVector.html][CompactVector]]
  - increasing-integer sequence packing: [[https://docs.rs/sucds/latest/sucds/mii_sequences/index.html][EliasFano]]
    - Giulio has [[https://github.com/jermp/data_compression_course][lecture notes]] on this.

** Construction
- Storing buckets as ~Vec<Vec<Key>>~ is bad for large keys, so now I store
  ~Vec<Vec<usize>>~, but the nested ~Vec~s still waste a lot of space and will
  cause allocation slowdowns. PTHash pushes onto a vector which is sorted later,
  which seems more efficient.
- When testing $k_i$, not only do we need to test that positions are not filled
  by previous buckets, but also we have to check that elements within the bucket
  do not collide. *It is not sufficient that $h(x, s)$ does not collide within
  buckets,* since they could collide after taking the ~% n~.

** Fastmod
It seems that Daniel Lemire's =fastmod= C++ library has not yet been ported to
Rust, so I converted the few parts I need.

There is also [[https://crates.io/crates/strength_reduce][=strength_reduce=]], which contains a similar but distinct algorithm
for ~a % b~ that computes the remainder from the quotient.

** First benchmark
I [[https://github.com/RagnarGrootKoerkamp/pthash-rs/commit/c070936558e756bafaae92af5be31ac383f2c3ee][implemented]] these under a generic =Reduce= trait.

~just bench~ at the linked commit at ~2.6GHz~ gives the following for $10^7$ keys:

| method           | construction (s) | query (ns) |
| u64              |        10.474591 |         91 |
| fastmod64        |        10.731583 |         55 |
| fastmod32        |         9.911751 |         *50* |
| strengthreduce64 |        11.520939 |         56 |
| strengthreduce32 |        10.002017 |         *50* |

The =u32= versions simply only use the lower $32$ bits of the $64$ bit hash.

This is not yet as fast as the fastest =28ns= reported in the PTHash paper (for
C-C encoding), but I also haven't optimized anything else yet. Time for profiling.

*Profiling:* Looking at the flamegraph (~cargo flamegraph~), and zooming in on the hash function, we see

#+attr_html: :class inset
[[file:hash_flame.png]]

A lot of time is spend on fold! The ~murmur2~ function I use has signature
~murmur2(bytes: &[u8], seed: u64)~, and even though my keys/bytes always correspond
to just a ~u64~, it's iterating over them!

In the generated ~perf report~, we see
#+begin_src txt
  33.14%         27328  test::queries_e  pthash_rs-f15b4648f77f672b           [.] pthash_rs::PTHash<P,R>::new
  18.18%         14823  test::queries_e  pthash_rs-f15b4648f77f672b           [.] pthash_rs::PTHash<P,R>::index
  13.76%         11245  test::queries_e  pthash_rs-f15b4648f77f672b           [.] murmur2::murmur64ane
#+end_src
We can ignore the $33\%$ for construction and only focus on querying here, where
we see that the =index= function calls to =murmur2= and a lot of time is spent
in both. In fact, =murmur2= is not inlined at all! That explains the iterator
appearing in the flamegraph.

*Thin-LTO:* This is fixed by [[https://github.com/RagnarGrootKoerkamp/pthash-rs/commit/4b25317bf4c78bc1264f88b0592af2c08de54044][enabling]] link-time optimization: add ~lto = "thin"~ to
~cargo.toml~.

Rerunning the benchmark we get

|                  | construction (s) | construction (s) | query (ns) | query (ns) |
| method           |           no LTO |         thin-LTO | no LTO     |   thin-LTO |
| u64              |             10.5 |              8.9 | 91         |         60 |
| fastmod64        |             10.7 |              8.3 | 55         |         34 |
| fastmod32        |              9.9 |              8.5 | *50*       |       *26* |
| strengthreduce64 |             11.5 |              8.3 | 56         |         38 |
| strengthreduce32 |             10.0 |              8.8 | *50*       |         31 |

Sweet! =26ns= is faster than any of the numbers in table 5 of [cite/t:@pthash]!
(Admittedly, there is no compression yet and the dictionary size is $10\times$
smaller, but still!)

*More inlining:*
Actually, we don't even want the =index()= function call to show up in our logs:
[[https://github.com/RagnarGrootKoerkamp/pthash-rs/commit/39a3411332f70bde37de90221c9f460bd8b79f9a][inlining]] it should give better instruction pipelining in the benchmarking hot-loop
#+begin_src rust
for key in &keys {
    mphf.index(key);
}
#+end_src
and indeed, we now get
| query (ns)       | no LTO | thin-LTO | inline index() |
| u64              |     91 |       60 |             55 |
| fastmod64        |     55 |       34 |             33 |
| fastmod32        |   *50* |     *26* |           *24* |
| strengthreduce64 |     56 |       38 |             33 |
| strengthreduce32 |   *50* |       31 |             26 |


*Conclusion:* From now on let's only use =fastmod64= and =fastmod32=. (I suspect
the =32bit= variant does not have sufficient entropy for large key sets, so we
keep the original =64bit= variant as well.)

** Faster bucket computation

After inlining everything, the generated assembly for our test is just one big
$\sim 100$ line assembly function. Currently, the ~bucket(hx)~ function (that
computes the bucket for the given hash ~hx = hash(x, s)~) looks like
#+begin_src rust
fn bucket(&self, hx: u64) -> u64 {
    if (hx % self.rem_n) < self.p1 { // Compare
        hx % self.rem_p2
    } else {
        self.p2 + hx % self.rem_mp2
    }
}
#+end_src
The assembly looks like this:
#+begin_src asm
     5 │    ┌──cmp        %rdi,0xc0(%rsp)       # compare
     1 │    ├──jbe        370
   102 │    │  mov        0x98(%rsp),%rdx       # first branch: fastmod
    41 │    │  mulx       %r9,%rdx,%rdi
     4 │    │  imul       0xa0(%rsp),%r9
    85 │    │  mulx       %r10,%r13,%r13
    12 │    │  add        %rdi,%r9
     7 │    │  mov        %r9,%rdx
    72 │    │  mulx       %r10,%rdx,%rdi
    17 │    │  add        %r13,%rdx
     3 │    │  adc        $0x0,%rdi             # add 0
       │    │  cmp        %rdi,0x58(%rsp)       # index-out-of-bound check for k array
    56 │    │↓ ja         3ac                   # ok: continue below at line 3ac:
       │    │↓ jmp        528                   # panic!
       │    │  cs         nopw 0x0(%rax,%rax,1)
   128 │370:└─→mov        0xa8(%rsp),%rdx       # second branch: fastmod
    41 │       mulx       %r9,%rdx,%rdi
       │       imul       0xb0(%rsp),%r9
    66 │       mulx       %rcx,%r13,%r13
    12 │       add        %rdi,%r9
       │       mov        %r9,%rdx
    58 │       mulx       %rcx,%rdx,%rdi
    14 │       add        %r13,%rdx
       │       adc        0xb8(%rsp),%rdi       # add p2
    54 │       cmp        %rdi,0x58(%rsp)       # out-of-bound check for k array
     1 │     ↓ jbe        528                   # panic!
  8100 │3ac:   mov        (%r11,%rdi,8),%rdx    # Do array index.
#+end_src
We see that there are quite some branches:
- The first and second branch of the ~bucket()~ function are both fully written out.
- They use the same number of instructions.
- One branch does =add 0=, I suppose because the CPU likes equal-sized branches.
- There are redundant index-out-of-bounds checks.
- The last line, the array index itself, has $8000$ samples: $57\%$ of the total
  samples is *this single assembly instruction*!

*Branchless bucket index:*
I tried rewriting the ~bucket()~ function into a branchless form as follows:
#+begin_src rust
fn bucket(&self, hx: u64) -> u64 {
    let is_large = (hx % self.rem_n) >= self.p1;
    let rem = if is_large { self.rem_mp2 } else { self.rem_p2 };
    is_large as u64 * self.p2 + hx % rem
}
#+end_src
but this turns out to be *slower* than the original, probably because the new
assembly now needs a lot of =cmov= instructions. (In particular, =rem= contains
a =u128= and a =u64=, so needs $3$ =mov='s and $3$ =cmov='s.)
#+begin_src asm
       │       cmp        %rdi,0xd0(%rsp)     # comparison
   112 │       mov        0xb8(%rsp),%rdi     # load rem_p2
    29 │       cmova      0xc8(%rsp),%rdi     # conditionally overwrite with rem_mp2
       │       mov        0xb0(%rsp),%rdx
   137 │       cmova      0xc0(%rsp),%rdx
       │       mov        0xa0(%rsp),%r14
   137 │       cmova      0xa8(%rsp),%r14
    26 │       mov        %r9,%r10
       │       mov        $0x0,%r11d          # set offset to 0
    90 │       cmova      %r11,%r10           # conditionally overwrite offset
    38 │       imul       %r8,%rdi            # start computation
     2 │       mulx       %r8,%rdx,%r8
   122 │       add        %r8,%rdi
    48 │       mulx       %r14,%r8,%r8
       │       mov        %rdi,%rdx
   163 │       mulx       %r14,%rdx,%rdi
       │       add        %r8,%rdx
       │       adc        %r10,%rdi
   184 │       cmp        %rdi,0x60(%rsp)     # index-out-of-bounds check
       │     ↓ jbe        52f                 # panic
    38 │       mov        0x98(%rsp),%rdx
 10798 │       mov        (%rdx,%rdi,8),%rdx  # Do array index.
#+end_src

*No bounds check:*
We can replace ~k[index]~ by ~unsafe { *k.get_unchecked(index) }~.
This doesn't give much performance gain (less than the few ~ns~ of measurement
noise I have), but can't hurt. It removes the final =cmp; jbe= lines from the assembly.

*Fix tests:* Instead of ignoring test results we can accumulate the resulting
indices and pass them to =black_box(sum)=. This prevents the compiler from
optimizing away all queries. /Somehow/ this affects the reported timings. I now get:

| query (ns)       | no LTO | thin-LTO | inline index() | fixed tests |
| u64              |     91 |       60 |             55 |          63 |
| fastmod64        |     55 |       34 |             33 |          35 |
| fastmod32        |   *50* |     *26* |           *24* |        *20* |
| strengthreduce64 |     56 |       38 |             33 |          38 |
| strengthreduce32 |   *50* |       31 |             26 |          30 |

I'm confused how the =fastmod32= timing went down, but the =fastmod64= went up.
(Typical situation when you do constant profiling and there are more numbers
than you can make sense of, sadly.)

#+print_bibliography:
