#+title: PTHash: Notes on implementing PTHash in Rust [ongoing]
#+HUGO_SECTION: notes
#+hugo_tags: mphf
#+HUGO_LEVEL_OFFSET: 1
#+OPTIONS: ^:{}
#+hugo_front_matter_key_replace: author>authors
#+toc: headlines 3
#+date: <2023-09-21>
#+author: Ragnar Groot Koerkamp

$$
%\newcommand{\mm}{\,\%\,}
\newcommand{\mm}{\bmod}
$$

Daniel got me excited about minimal perfect hashing functions (MPHFs), and then
[[https://twitter.com/nomad421/status/1701593870734336290][+twitter+ Rob]] asked for rust a implementation of [[https://github.com/jermp/pthash][PTHash]] [cite:@pthash], and also
I have some ideas related to memory prefetching I want to play with, so here we
are: I'm working on a Rust implementation ~pthash-rs~ at [[https://github.com/ragnargrootkoerkamp/pthash-rs]].

This post is just to collect random thoughts and questions that come up while
writing the code.

Twitter discussion is [[https://twitter.com/curious_coding/status/1704989305158979656][here]].

(See also my [[file:bbhash.org][note on BBHash]] [cite:@bbhash].)

* Questions and remarks on PTHash paper
Probably some of these questions are answered by reading into the original FCH
paper [cite:@fch] -- that'll come at some later time.
- FCH uses $cm/\log_2 n$ buckets because each bucket needs $\log_2 n$ bits
  storage, but for PTHash the bits per bucket is lower so this isn't really the
  right formula maybe.
  - [[https://twitter.com/giulio_pibiri/status/1705114424787308718][Tweet]]: another option is to fix $c = \alpha * \log(n) / d$ for some constant
    $d$.
- FCH uses $\lceil cn / (\log_2n+1)\rceil$ buckets, but PTHash uses $\lceil
  cn/\log_2 n\rceil$ buckets. To me, $\lceil cn/\lceil \log_2n\rceil\rceil$
  actually sounds most intuitive.
- $p_1=0.6n$ and $p_2=0.3m$ seem somewhat arbitrary. Can this be tuned better?
  Could partitioning into more chunks help?

  Anyway, this puts $60\%$ of data in the first $30\%$ of buckets (i.e. double
  the average size), and $40\%$ of data in $70\%$ of buckets (i.e. roughly half
  the average size).

  I suppose the goal is to stitch two binomial distributions of bucket sizes
  together in a nice way to get a more even distribution over sizes.
  - This is an open question ([[https://twitter.com/giulio_pibiri/status/1705112904779915662][tweet]])
    - Proof in appendix of [[https://jermp.github.io/assets/pdf/papers/TKDE2023.pdf][paper]] could possibly be extended.

- How many bits of entropy are really needed? When $n$ is a power of $2$, we
  make sure to use all $64$ bits instead of just $\log_2 n$. But how many are
  really needed. Maybe as long as $n \ll 2^{32}$, a simpler/faster $32$ bit hash
  is sufficient?

- And how large will $n$ be at most? Can we reasonably assume $2^{32} \approx
  4\cdot 10^9$ as a bound?

- Is it reasonable to compute and store hashes of all keys, and then changing
  the API to let the user pass in the hash instead of the key?

* Ideas for improvement
** Parameters
- Better tuning of parameters may work?
- Partitioning into three or more chunks may speed up construction?

** Align packed vectors to cachelines
Instead of packing $r$ bit values throughout the entire vector, only pack them
inside cachelines.

** Prefetching
The query performance of this and other MPHFs seem to be limited by the latency
of memory lookups. For this reason, most MPHFs minimize the number of memory
lookups, and in particular PTHash uses one random memory access followed by one
lookup in a small (cached) dictionary.

When processing a number of lookups sequentially, each lookup currently incurs a
cache miss. This could be made much more efficient by doing $k$ (e.g. $k=16$) cache
lookups in parallel:
1. first compute the array location to lookup for $k$ keys (possibly using SIMD),
2. then prefetch the $k$ memory locations,
3. then do the rest of the computation in parallel.
4. If needed, do lookups in the $free$ table in parallel.

This should mostly hide the memory latency and could give significant speedup.
I'm not quite sure yet whether this would work best when done in batches (as
described), or in streaming fashion, where we iterate over elements and prefetch
memory for the element $k$ iterations ahead. [[https://en.algorithmica.org/hpc/cpu-cache/prefetching/][Algorithmica.org]] has a nice article
on the streaming approach.

** Faster modulo operations
There are a lot of ~% n~, ~% p1~ and ~% (m-p1)~ operations throughout the code.
I didn't look into it yet, but possibly these are the bottleneck on the CPU
latency.

First note that
$$
a\, \%\, b = a - \lfloor a/b\rfloor * b.
$$
This division by a constant can be computed efficiently using a trick which
replaces division by multiplication with the inversion.
Using the formula of the [[https://en.wikipedia.org/wiki/Division_algorithm#Division_by_a_constant][wikipedia article]] we can precompute some constants to
evaluate $\lfloor a/b\rfloor$ in $6$ operations and ~a % b~ in $8$ operations.

+(Note that it might be possible compilers already do this, but I don't expect so.)+

Some blogposts by Daniel Lemire ([[https://twitter.com/daniel_c0deb0t/status/1704999240802636051][Thanks Daniel Liu]] ;)
- [[https://lemire.me/blog/2016/06/27/a-fast-alternative-to-the-modulo-reduction/][A fast alternative to the modulo reduction]]

  Instead of ~a % b~, compute ~a * b >> 64~, assuming that $a$ is uniform in
  $[2^{64}-1]$.

  This doesn't seem to work well in practice though for PTHash, probably since
  this only uses the entropy in the high-order bits of $a$.
- [[https://lemire.me/blog/2019/02/08/faster-remainders-when-the-divisor-is-a-constant-beating-compilers-and-libdivide/][Faster remainders when the divisor is a constant: beating compilers and libdivide]]

  *Indeed, the C++ PTHash implementation [[https://twitter.com/giulio_pibiri/status/1705104355270037980][already uses]] the =fastmod= library.*

- [[https://lemire.me/blog/2019/02/20/more-fun-with-fast-remainders-when-the-divisor-is-a-constant/][More fun with fast remainders when the divisor is a constant]]
- [cite/t:@fast-remainder]

** Store dictionary $D$ sorted using Elias-Fano coding
I doubt whether the memory savings here are worth the time overhead, but it's an
idea :shrug:.

** How many bits of $n$ and hash entropy do we need?
One bottleneck of =fastmod64= is that it needs to compute the
highest $64$ bits of a =u128 * u64= product.
If we can assume that $n$ is at most $40$ bits, and that $44$ bits are
sufficient entropy, then I think we could do away with the ~u128 x u64~
multiplication and do everything inside ~u128~.

* Implementation log
A somewhat chronological list of notes and remarks.
** Hashing function
For now I use =murmur64a=.
** Bitpacking crates
There are *a lot* of bitvector and bitpacking crates!
- [[https://crates.io/search?q=bitvec][bitvectors]] :: All of the below seem to do the same
  - =bitvec=: $30M$ downloads
  - =bit-vec=: $30M$ downloads
  - =fixedbitset=: $55M$ downloads
  No idea which is best; probably I'll settle for the one below in =sucds=.
- [[https://crates.io/crates/sucds][sucds]] :: only $60K$ downloads, but contains
  - [[https://docs.rs/sucds/latest/sucds/bit_vectors/bit_vector/struct.BitVector.html][BitVector]]
  - fixed-width integer packing: [[https://docs.rs/sucds/latest/sucds/int_vectors/compact_vector/struct.CompactVector.html][CompactVector]]
    - Decoding seems somewhat inefficient
  - increasing-integer sequence packing: [[https://docs.rs/sucds/latest/sucds/mii_sequences/index.html][EliasFano]]
    - Giulio has [[https://github.com/jermp/data_compression_course][lecture notes]] on this.
- [[https://docs.rs/succinct/][succinct]]
  - [[https://docs.rs/succinct/0.5.2/succinct/struct.IntVector.html][IntVector]]
    - Can not be constructed from slice/iterator of values.
    - Decoding seems somewhat inefficient
    - No updates in the past 2 years.

** Construction
- Storing buckets as ~Vec<Vec<Key>>~ is bad for large keys, so now I store
  ~Vec<Vec<usize>>~, but the nested ~Vec~s still waste a lot of space and will
  cause allocation slowdowns. PTHash pushes onto a vector which is sorted later,
  which seems more efficient.
- When testing $k_i$, not only do we need to test that positions are not filled
  by previous buckets, but also we have to check that elements within the bucket
  do not collide. *It is not sufficient that $h(x, s)$ does not collide within
  buckets,* since they could collide after taking the ~% n~.

** Fastmod
It seems that Daniel Lemire's =fastmod= C++ library has not yet been ported to
Rust, so I converted the few parts I need.

There is also [[https://crates.io/crates/strength_reduce][=strength_reduce=]], which contains a similar but distinct algorithm
for ~a % b~ that computes the remainder from the quotient.

** First benchmark
I [[https://github.com/RagnarGrootKoerkamp/pthash-rs/commit/c070936558e756bafaae92af5be31ac383f2c3ee][implemented]] these under a generic =Reduce= trait.

~just bench~ at the linked commit at ~2.6GHz~ gives the following for $10^7$ keys:

| method           | construction (s) | query (ns) |
| u64              |        10.474591 |         91 |
| fastmod64        |        10.731583 |         55 |
| fastmod32        |         9.911751 |         *50* |
| strengthreduce64 |        11.520939 |         56 |
| strengthreduce32 |        10.002017 |         *50* |

The =u32= versions simply only use the lower $32$ bits of the $64$ bit hash.

This is not yet as fast as the fastest =28ns= reported in the PTHash paper (for
C-C encoding), but I also haven't optimized anything else yet. Time for profiling.

*Profiling:* Looking at the flamegraph (~cargo flamegraph~), and zooming in on the hash function, we see

#+attr_html: :class inset
[[file:hash_flame.png]]

A lot of time is spend on fold! The ~murmur2~ function I use has signature
~murmur2(bytes: &[u8], seed: u64)~, and even though my keys/bytes always correspond
to just a ~u64~, it's iterating over them!

In the generated ~perf report~, we see
#+begin_src txt
  33.14%         27328  test::queries_e  pthash_rs-f15b4648f77f672b           [.] pthash_rs::PTHash<P,R>::new
  18.18%         14823  test::queries_e  pthash_rs-f15b4648f77f672b           [.] pthash_rs::PTHash<P,R>::index
  13.76%         11245  test::queries_e  pthash_rs-f15b4648f77f672b           [.] murmur2::murmur64ane
#+end_src
We can ignore the $33\%$ for construction and only focus on querying here, where
we see that the =index= function calls to =murmur2= and a lot of time is spent
in both. In fact, =murmur2= is not inlined at all! That explains the iterator
appearing in the flamegraph.

*Thin-LTO:* This is fixed by [[https://github.com/RagnarGrootKoerkamp/pthash-rs/commit/4b25317bf4c78bc1264f88b0592af2c08de54044][enabling]] link-time optimization: add ~lto = "thin"~ to
~cargo.toml~.

Rerunning the benchmark we get

|                  | construction (s) | construction (s) | query (ns) | query (ns) |
| method           |           no LTO |         thin-LTO | no LTO     |   thin-LTO |
| u64              |             10.5 |              8.9 | 91         |         60 |
| fastmod64        |             10.7 |              8.3 | 55         |         34 |
| fastmod32        |              9.9 |              8.5 | *50*       |       *26* |
| strengthreduce64 |             11.5 |              8.3 | 56         |         38 |
| strengthreduce32 |             10.0 |              8.8 | *50*       |         31 |

Sweet! =26ns= is faster than any of the numbers in table 5 of [cite/t:@pthash]!
(Admittedly, there is no compression yet and the dictionary size is $10\times$
smaller, but still!)

*More inlining:*
Actually, we don't even want the =index()= function call to show up in our logs:
[[https://github.com/RagnarGrootKoerkamp/pthash-rs/commit/39a3411332f70bde37de90221c9f460bd8b79f9a][inlining]] it should give better instruction pipelining in the benchmarking hot-loop
#+begin_src rust
for key in &keys {
    mphf.index(key);
}
#+end_src
and indeed, we now get
| query (ns)       | no LTO | thin-LTO | inline index() |
| u64              |     91 |       60 |             55 |
| fastmod64        |     55 |       34 |             33 |
| fastmod32        |   *50* |     *26* |           *24* |
| strengthreduce64 |     56 |       38 |             33 |
| strengthreduce32 |   *50* |       31 |             26 |


*Conclusion:* From now on let's only use =fastmod64= and =fastmod32=. (I suspect
the =32bit= variant does not have sufficient entropy for large key sets, so we
keep the original =64bit= variant as well.)

** Faster bucket computation

After inlining everything, the generated assembly for our test is just one big
$\sim 100$ line assembly function. Currently, the ~bucket(hx)~ function (that
computes the bucket for the given hash ~hx = hash(x, s)~) looks like
#+begin_src rust
fn bucket(&self, hx: u64) -> u64 {
    if (hx % self.rem_n) < self.p1 { // Compare
        hx % self.rem_p2
    } else {
        self.p2 + hx % self.rem_mp2
    }
}
#+end_src
The assembly looks like this:
#+begin_src asm
     5 │    ┌──cmp        %rdi,0xc0(%rsp)       # compare
     1 │    ├──jbe        370
   102 │    │  mov        0x98(%rsp),%rdx       # first branch: fastmod
    41 │    │  mulx       %r9,%rdx,%rdi
     4 │    │  imul       0xa0(%rsp),%r9
    85 │    │  mulx       %r10,%r13,%r13
    12 │    │  add        %rdi,%r9
     7 │    │  mov        %r9,%rdx
    72 │    │  mulx       %r10,%rdx,%rdi
    17 │    │  add        %r13,%rdx
     3 │    │  adc        $0x0,%rdi             # add 0
       │    │  cmp        %rdi,0x58(%rsp)       # index-out-of-bound check for k array
    56 │    │↓ ja         3ac                   # ok: continue below at line 3ac:
       │    │↓ jmp        528                   # panic!
       │    │  cs         nopw 0x0(%rax,%rax,1)
   128 │370:└─→mov        0xa8(%rsp),%rdx       # second branch: fastmod
    41 │       mulx       %r9,%rdx,%rdi
       │       imul       0xb0(%rsp),%r9
    66 │       mulx       %rcx,%r13,%r13
    12 │       add        %rdi,%r9
       │       mov        %r9,%rdx
    58 │       mulx       %rcx,%rdx,%rdi
    14 │       add        %r13,%rdx
       │       adc        0xb8(%rsp),%rdi       # add p2
    54 │       cmp        %rdi,0x58(%rsp)       # out-of-bound check for k array
     1 │     ↓ jbe        528                   # panic!
  8100 │3ac:   mov        (%r11,%rdi,8),%rdx    # Do array index.
#+end_src
We see that there are quite some branches:
- The first and second branch of the ~bucket()~ function are both fully written out.
- They use the same number of instructions.
- One branch does =add 0=, I suppose because the CPU likes equal-sized branches.
- There are redundant index-out-of-bounds checks.
- The last line, the array index itself, has $8000$ samples: $57\%$ of the total
  samples is *this single assembly instruction*!

*Branchless bucket index:*
I tried rewriting the ~bucket()~ function into a branchless form as follows:
#+begin_src rust
fn bucket(&self, hx: u64) -> u64 {
    let is_large = (hx % self.rem_n) >= self.p1;
    let rem = if is_large { self.rem_mp2 } else { self.rem_p2 };
    is_large as u64 * self.p2 + hx % rem
}
#+end_src
but this turns out to be *slower* than the original, probably because the new
assembly now needs a lot of =cmov= instructions. (In particular, =rem= contains
a =u128= and a =u64=, so needs $3$ =mov='s and $3$ =cmov='s.)
#+begin_src asm
       │       cmp        %rdi,0xd0(%rsp)     # comparison
   112 │       mov        0xb8(%rsp),%rdi     # load rem_p2
    29 │       cmova      0xc8(%rsp),%rdi     # conditionally overwrite with rem_mp2
       │       mov        0xb0(%rsp),%rdx
   137 │       cmova      0xc0(%rsp),%rdx
       │       mov        0xa0(%rsp),%r14
   137 │       cmova      0xa8(%rsp),%r14
    26 │       mov        %r9,%r10
       │       mov        $0x0,%r11d          # set offset to 0
    90 │       cmova      %r11,%r10           # conditionally overwrite offset
    38 │       imul       %r8,%rdi            # start computation
     2 │       mulx       %r8,%rdx,%r8
   122 │       add        %r8,%rdi
    48 │       mulx       %r14,%r8,%r8
       │       mov        %rdi,%rdx
   163 │       mulx       %r14,%rdx,%rdi
       │       add        %r8,%rdx
       │       adc        %r10,%rdi
   184 │       cmp        %rdi,0x60(%rsp)     # index-out-of-bounds check
       │     ↓ jbe        52f                 # panic
    38 │       mov        0x98(%rsp),%rdx
 10798 │       mov        (%rdx,%rdi,8),%rdx  # Do array index.
#+end_src

*No bounds check:*
We can replace ~k[index]~ by ~unsafe { *k.get_unchecked(index) }~.
This doesn't give much performance gain (less than the few ~ns~ of measurement
noise I have), but can't hurt. It removes the final =cmp; jbe= lines from the assembly.

*Fix tests:* Instead of ignoring test results we can accumulate the resulting
indices and pass them to =black_box(sum)=. This prevents the compiler from
optimizing away all queries. /Somehow/ this affects the reported timings. I now get:

| query (ns)       | no LTO | thin-LTO | inline index() | fixed tests |
| u64              |     91 |       60 |             55 |          63 |
| fastmod64        |     55 |       34 |             33 |          35 |
| fastmod32        |   *50* |     *26* |           *24* |        *20* |
| strengthreduce64 |     56 |       38 |             33 |          38 |
| strengthreduce32 |   *50* |       31 |             26 |          30 |

I'm confused how the =fastmod32= timing went down, but the =fastmod64= went up.
(Typical situation when you do constant profiling and there are more numbers
than you can make sense of, sadly.)

** Branchless, for real now! (aka the trick-of-thirds)

I'm still annoyed by this branching. Branches are bad! They may be fast for now,
but I kinda have the long term goal to put SIMD on top of this and that doesn't
go well with branching. Also, branch-misprediction is a thing, and the $70\% -
30\%$ uniform random split is about as bad as you can do to a branch predictor.
The code from earlier does fix it, but at the cost of a whole bunch of =mov='s and
=cmov='s.

But there is a trick we can do! $p_1$ and $p_2$ are sort of arbitrarily
chosen, and all the original paper [cite:@fch] has to say about it is
#+begin_quote
Good values for these two parameters are experimentally
determined to be around $0.6n$ and $0.3m$, respectively.
#+end_quote
Thus I feel at liberty to change the value of $p_2$ from $0.3m$ to $m/3$.
This gives:
$$m-p_2 = m-m/3 = \frac 23 m = 2p_2.$$
The cute tick is that now we can use that
$$x \mm p_2 = (x \mm (2p_2)) \mm p_2 = (x \mm (m - p_2)) \mm p_2,$$
and since $ 0\leq x \mm (2p_2) < 2p_2$, computing that value modulo $p_2$ is as
simple as comparing the value to $p_2$ and subtracting $p_2$ if needed.

Thus, we modify the initialization to round $m$ up to the next multiple of $3$,
and change the bucket function to
#+begin_src rust
fn bucket(&self, hx: u64) -> u64 {
    let mod_mp2 = hx % self.rem_mp2;
    let mod_p2 = mod_mp2 - self.p2 * (mod_mp2 >= self.p2) as u64;
    let large = (hx % self.rem_n) >= self.p1;
    self.p2 * large as u64 + if large { mod_mp2 } else { mod_p2 }
}
#+end_src

The new timings are

| query (ns)       | no LTO | thin-LTO | inline index() | fixed tests | $p_2 = m/3$ |
| u64              |     91 |       60 |             55 |          63 |          54 |
| fastmod64        |     55 |       34 |             33 |          35 |          27 |
| fastmod32        |   *50* |     *26* |           *24* |        *20* |        *19* |
| strengthreduce64 |     56 |       38 |             33 |          38 |          33 |
| strengthreduce32 |   *50* |       31 |             26 |          30 |          21 |

=fastmod32= didn't get much faster, but all others went down a lot! Let's check
out the generated assembly for =fastmod64=:
#+begin_src asm
    10 │          vpunpcklqdq  %xmm3,%xmm5,%xmm3
     3 │          vmovq        %r15,%xmm12
     1 │          vpunpcklqdq  %xmm6,%xmm7,%xmm5
    33 │          vinserti128  $0x1,%xmm3,%ymm5,%ymm5
    14 │          vpunpcklqdq  %xmm8,%xmm14,%xmm3
     2 │          vpunpcklqdq  %xmm15,%xmm12,%xmm6
     2 │          vinserti128  $0x1,%xmm3,%ymm6,%ymm3
    32 │          vmovdqu      0x570(%rsp),%ymm14
    15 │          vpxor        %ymm3,%ymm14,%ymm3
     2 │          vpxor        0x610(%rsp),%ymm14,%ymm12
     2 │          vpxor        %ymm5,%ymm14,%ymm6
    29 │          vmovdqu      0x630(%rsp),%ymm11
    13 │          vpxor        %ymm14,%ymm11,%ymm15
     2 │          vpcmpgtq     %ymm6,%ymm15,%ymm6
     2 │          vpcmpgtq     %ymm3,%ymm12,%ymm3
    39 │          vpandn       %ymm11,%ymm6,%ymm6
    13 │          vpandn       %ymm11,%ymm3,%ymm7
     4 │          vpand        %ymm6,%ymm3,%ymm3
     2 │          vpaddq       %ymm5,%ymm7,%ymm5
    31 │          vpcmpeqd     %ymm7,%ymm7,%ymm7
     8 │          vpsubq       %ymm3,%ymm5,%ymm3
       │          vpxor        %xmm6,%xmm6,%xmm6
     2 │          mov          0x1c0(%rsp),%r14
  4264 │          vpgatherqq   %ymm7,(%r14,%ymm3,8),%ymm6 # Do array index
#+end_src

Huh what?! I don't really what is going on here, but I do know that the compiler
just vectorized our code for us! All the =vp= instructions are vector/packed
instructions! Magic! This probably explains the big speedup we get for =fastmod64=.

*Closer inspection:* As it turns out, the =32bit= versions were already
auto-vectorized before we implemented this last optimization. Probably because
the ~FastMod32~ type is smaller (two ~u64~) than the ~Fastmod64~ type (~u128~
and ~u64~) and hence easier to vectorize (and similar for =StrengthReduce32=).
But either way this last trick helps a lot for the =64bit= variants that will
be needed for large hashmaps.

** Compiling and benchmarking PTHash
Compiling PTHash was very smooth; just a =git clone=, submodule init, and
building /just worked/ :)

Running a benchmark similar to the ones here:
#+begin_src shell
 ./build -n 10000000 -c 7.0 -a 1 -e compact_compact -s 1234567890 --minimal --verbose --lookup
#+end_src
reports a query performance of =26ns/key=, similar to the =fastmod64=
performance I get.

Note that PTHash uses fixed-width bitpacking here, while I just store =u64='s
directly, but this shouldn't affect the time too much.

*Vectorization:* More interestingly, PTHash is not auto-vectorized by my
compiler, so I'm surprised it performs this well. Maybe the =vpgatherqq=
instruction just doesn't give that much speedup over sequential lookups -- I
don't know yet. But still, my equivalent code using =fastmod64= with $p_2 =
0.3m$ has =35ns/key= vs =26ns/key= for PTHash. Confusing.

*Branching:* PTHash compiles to a branchy version of =fastmod(x, p2) or
fastmod(x, m-p2)=, but is still fast.

** Compact encoding

Adding fixed-width encoding was easy using the =sucds= =CompactVector= type.
The generated code doesn't look so pretty though -- it branches on whether the
bits cross a =usize= boundary, whereas PTHash's implementation does an unaligned
read from a =*u8= to avoid this, which seems nicer.

** Find the $x$ differences

At this point, both =pthash-rs= and the original =PTHash= support encoding by a
single compacted vector, but there is still quite some time difference: =31ns=
vs =25ns=. Time to find all the differences.

This may or may not be the best approach, but I decided to put the assemblies
side-by-side.

*Exhibit A: the missing modulo* Ok, I won't bore you with the full assembly, but I found this in
the PTHash assembly:
#+begin_src asm
movabs     $0x999999ffffffffff,%rbx
#+end_src
with nothing similar in the rust version. Turns out that this is $0.6 \cdot
(2^{64}-1)$. Indeed, [[https://github.com/jermp/pthash/blob/master/include/utils/bucketers.hpp#L18][the code is]]:
#+begin_src c++
inline uint64_t bucket(uint64_t hash) const {
    static const uint64_t T = constants::a * UINT64_MAX;
    return (hash < T) ? fastmod::fastmod_u64(hash, m_M_num_dense_buckets, m_num_dense_buckets)
                        : m_num_dense_buckets + fastmod::fastmod_u64(hash, m_M_num_sparse_buckets,
                                                                    m_num_sparse_buckets);
}
#+end_src
note how it does $hash < 0.6 2^{64}$ instead of $hash \mm n < 0.6 n$ as written
in the paper for what FCH does.
Basically we can completely drop the $\mm n$ there! Sweet! That's $1$ of $3$
modulo operations gone :)

#+print_bibliography:
