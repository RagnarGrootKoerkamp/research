#+title: PTHash: Notes on implementing PTHash in Rust [ongoing]
#+HUGO_SECTION: notes
#+hugo_tags: mphf
#+HUGO_LEVEL_OFFSET: 1
#+OPTIONS: ^:{}
#+hugo_front_matter_key_replace: author>authors
#+toc: headlines 3
#+date: <2023-09-21>
#+author: Ragnar Groot Koerkamp

$$
%\newcommand{\mm}{\,\%\,}
\newcommand{\mm}{\bmod}
$$

Daniel got me excited about minimal perfect hashing functions (MPHFs), and then
[[https://twitter.com/nomad421/status/1701593870734336290][+twitter+ Rob]] asked for rust a implementation of [[https://github.com/jermp/pthash][PTHash]] [cite:@pthash], and also
I have some ideas related to memory prefetching I want to play with, so here we
are: I'm working on a Rust implementation ~pthash-rs~ at [[https://github.com/ragnargrootkoerkamp/pthash-rs]].

This post is just to collect random thoughts and questions that come up while
writing the code.

Twitter discussion is [[https://twitter.com/curious_coding/status/1704989305158979656][here]].

(See also my [[file:../bbhash.org][note on BBHash]] [cite:@bbhash].)

* Questions and remarks on PTHash paper
Probably some of these questions are answered by reading into the original FCH
paper [cite:@fch] -- that'll come at some later time.
- FCH uses $cm/\log_2 n$ buckets because each bucket needs $\log_2 n$ bits
  storage, but for PTHash the bits per bucket is lower so this isn't really the
  right formula maybe.
  - [[https://twitter.com/giulio_pibiri/status/1705114424787308718][Tweet]]: another option is to fix $c = \alpha * \log(n) / d$ for some constant
    $d$.
- FCH uses $\lceil cn / (\log_2n+1)\rceil$ buckets, but PTHash uses $\lceil
  cn/\log_2 n\rceil$ buckets. To me, $\lceil cn/\lceil \log_2n\rceil\rceil$
  actually sounds most intuitive.
- $p_1=0.6n$ and $p_2=0.3m$ seem somewhat arbitrary. Can this be tuned better?
  Could partitioning into more chunks help?

  Anyway, this puts $60\%$ of data in the first $30\%$ of buckets (i.e. double
  the average size), and $40\%$ of data in $70\%$ of buckets (i.e. roughly half
  the average size).

  I suppose the goal is to stitch two binomial distributions of bucket sizes
  together in a nice way to get a more even distribution over sizes.
  - This is an open question ([[https://twitter.com/giulio_pibiri/status/1705112904779915662][tweet]])
    - Proof in appendix of [[https://jermp.github.io/assets/pdf/papers/TKDE2023.pdf][paper]] could possibly be extended.

- How many bits of entropy are really needed? When $n$ is a power of $2$, we
  make sure to use all $64$ bits instead of just $\log_2 n$. But how many are
  really needed. Maybe as long as $n \ll 2^{32}$, a simpler/faster $32$ bit hash
  is sufficient?

- And how large will $n$ be at most? Can we reasonably assume $2^{32} \approx
  4\cdot 10^9$ as a bound?

- Is it reasonable to compute and store hashes of all keys, and then changing
  the API to let the user pass in the hash instead of the key?

- What's the probability of failure with the first global seed $s$ that's tried?

- What are the largest $k_i$ found in typical cases?

- What if $\gcd(m, n)$ is large?

* Ideas for improvement
** Parameters
- Better tuning of parameters may work?
- Partitioning into three or more chunks may speed up construction?

** Align packed vectors to cachelines
Instead of packing $r$ bit values throughout the entire vector, only pack them
inside cachelines.

** Prefetching
The query performance of this and other MPHFs seem to be limited by the latency
of memory lookups. For this reason, most MPHFs minimize the number of memory
lookups, and in particular PTHash uses one random memory access followed by one
lookup in a small (cached) dictionary.

When processing a number of lookups sequentially, each lookup currently incurs a
cache miss. This could be made much more efficient by doing $k$ (e.g. $k=16$) cache
lookups in parallel:
1. first compute the array location to lookup for $k$ keys (possibly using SIMD),
2. then prefetch the $k$ memory locations,
3. then do the rest of the computation in parallel.
4. If needed, do lookups in the $free$ table in parallel.

This should mostly hide the memory latency and could give significant speedup.
I'm not quite sure yet whether this would work best when done in batches (as
described), or in streaming fashion, where we iterate over elements and prefetch
memory for the element $k$ iterations ahead. [[https://en.algorithmica.org/hpc/cpu-cache/prefetching/][Algorithmica.org]] has a nice article
on the streaming approach.

** Faster modulo operations
There are a lot of ~% n~, ~% p1~ and ~% (m-p1)~ operations throughout the code.
I didn't look into it yet, but possibly these are the bottleneck on the CPU
latency.

First note that
$$
a\, \%\, b = a - \lfloor a/b\rfloor * b.
$$
This division by a constant can be computed efficiently using a trick which
replaces division by multiplication with the inversion.
Using the formula of the [[https://en.wikipedia.org/wiki/Division_algorithm#Division_by_a_constant][wikipedia article]] we can precompute some constants to
evaluate $\lfloor a/b\rfloor$ in $6$ operations and ~a % b~ in $8$ operations.

+(Note that it might be possible compilers already do this, but I don't expect so.)+

Some blogposts by Daniel Lemire ([[https://twitter.com/daniel_c0deb0t/status/1704999240802636051][Thanks Daniel Liu]] ;)
- [[https://lemire.me/blog/2016/06/27/a-fast-alternative-to-the-modulo-reduction/][A fast alternative to the modulo reduction]]

  Instead of ~a % b~, compute ~a * b >> 64~, assuming that $a$ is uniform in
  $[2^{64}-1]$.

  This doesn't seem to work well in practice though for PTHash, probably since
  this only uses the entropy in the high-order bits of $a$.
- [[https://lemire.me/blog/2019/02/08/faster-remainders-when-the-divisor-is-a-constant-beating-compilers-and-libdivide/][Faster remainders when the divisor is a constant: beating compilers and libdivide]]

  *Indeed, the C++ PTHash implementation [[https://twitter.com/giulio_pibiri/status/1705104355270037980][already uses]] the =fastmod= library.*

- [[https://lemire.me/blog/2019/02/20/more-fun-with-fast-remainders-when-the-divisor-is-a-constant/][More fun with fast remainders when the divisor is a constant]]
- [cite/t:@fast-remainder]

** Store dictionary $D$ sorted using Elias-Fano coding
I doubt whether the memory savings here are worth the time overhead, but it's an
idea :shrug:.

** How many bits of $n$ and hash entropy do we need?
One bottleneck of =fastmod64= is that it needs to compute the
highest $64$ bits of a =u128 * u64= product.
If we can assume that $n$ is at most $40$ bits, and that $44$ bits are
sufficient entropy, then I think we could do away with the ~u128 x u64~
multiplication and do everything inside ~u128~.

** Ideas for faster construction
Once the table is almost filled, determining the $k_i$ becomes slower.
Some ideas to speed this up:
- reversing =murmurhash= :: Instead of finding a $k_i$ such that $position(x) :=
  (h(x) \lxor
  h(k_i))\mm n$ is not taken becomes slow. Instead, we could keep a list of
  empty positions $p$ and determine $k_i$ as $h^{inv}((p + j \cdot n) \lxor
  h(x))$ for different $j$, assuming we can invert =murmurhash=. As it turns
  out, =murmurhash2= is an invertible function with $O(1)$ inverse! See [[http://bitsquid.blogspot.com/2011/08/code-snippet-murmur-hash-inverse-pre.html][this
  blogpost]]. Thus, we can easily find many $k_i$ such that $position(x, k_i)$
  maps to an empty position.

  - All buckets of size $1$ can be directly assigned to slots this way.
  - Size $2$ and $3$ buckets also need fewer tries than before.

  The big drawback though is that the $k_i$ values found will be uniform in $[0,
  2^{64}]$.

- Cuckoo-hashing :: For sets of size $1$ and $2$ (and maybe $3$?) we could
  /displace/ an already taken slot if that is the last remaining slot needed to
  fix the current bucket. Probably we want to only displace buckets of the same
  size and never buckets of larger size.

  I wonder though how useful this actually is. If the current bucket is hard to
  place, there is not really any reason a different bucket of the same size
  would be easier to fix.

* Implementation log
A somewhat chronological list of notes and remarks.
** Hashing function
For now I use =murmur64a=, documented on the =SMHasher= [[https://github.com/aappleby/smhasher/wiki][GitHub wiki]].
** Bitpacking crates
There are *a lot* of bitvector and bitpacking crates!
- [[https://crates.io/search?q=bitvec][bitvectors]] :: All of the below seem to do the same
  - =bitvec=: $30M$ downloads
  - =bit-vec=: $30M$ downloads
  - =fixedbitset=: $55M$ downloads
  No idea which is best; probably I'll settle for the one below in =sucds=.
- [[https://crates.io/crates/sucds][sucds]] :: only $60K$ downloads, but contains
  - [[https://docs.rs/sucds/latest/sucds/bit_vectors/bit_vector/struct.BitVector.html][BitVector]]
  - fixed-width integer packing: [[https://docs.rs/sucds/latest/sucds/int_vectors/compact_vector/struct.CompactVector.html][CompactVector]]
    - Decoding seems somewhat inefficient
  - increasing-integer sequence packing: [[https://docs.rs/sucds/latest/sucds/mii_sequences/index.html][EliasFano]]
    - Giulio has [[https://github.com/jermp/data_compression_course][lecture notes]] on this.
- [[https://docs.rs/succinct/][succinct]]
  - [[https://docs.rs/succinct/0.5.2/succinct/struct.IntVector.html][IntVector]]
    - Can not be constructed from slice/iterator of values.
    - Decoding seems somewhat inefficient
    - No updates in the past 2 years.

** Construction
- Storing buckets as ~Vec<Vec<Key>>~ is bad for large keys, so now I store
  ~Vec<Vec<usize>>~, but the nested ~Vec~s still waste a lot of space and will
  cause allocation slowdowns. PTHash pushes onto a vector which is sorted later,
  which seems more efficient.
- When testing $k_i$, not only do we need to test that positions are not filled
  by previous buckets, but also we have to check that elements within the bucket
  do not collide. *It is not sufficient that $h(x, s)$ does not collide within
  buckets,* since they could collide after taking the ~% n~.

** Fastmod
It seems that Daniel Lemire's =fastmod= C++ library has not yet been ported to
Rust, so I converted the few parts I need.

There is also [[https://crates.io/crates/strength_reduce][=strength_reduce=]], which contains a similar but distinct algorithm
for ~a % b~ that computes the remainder from the quotient.

** First benchmark
I [[https://github.com/RagnarGrootKoerkamp/pthash-rs/commit/c070936558e756bafaae92af5be31ac383f2c3ee][implemented]] these under a generic =Reduce= trait.

~just bench~ at the linked commit at ~2.6GHz~ gives the following for $10^7$ keys:

| method           | construction (s) | query (ns) |
| u64              |        10.474591 |         91 |
| fastmod64        |        10.731583 |         55 |
| fastmod32        |         9.911751 |         *50* |
| strengthreduce64 |        11.520939 |         56 |
| strengthreduce32 |        10.002017 |         *50* |

The =u32= versions simply only use the lower $32$ bits of the $64$ bit hash.

This is not yet as fast as the fastest =28ns= reported in the PTHash paper (for
C-C encoding), but I also haven't optimized anything else yet. Time for profiling.

*Profiling:* Looking at the flamegraph (~cargo flamegraph~), and zooming in on the hash function, we see

#+attr_html: :class inset
[[file:hash_flame.png]]

A lot of time is spend on fold! The ~murmur2~ function I use has signature
~murmur2(bytes: &[u8], seed: u64)~, and even though my keys/bytes always correspond
to just a ~u64~, it's iterating over them!

In the generated ~perf report~, we see
#+begin_src txt
  33.14%         27328  test::queries_e  pthash_rs-f15b4648f77f672b           [.] pthash_rs::PTHash<P,R>::new
  18.18%         14823  test::queries_e  pthash_rs-f15b4648f77f672b           [.] pthash_rs::PTHash<P,R>::index
  13.76%         11245  test::queries_e  pthash_rs-f15b4648f77f672b           [.] murmur2::murmur64ane
#+end_src
We can ignore the $33\%$ for construction and only focus on querying here, where
we see that the =index= function calls to =murmur2= and a lot of time is spent
in both. In fact, =murmur2= is not inlined at all! That explains the iterator
appearing in the flamegraph.

*Thin-LTO:* This is fixed by [[https://github.com/RagnarGrootKoerkamp/pthash-rs/commit/4b25317bf4c78bc1264f88b0592af2c08de54044][enabling]] link-time optimization: add ~lto = "thin"~ to
~cargo.toml~.

Rerunning the benchmark we get

|                  | construction (s) | construction (s) | query (ns) | query (ns) |
| method           |           no LTO |         thin-LTO | no LTO     |   thin-LTO |
| u64              |             10.5 |              8.9 | 91         |         60 |
| fastmod64        |             10.7 |              8.3 | 55         |         34 |
| fastmod32        |              9.9 |              8.5 | *50*       |       *26* |
| strengthreduce64 |             11.5 |              8.3 | 56         |         38 |
| strengthreduce32 |             10.0 |              8.8 | *50*       |         31 |

Sweet! =26ns= is faster than any of the numbers in table 5 of [cite/t:@pthash]!
(Admittedly, there is no compression yet and the dictionary size is $10\times$
smaller, but still!)

*More inlining:*
Actually, we don't even want the =index()= function call to show up in our logs:
[[https://github.com/RagnarGrootKoerkamp/pthash-rs/commit/39a3411332f70bde37de90221c9f460bd8b79f9a][inlining]] it should give better instruction pipelining in the benchmarking hot-loop
#+begin_src rust
for key in &keys {
    mphf.index(key);
}
#+end_src
and indeed, we now get
| query (ns)       | no LTO | thin-LTO | inline index() |
| u64              |     91 |       60 |             55 |
| fastmod64        |     55 |       34 |             33 |
| fastmod32        |   *50* |     *26* |           *24* |
| strengthreduce64 |     56 |       38 |             33 |
| strengthreduce32 |   *50* |       31 |             26 |


*Conclusion:* From now on let's only use =fastmod64= and =fastmod32=. (I suspect
the =32bit= variant does not have sufficient entropy for large key sets, so we
keep the original =64bit= variant as well.)

** Faster bucket computation

After inlining everything, the generated assembly for our test is just one big
$\sim 100$ line assembly function. Currently, the ~bucket(hx)~ function (that
computes the bucket for the given hash ~hx = hash(x, s)~) looks like
#+begin_src rust
fn bucket(&self, hx: u64) -> u64 {
    if (hx % self.rem_n) < self.p1 { // Compare
        hx % self.rem_p2
    } else {
        self.p2 + hx % self.rem_mp2
    }
}
#+end_src
The assembly looks like this:
#+begin_src asm
     5 │    ┌──cmp        %rdi,0xc0(%rsp)       # compare
     1 │    ├──jbe        370
   102 │    │  mov        0x98(%rsp),%rdx       # first branch: fastmod
    41 │    │  mulx       %r9,%rdx,%rdi
     4 │    │  imul       0xa0(%rsp),%r9
    85 │    │  mulx       %r10,%r13,%r13
    12 │    │  add        %rdi,%r9
     7 │    │  mov        %r9,%rdx
    72 │    │  mulx       %r10,%rdx,%rdi
    17 │    │  add        %r13,%rdx
     3 │    │  adc        $0x0,%rdi             # add 0
       │    │  cmp        %rdi,0x58(%rsp)       # index-out-of-bound check for k array
    56 │    │↓ ja         3ac                   # ok: continue below at line 3ac:
       │    │↓ jmp        528                   # panic!
       │    │  cs         nopw 0x0(%rax,%rax,1)
   128 │370:└─→mov        0xa8(%rsp),%rdx       # second branch: fastmod
    41 │       mulx       %r9,%rdx,%rdi
       │       imul       0xb0(%rsp),%r9
    66 │       mulx       %rcx,%r13,%r13
    12 │       add        %rdi,%r9
       │       mov        %r9,%rdx
    58 │       mulx       %rcx,%rdx,%rdi
    14 │       add        %r13,%rdx
       │       adc        0xb8(%rsp),%rdi       # add p2
    54 │       cmp        %rdi,0x58(%rsp)       # out-of-bound check for k array
     1 │     ↓ jbe        528                   # panic!
  8100 │3ac:   mov        (%r11,%rdi,8),%rdx    # Do array index.
#+end_src
We see that there are quite some branches:
- The first and second branch of the ~bucket()~ function are both fully written out.
- They use the same number of instructions.
- One branch does =add 0=, I suppose because the CPU likes equal-sized branches.
- There are redundant index-out-of-bounds checks.
- The last line, the array index itself, has $8000$ samples: $57\%$ of the total
  samples is *this single assembly instruction*!

*Branchless bucket index:*
I tried rewriting the ~bucket()~ function into a branchless form as follows:
#+begin_src rust
fn bucket(&self, hx: u64) -> u64 {
    let is_large = (hx % self.rem_n) >= self.p1;
    let rem = if is_large { self.rem_mp2 } else { self.rem_p2 };
    is_large as u64 * self.p2 + hx % rem
}
#+end_src
but this turns out to be *slower* than the original, probably because the new
assembly now needs a lot of =cmov= instructions. (In particular, =rem= contains
a =u128= and a =u64=, so needs $3$ =mov='s and $3$ =cmov='s.)
#+begin_src asm
       │       cmp        %rdi,0xd0(%rsp)     # comparison
   112 │       mov        0xb8(%rsp),%rdi     # load rem_p2
    29 │       cmova      0xc8(%rsp),%rdi     # conditionally overwrite with rem_mp2
       │       mov        0xb0(%rsp),%rdx
   137 │       cmova      0xc0(%rsp),%rdx
       │       mov        0xa0(%rsp),%r14
   137 │       cmova      0xa8(%rsp),%r14
    26 │       mov        %r9,%r10
       │       mov        $0x0,%r11d          # set offset to 0
    90 │       cmova      %r11,%r10           # conditionally overwrite offset
    38 │       imul       %r8,%rdi            # start computation
     2 │       mulx       %r8,%rdx,%r8
   122 │       add        %r8,%rdi
    48 │       mulx       %r14,%r8,%r8
       │       mov        %rdi,%rdx
   163 │       mulx       %r14,%rdx,%rdi
       │       add        %r8,%rdx
       │       adc        %r10,%rdi
   184 │       cmp        %rdi,0x60(%rsp)     # index-out-of-bounds check
       │     ↓ jbe        52f                 # panic
    38 │       mov        0x98(%rsp),%rdx
 10798 │       mov        (%rdx,%rdi,8),%rdx  # Do array index.
#+end_src

*No bounds check:*
We can replace ~k[index]~ by ~unsafe { *k.get_unchecked(index) }~.
This doesn't give much performance gain (less than the few ~ns~ of measurement
noise I have), but can't hurt. It removes the final =cmp; jbe= lines from the assembly.

*Fix tests:* Instead of ignoring test results we can accumulate the resulting
indices and pass them to =black_box(sum)=. This prevents the compiler from
optimizing away all queries. /Somehow/ this affects the reported timings. I now get:

| query (ns)       | no LTO | thin-LTO | inline index() | fixed tests |
| u64              |     91 |       60 |             55 |          63 |
| fastmod64        |     55 |       34 |             33 |          35 |
| fastmod32        |   *50* |     *26* |           *24* |        *20* |
| strengthreduce64 |     56 |       38 |             33 |          38 |
| strengthreduce32 |   *50* |       31 |             26 |          30 |

I'm confused how the =fastmod32= timing went down, but the =fastmod64= went up.
(Typical situation when you do constant profiling and there are more numbers
than you can make sense of, sadly.)

** Branchless, for real now! (aka the trick-of-thirds)

I'm still annoyed by this branching. Branches are bad! They may be fast for now,
but I kinda have the long term goal to put SIMD on top of this and that doesn't
go well with branching. Also, branch-misprediction is a thing, and the $70\% -
30\%$ uniform random split is about as bad as you can do to a branch predictor.
The code from earlier does fix it, but at the cost of a whole bunch of =mov='s and
=cmov='s.

But there is a trick we can do! $p_1$ and $p_2$ are sort of arbitrarily
chosen, and all the original paper [cite:@fch] has to say about it is
#+begin_quote
Good values for these two parameters are experimentally
determined to be around $0.6n$ and $0.3m$, respectively.
#+end_quote
Thus I feel at liberty to change the value of $p_2$ from $0.3m$ to $m/3$.
This gives:
$$m-p_2 = m-m/3 = \frac 23 m = 2p_2.$$
The cute tick is that now we can use that
$$x \mm p_2 = (x \mm (2p_2)) \mm p_2 = (x \mm (m - p_2)) \mm p_2,$$
and since $ 0\leq x \mm (2p_2) < 2p_2$, computing that value modulo $p_2$ is as
simple as comparing the value to $p_2$ and subtracting $p_2$ if needed.

Thus, we modify the initialization to round $m$ up to the next multiple of $3$,
and change the bucket function to
#+begin_src rust
fn bucket(&self, hx: u64) -> u64 {
    let mod_mp2 = hx % self.rem_mp2;
    let mod_p2 = mod_mp2 - self.p2 * (mod_mp2 >= self.p2) as u64;
    let large = (hx % self.rem_n) >= self.p1;
    self.p2 * large as u64 + if large { mod_mp2 } else { mod_p2 }
}
#+end_src

The new timings are

| query (ns)       | no LTO | thin-LTO | inline index() | fixed tests | $p_2 = m/3$ |
| u64              |     91 |       60 |             55 |          63 |          54 |
| fastmod64        |     55 |       34 |             33 |          35 |          27 |
| fastmod32        |   *50* |     *26* |           *24* |        *20* |        *19* |
| strengthreduce64 |     56 |       38 |             33 |          38 |          33 |
| strengthreduce32 |   *50* |       31 |             26 |          30 |          21 |

=fastmod32= didn't get much faster, but all others went down a lot! Let's check
out the generated assembly for =fastmod64=:
#+begin_src asm
    10 │          vpunpcklqdq  %xmm3,%xmm5,%xmm3
     3 │          vmovq        %r15,%xmm12
     1 │          vpunpcklqdq  %xmm6,%xmm7,%xmm5
    33 │          vinserti128  $0x1,%xmm3,%ymm5,%ymm5
    14 │          vpunpcklqdq  %xmm8,%xmm14,%xmm3
     2 │          vpunpcklqdq  %xmm15,%xmm12,%xmm6
     2 │          vinserti128  $0x1,%xmm3,%ymm6,%ymm3
    32 │          vmovdqu      0x570(%rsp),%ymm14
    15 │          vpxor        %ymm3,%ymm14,%ymm3
     2 │          vpxor        0x610(%rsp),%ymm14,%ymm12
     2 │          vpxor        %ymm5,%ymm14,%ymm6
    29 │          vmovdqu      0x630(%rsp),%ymm11
    13 │          vpxor        %ymm14,%ymm11,%ymm15
     2 │          vpcmpgtq     %ymm6,%ymm15,%ymm6
     2 │          vpcmpgtq     %ymm3,%ymm12,%ymm3
    39 │          vpandn       %ymm11,%ymm6,%ymm6
    13 │          vpandn       %ymm11,%ymm3,%ymm7
     4 │          vpand        %ymm6,%ymm3,%ymm3
     2 │          vpaddq       %ymm5,%ymm7,%ymm5
    31 │          vpcmpeqd     %ymm7,%ymm7,%ymm7
     8 │          vpsubq       %ymm3,%ymm5,%ymm3
       │          vpxor        %xmm6,%xmm6,%xmm6
     2 │          mov          0x1c0(%rsp),%r14
  4264 │          vpgatherqq   %ymm7,(%r14,%ymm3,8),%ymm6 # Do array index
#+end_src

Huh what?! I don't really what is going on here, but I do know that the compiler
just vectorized our code for us! All the =vp= instructions are vector/packed
instructions! Magic! This probably explains the big speedup we get for =fastmod64=.

*Closer inspection:* As it turns out, the =32bit= versions were already
auto-vectorized before we implemented this last optimization. Probably because
the ~FastMod32~ type is smaller (two ~u64~) than the ~Fastmod64~ type (~u128~
and ~u64~) and hence easier to vectorize (and similar for =StrengthReduce32=).
But either way this last trick helps a lot for the =64bit= variants that will
be needed for large hashmaps.

** Compiling and benchmarking PTHash
Compiling PTHash was very smooth; just a =git clone=, submodule init, and
building /just worked/ :)

Running a benchmark similar to the ones here:
#+begin_src shell
 ./build -n 10000000 -c 7.0 -a 1 -e compact_compact -s 1234567890 --minimal --verbose --lookup
#+end_src
reports a query performance of =26ns/key=, similar to the =fastmod64=
performance I get.

Note that PTHash uses fixed-width bitpacking here, while I just store =u64='s
directly, but this shouldn't affect the time too much.

*Vectorization:* More interestingly, PTHash is not auto-vectorized by my
compiler, so I'm surprised it performs this well. Maybe the =vpgatherqq=
instruction just doesn't give that much speedup over sequential lookups -- I
don't know yet. But still, my equivalent code using =fastmod64= with $p_2 =
0.3m$ has =35ns/key= vs =26ns/key= for PTHash. Confusing.

*Branching:* PTHash compiles to a branchy version of =fastmod(x, p2) or
fastmod(x, m-p2)=, but is still fast.

** Compact encoding

Adding fixed-width encoding was easy using the =sucds= =CompactVector= type.
The generated code doesn't look so pretty though -- it branches on whether the
bits cross a =usize= boundary, whereas PTHash's implementation does an unaligned
read from a =*u8= to avoid this, which seems nicer.

** Find the $x$ differences

At this point, both =pthash-rs= and the original =PTHash= support encoding by a
single compacted vector, but there is still quite some time difference: =31ns=
vs =25ns=. Time to find all the differences.

This may or may not be the best approach, but I decided to put the assemblies
side-by-side.

*Exhibit A: the missing modulo* Ok, I won't bore you with the full assembly, but I found this in
the PTHash assembly:
#+begin_src asm
movabs     $0x999999ffffffffff,%rbx
#+end_src
with nothing similar in the rust version. Turns out that this is $0.6 \cdot
(2^{64}-1)$. Indeed, [[https://github.com/jermp/pthash/blob/master/include/utils/bucketers.hpp#L18][the code is]]:
#+begin_src c++
inline uint64_t bucket(uint64_t hash) const {
    static const uint64_t T = constants::a * UINT64_MAX;
    return (hash < T) ? fastmod::fastmod_u64(hash, m_M_num_dense_buckets, m_num_dense_buckets)
                        : m_num_dense_buckets + fastmod::fastmod_u64(hash, m_M_num_sparse_buckets,
                                                                    m_num_sparse_buckets);
}
#+end_src
note how it does $hash < 0.6 2^{64}$ instead of $hash \mm n < 0.6 n$ as written
in the paper for what FCH does.
Basically we can completely drop the $\mm n$ there! Sweet! That's $1$ of $3$
modulo operations gone :)

** =FastReduce= revisited

Earlier I mentioned the blogpost [[https://lemire.me/blog/2016/06/27/a-fast-alternative-to-the-modulo-reduction/][A fast alternative to the modulo reduction]]:
to map $h\in[2^k]$ to $[n]$ ($[n] := \{0, \dots, n-1\}$ here), instead of taking $h\mm n$, one can do
$h*n/2^k$ which is must faster to evaluate. The problem with this approach is
that it only uses the $\log_2 n$ high-order bits of $n$, discarding some
necessary entropy.

On second thought, it seems like this may still be useful though. There are two
modulo operations in the PTHash algorithm:
1. In the =bucket()= function, mapping from $[2^{64}]$ to $[p_2]$ or $[m - p_2]$.
2. In the =position()= function, mapping from $[2^{64}]$ to $[n]$.

And there is one related check:
3. In the =bucket()= function, check whether $h < p_1 \cdot 2^{64}$ (or $h \mm n
   \leq p_1 \cdot n$), which is actually also a reduction operation.

While we cannot use =FastReduce= twice, I think it may still be possible to use
it once. In particular, it should be fine to use a low-entropy hash for
bucket-selection since we anyway have collisions there -- that's the point of
bucketing in the first place.

A second optimization may be possible if we could find a function that uses the
lower $\log_2 n$ bits of the hash for =position()=. Then we can access $2\cdot
\log_2 n$ bits of entropy in total which should be sufficient to avoid
collisions with constant probability (via the birthday paradox).

One idea is something like $h \mm 2^{\lceil{\log_2 n\rceil}}$, but this is not
quite fair so it may not work out nicely.
But then again, maybe we could use that for the bucketing modulo, since it
possibly doesn't require (as much) fairness.

Or maybe we can just take the lower $32$ bits of $h$ and do $(h\mm 2^{32}) * n /
2^{32}$. That should probably work just fine :)


So, we have 7 possible reduction functions now:
1. =FastMod64=, same as =mod= but faster.
2. =FastMod32L=, taking the lower $32$ bits modulo $n$.
3. =FastMod32H=, taking the higher $32$ bits modulo $n$.
4. =FastReduce64=: $(h * n) >> 64$
5. =FastReduce32L=, fastreduce on the $32$ low bits: $((h \mm 2^{32}) * n) >> 32$
6. =FastReduce32H=, fastreduce on the $32$ high bits: $((h >> 32) * n) >> 32$

Excluding =mod= (which is never better than =FastMod64=, we can make $36$
combinations from this to use for the two modulo operations.

Not all combinations end up working (because of lack of entropy when e.g. only
the $32$ high bits are used). It's clear that =FastMod64= tends to be slow, and
that =Reduce= operations are usually (but not always) faster than =Mod= operations.

The big problem with this benchmark though seems to be that timings are quite
inconsistent (variance of a few nanoseconds), and that the quality of generated
code (auto-vectorization and loop unrolling) depends on a lot of things and is
somewhat unpredictable/inconsistent.

#+caption: Query time (=ns=) for different combinations of reduction functions. Missing entries either fail or slow down construction.
| =bucket= reduce ($\mm m$) \ =position= reduce ($\mm n$) | =FM32L= | =FM32H= | =FM64= | =FR32L= | =FR32H= | =FR64= |
| =FastMod32L=                                            |         |      20 |     27 |         |      19 |     20 |
| =FastMod32H=                                            |      18 |         |     26 |      19 |         |        |
| =FastMod64=                                             |      29 |      27 |     33 |      28 |      26 |     27 |
| =FastReduce32L=                                         |         |      18 |        |         |      23 |     18 |
| =FastReduce32H=                                         |         |         |        |         |         |        |
| =FastReduce64=                                          |         |         |        |         |         |        |

Note that the last two rows fail in particular, because they strongly correlate
with the check whether elements belong in a small or large bucket, $h(x, s) <
p_1$.

** TODO Is there a problem if $\gcd(m, n)$ is large?


** Faster hashing

The current implementation uses [[https://github.com/aappleby/smhasher/wiki/MurmurHash2][=Murmur2=]] to hash both the key $x\mapsto h(x, s)$ and the
pilot $k_i \mapsto h(k_i, s)$. While this hash is fast, it's still quite some
instructions. Instead, especially for the $h(k_i, s)$, we may be able to get
away with either no or a much simpler hash.

*No hash:* A first try of $h_0(k_i, s) := k_i$ returns in failures because the
$k_i$ become too large.

*Multiplication hash (=MulHash=):* So we do need more /mixing/ of bits rather than just
incrementally increasing $k_i$ starting at $0$. One common way of doing that is
simply to multiply by a large semi-random $64$bit integer. In particular,
=Murmur= also does this, so let's just reuse their mixing constant and set:
$$
h_1(k_i, s) := (0xc6a4a7935bd1e995 \cdot k_i) \mm 2^{64}.
$$
(I tried looking for documentation on why this constant was chosen, but there
doesn't seem to be more to it than /it works/.)

In experiments, this gives anywhere between $1$ and $4$ nanoseconds of speedup.

** An experiment
Another fun comparison is here, where I use =MulHash= for $k_i$ and replace the
=MurmurHash= for $h(x)$ by simply the identity operation (since we're testing on
uniform random $x$ anyway):

#+caption: Assembly of $index()$ function when using =Murmur= (which takes $17/38$ instructions): =18ns/query=.
#+begin_src asm
    17 │1f0:   mov        (%rsi,%rax,1),%rdx
    11 │       imul       %r13,%rdx           # Start of Murmur
    32 │       mov        %rdx,%rbx
   106 │       shr        $0x2f,%rbx
     7 │       xor        %rdx,%rbx
    23 │       imul       %r13,%rbx
    24 │       mov        %rdi,%rdx
    99 │       movabs     $0x35253c9ade8f4ca8,%rbp
    11 │       xor        %rbp,%rdx
    27 │       xor        %rbx,%rdx
    28 │       imul       %r13,%rdx
    92 │       mov        %rdx,%rbx
    15 │       shr        $0x2f,%rbx
    15 │       xor        %rdx,%rbx
    28 │       imul       %r13,%rbx
    95 │       mov        %rbx,%rbp
    16 │       shr        $0x2f,%rbp
    17 │       xor        %rbx,%rbp           # End of Murmur
    25 │       mov        %ebp,%edx
    64 │       cmp        %rbp,%r8
    43 │     ↓ jbe        250                 # Branch for bucket index
     9 │       imul       %r14,%rdx
    12 │       shr        $0x20,%rdx
    27 │     ↓ jmp        25d
       │       cs         nopw 0x0(%rax,%rax,1) # nop; for code alignment
     7 │250:   imul       %r15,%rdx
     4 │       shr        $0x20,%rdx
    11 │       add        0x10(%rsp),%rdx
  5088 │25d:   mov        (%r9,%rdx,8),%rdx   # Memory lookup -- most waiting is here.
   301 │       imul       %r13,%rdx
    98 │       xor        %rbp,%rdx
   453 │       mulx       %r11,%rdx,%rdx
   100 │       cmp        %rdx,%r10
     2 │     ↓ jbe        6e8
    13 │       add        %rdx,%r12
    10 │       add        $0x8,%rax
    37 │       cmp        %rax,%rcx
   100 │     ↑ jne        1f0
#+end_src

#+caption: Assembly of $index()$ function when using $h(x) = x$ instead: =7ns/query=.
#+begin_src asm
    72 │4a0:   mov        (%rcx,%rbp,1),%rbx
    38 │       mov        %ebx,%edx
    21 │       cmp        %rbx,%rsi
    36 │     ↓ jbe        4c0                 # Branch for bucket index
    16 │       imul       %r15,%rdx
    22 │       shr        $0x20,%rdx
    24 │     ↓ jmp        4cb
       │       data16     cs nopw 0x0(%rax,%rax,1) # code alignment
       │4c0:   imul       %r14,%rdx
    20 │       shr        $0x20,%rdx
    21 │       add        %r10,%rdx
  1895 │4cb:   mov        (%rdi,%rdx,8),%rdx  # Memory lookup -- most waiting is here
   172 │       imul       %r13,%rdx
    75 │       xor        %rbx,%rdx
   252 │       mulx       %r9,%rdx,%rdx
    56 │       cmp        %rdx,%r8
       │     ↓ jbe        706
    43 │       add        %rdx,%r11
    26 │       add        $0x8,%rbp
       │       cmp        %rbp,%rax
    34 │     ↑ jne        4a0
#+end_src

=MurmurHash= takes slightly less than half the instructions, but removing them
gives almost $2.5\times$ speedup! My current thinking is that this is not so
much due to the reduced instruction count itself (the CPU is stalling anyway to
wait for memory), but rather due to the better pipelining it results in: when
loop iterations are shorter (in number of assembly instructions), pipelining can
look ahead more iterations, and hence does a better job at prefetching memory.
But even with this short loop, around two thirds of the time is still spend
waiting for memory.

*Conclusion 1.:* I should really write code with prefetching.

*Conclusion 2.:* It's time to use =perf stat= for some metrics on /branch
mispredictions/ and /instructions per cycle/.

** Compiler struggles

*Auto-vectorization:* The compiler is quite eager to generated vectorized assembly code.
It's quite unpredictable when auto-vectorization triggers, and it seems I have
to keep at least one branch in the hot loop to prevent it. The vectorized code
seems bad for a few reasons:
- *Gather instructions* (=vpgatherqq=) are slow.
- *Pipelining:* It seems that pipelining works much better for the scalar
  version, being able to look ahead further and keeping busy while waiting for
  memory to load.

*Even worse:* Also, it did the following terrible thing. Starting with this piece of innocent looking code:
#+begin_src rust
if likely(p < self.n0) {
    p
} else {
    unsafe { *self.free.get_unchecked(p - self.n0) }
}
#+end_src
the compiler decided to generate:

#+attr_html: :class inset
[[file:bad-asm.png]]

Basically: it created a branchless implementation of this if statement where the
=false= branch is always executed. But that branch is super slow! Basically a
completely unnecessary read from main memory!
For now I'll just completely remove the =false= branch to prevent this issue...

** Prefetching, at last

Without further ado, here we go:

#+begin_src rust
#[inline(always)]
pub fn index_stream<'a, const L: usize>(
    &'a self,
    xs: &'a [Key],
) -> impl Iterator<Item = usize> + 'a {
    let mut next_hx: [Hash; L] = xs.split_array_ref().0.map(|x| self.hash_key(&x));
    let mut next_i: [usize; L] = next_hx.map(|hx| self.bucket(hx));
    xs[L..].iter().enumerate().map(move |(idx, next_x)| {
        let idx = idx % L;
        let cur_hx = next_hx[idx];
        let cur_i = next_i[idx];
        next_hx[idx] = self.hash_key(next_x);
        next_i[idx] = self.bucket(next_hx[idx]);
        // TODO: Use 0 or 3 here?
        // I.e. populate caches or do a 'Non-temporal access', meaning the
        // cache line can skip caches and be immediately discarded after
        // reading.
        unsafe { prefetch_read_data(self.k.address(next_i[idx]), 3) };
        let ki = self.k.index(cur_i);
        let p = self.position(cur_hx, ki);
        p
    })
}
#+end_src

For $L = 64$, this is around twice as fast as the non-streaming/non-prefetching version!

In our $n=10^7$ benchmark, *this reduces latency to =4.2ns=*!!
For the larger $n=10^8$ benchmark, latency is *=7.5ns=*, down from *=28ns=* of
the original PTHash paper! (But note that I don't do any compression here.)

And this is without vectorization still :)

** TODO: Prefetching with vectorization




#+print_bibliography:
