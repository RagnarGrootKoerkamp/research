#+title: [WIP] PtrHash: Perfect Hashing at the Speed of Memory
#+filetags: @paper mphf wip
#+OPTIONS: ^:{} num:t
#+hugo_front_matter_key_replace: author>authors
#+toc: headlines 3
#+date: <2024-11-20 Wed>

$$
\newcommand{\part}{\mathsf{part}}
\newcommand{\bucket}{\mathsf{bucket}}
\newcommand{\slot}{\mathsf{slot}}
\newcommand{\reduce}{\mathsf{reduce}}
\newcommand{\h}{\mathsf{h}}
\newcommand{\hp}{\mathsf{h}_{\mathsf{p}}}
\newcommand{\C}{\mathsf{C}}
\newcommand{\select}{\mathsf{select}}
\newcommand{\free}{F}
\newcommand{\mphf}{\mathsf{H_{mphf}}}
$$

This is the work-in-progress paper on PtrHash. The original blog post on its
development is [[../ptrhash][here]].

* Abstract
:PROPERTIES:
:UNNUMBERED:
:END:
*Motivation.*
Given a set $K$ of $n$ keys, a minimal perfect hash function (MPHF) is a
collision-free bijective map $f$ from $K$ to $\{0, \dots,
n-1\}$. These functions have uses in databases, search engines, and are used in
bioinformatics indexing tools such as Pufferfish (using BBHash), and
Piscem (both using PTHash).
Our main motivating usage is that as in SSHash, where PTHash only takes $5\%$ of the
total space.
Thus, this work presents a (minimal) perfect hash function that first prioritizes query
throughput, while also allowing efficient construction for $10^9$ or more elements
using under $3$ bits of memory per key.

*Contributions.*
PtrHash builds on
PTHash by 1) partitioning the table for faster construction, and 2) using cuckoo
hashing to find $8$ bit pilots, which 3) makes further compression redundant. We further
speed up queries by 4) simplifying hash functions and modulo operations, and 5)
streaming multiple queries in parallel and prefetching memory reads.

*Results.*
(TODO: update numbers. TODO: add something about speed of memory) We implemented PtrHash in Rust and
show that at $3$ bits/key memory usage it is $3\times$ faster to construct
than PTHash while achieving $8$ns/key query throughput, $5\times$ faster than
state-of-the-art methods.

*Source:* [[https://github.com/RagnarGrootKoerkamp/ptrhash][github.com/RagnarGrootKoerkamp/ptrhash]]

* Introduction
Given a set of $n$ keys $\{k_0, \dots, k_{n-1}\}$,
a /hash function/ maps them to some co-domain $[m] := \{0, \dots, m-1\}$.
When a hash is injective (collision-free), it is also called /perfect/.
When additionally it is surjective onto $[n]$, it is /minimal/.
Thus, a /minimal perfect hash function/ (MPHF) bijectively maps a set of $n$ keys onto $[n]$.

*Metrics.*
Various aspects of MPHF data structures can be optimized.
First, one could minimize its space usage and try to
approach the $\log_2(e)=1.44$ bits/key lower bound [cite:@mehlhorn82_mphf_size].
Indeed, there are many recent works in this direction, such as Bipartite
ShockHash-RS, which gets below 1.5 bits/key [cite:@shockhash;@bipartite-shockhash;@phf-thesis].

In this paper, we focus primarily on optimizing for query throughput and
secondary on construction speed, while relaxing space usage up to 3 bits/key.
This continues the line of work of FCH [cite:@fch], PTHash [cite:@pthash;@pthash-2], and
PHOBIC [cite:@phobic], that all provide relatively fast queries.

*Problem statement.*
Construct a /minimal perfect hash function/
data structure $\mphf$ that is fast to query, ideally in a single memory access,
and fast to construct, while staying below 3 bits/key of space.

*Motivation.*
Our main motivating application is to optimize the use of PTHash in SSHash
[cite:@sshash], a data structure to index a set of $k$-mers (sequences
of $k$ DNA bases).
There, the MPHF only takes around $5\%$ of the total space. Thus, a slightly
increased space usage of the MPHF has little effect on the total space, while
faster lookups could significantly improve the overall query speed. In this application,
$k$-mers are typically encoded as 64-bit integers, and thus we will focus our
attention on integer keys.

Further applications can be found in domains such as networking [cite:@Lu_2006],
databases [cite:@Chang_2005], and
full-text indexing [cite:@Belazzougui_2014], where one could imagine hashing IP addresses,
URLs, or (compact) suffix-trie edge labels.

*Contributions.*
We introduce PtrHash, a minimal perfect hash function that is primarily optimized for
query throughput and construction speed, at the cost of slightly more memory usage.
Its main novelties are:
1. the use of fixed-width 8-bit pilots, leading to very simple lookups;
2. a pilot search based on cuckoo-hashing;
3. a /remap table/ based on a per-cacheline Elias-Fano encoding, CacheLineEF;
4. construction in parts, while only using a single remap table, again
   simplifying lookups;
5. the use of /prefetching/ to /stream/ multiple queries in parallel.

*Results.*
We propose two parameter configurations, /fast/ and /compact/.
When using 300 million string keys, the fast configuration takes 3.0 bits/kmer and is nearly as fast to construct as the fastest
other methods. It provides $2.8\times$ faster queries when looping naively, or $5.4\times$ faster when streaming.
Similarly, the compact configuration takes 2.12 bits/kmer, and is over $3\times$
faster to query (or $6\times$ when streaming) than other methods requiring
similar space and construction time.

When using $10^9$ integer keys instead, PtrHash can achieve a throughput of up to
12ns/key when looping over queries, or even 8ns/key when streaming, close to matching the
maximum throughput of random memory accesses of a single thread. In a
multi-threaded setting, PtrHash can fully saturate the DDR4 memory bandwidth.

* Related work
There is a vast amount of literature on (minimal) perfect hashing. Here we only
give a highlight of recent approaches, and refer the reader to Section 2 of
[cite:@pthash-2] and Sections 4 and 8 of the thesis of Hans-Peter Lehmann
[cite:@phf-thesis], which is the basis for this section.

*Space lower bound.*
Given $n$ keys and a random hash function to $[n]$, the probability that this is
a MPHF is $n!/n^n$. From this, it follows that at least
$\log_2(n^n/n!)\approx n\log_2(e)$ bits of information are needed to `steer' the hash
function in the right direction [cite:@mehlhorn82_mphf_size].
Now, a naive approach is to use a seeded hash function, and try
$O(e^n)$ seeds until a perfect hash function is found. Clearly, that is not
feasible in practice.

*Brute-force.*
When $n$ is small, $e^n$ can be sufficiently small to allow a bruteforce search
over $n$. RecSplit exploits this by first partitioning the input
keys first into buckets, and then recursively splitting buckets until they have
size at most $\ell \leq 16$. These /leafs/ can then be solved using brute-force, and the
overall space usage can be as low as 1.56 bits/kmer. SIMDRecSplit significantly
improves the construction time by using a meet-in-the-middle approach for the
leafs, and generally speeds up the implementation.

*Graphs.*
SicHash [cite:@sichash] and its predecessor BPZ [cite:@bpz] are based on
/hypergraph peeling/: nodes are the $n$ hash values, and each key
corresponds to a size-$r$ hyper-edge. Then keys can be assigned a value
one-by-one as long as each set of $k$ keys covers at least $k+1$ values. This
is also alike cuckoo hashing, where each key has $r=2$ target locations.
ShockHash [cite:@shockhash] then takes the RecSplit framework and uses an $r=2$
cuckoo table for the base case. It then tries $O((e/2)^n)$ seeds until one is
found that allows building the cuckoo hash table.
Bipartite ShockHash-RS [cite:@bipartite-shockhash]
further improves this by using meet-in-the-middle on the seeds, improving the
construction time to $O((\sqrt{e/2})^n) = O(1.166^n)$. This is currently the
most space efficient approach. Bipartite ShockHash-Flat is a variant that trades
space for more efficient queries.

*Fingerprinting.*
A completely different technique was introduced by
[cite:@chapman_2011;@muller_2014], and used in BBHash [cite:@bbhash]. Here, the
idea is to start with any hash function mapping into $[\gamma n]$ for some
$\gamma \geq 1$. Any slots that have exactly one element mapping to them are
marked with a 1, and the remaining $n_1$ elements are processed recursively,
mapping them to $[\gamma n_1]$. Lookups are then done using rank queries on this
bitvector. FMPH [cite:@fmph] has a much faster implementation of the construction that goes
down to 3.0 bits/kmer. FiPS [cite:@phf-thesis] is a variant that trades some
space in the rank data structure for faster queries. FMPHGO [cite:@fmph] is
variant that first splits keys into buckets, then uses a seeded hash function
that has a low number of collisions, and only then recurses into colliding keys.
This reduces the space usage and number of recursion steps, leading to faster
queries, but takes longer to construct.

*Bucket placement.*
Lastly, we cover the line of work this paper builds on.
These methods first group the keys into
buckets of a few keys. Then, keys in the buckets are assigned their hash value
one bucket at a time, such that newly assigned values do not collide with
previously taken values. All methods iterate different possible key assignments
for each bucket until a collision-free one is found, but differ in the way
hash values are determined. To speed up the search for keys, large buckets are
placed before small buckets.

FCH [cite:@fch] uses a fixed number of bits to encode the seed for each bucket and
uses a /skew/ distribution of bucket sizes. The seed stored in each bucket
determines how far the keys are /displaced/ (shifted) to the right from their
initially hashed position, and a fallback hash can be used if needed, and
construction can fail if that also doesn't work. CHF [cite:@chd] uses uniform
bucket sizes, but uses a variable-width encoding for the seeds.
PTHash [cite:@pthash] combines these two ideas and introduces a number of
compression schemes for the seeds values, that are called /pilots/. Instead of
directly generating an MPHF, it first generates a PHF to $[n']$ for
$n'=n/\alpha \approx n/0.99$, and values mapping to $\geq n$ are /remapped/ to
the skipped values in $[n]$. PTHash-HEM [cite:@pthash-2] first partitions the keys, and uses this
to build multiple parts in parallel. This also enables external-memory construction.
Lastly, PHOBIC [cite:@phobic] improves from the simple /skew/ distribution of
FCH to an /optimal bucket assignment function/, which speeds up construction and
enables smaller space usage. Secondly, it partitions the input into parts of size
$2500$ and uses the same number of buckets for each part. Then, it uses that the
pilot values of the $i$'th bucket of each part follow the same distribution, and
encodes them together. Together, this saves 0.17 bits/key over PTHash.

* PtrHash

The core design goal of PtrHash[fn::The
PT in PTHash stand for /Pilot Table/. The
author of the present paper mistakenly understood it to stand for Pibiri and
Trani, the authors of the PTHash paper. Due to the current author's
unconventional last name, and PTGK not sounding great, the first initial (R) was
appended instead. As things go, nothing is as permanent as a temporary name.
Furthermore, we follow the Google style guide and avoid a long run of uppercase
letters, and write PtrHash instead of PTRHash.]
is to simplify PTHash to speed up both query speed
and construction time, at the cost of possibly using slightly more memory.
We first give a high level overview of PtrHash ([[*Overview]]). Then, we explain
specific parts of PtrHash in more detail.

** Overview

#+name: overview
#+caption: Overview of PtrHash on $n=23$ keys. The keys are hashed into $[H] = [2^{64}]$ and this range is split into $P=2$ parts and $B=5$ buckets per part. In red are four keys hashing to the same bucket in the first part, and in blue are three keys belonging to the same bucket in the second part. The /pilots/ of the $P\cdot B=10$ buckets in the highlighted area are the main component of the data structure, and control to which /slots/ keys in the bucket are mapped to avoid collisions. The blue highlighted key is initially mapped to a position $\geq n$, and thus (along with the other yellow cells) /remapped/ into an empty slot $<n$ via a (compressed) table of free slots.
#+attr_html: :class inset large
[[file:./overview.drawio.svg]]


Before going into details, we first briefly explain the fully constructed
PtrHash data structure and how to query it, see [[overview]] and [[query-code]]. We also
highlight differences to PTHash [cite:@pthash] and PHOBIC [cite:@phobic].

*Parts and buckets.*
The input is a set of $n$ /keys/ $\{k₀, ̣\dots, k_{n-1}\}$ that we want to hash to
$n$ /slots/ $[n]:=\{0, \dots, n-1\}$.
We first hash the keys using a 64-bit hash function $\h$ into
$\{\h(k_0), \dots, \h(k_{n-1})\}$. The total space of hashes $[2^{64}]$
is equally partitioned into $P$ /parts/, and the part of a key is easily found
as $\left\lfloor P\cdot \h(k_i) / 2^{64}\right\rfloor$ [cite:@fast-range].
Then, the expected $n/P$ keys in each part are partitioned into $B$ non-uniform /buckets/:
each key has a /relative position/ $x$ inside the part, and this is passed through
a /bucket
assignment function/ $\gamma: [0,1)\mapsto[0,1)$ such as $x\mapsto x^2$
that controls the distribution of expected bucket
sizes [cite:@phobic], as explained in detail in [[#bucket-fn]].
The result is then scaled to a bucket index in $[B]$:
\begin{align}
\begin{split}
  \part(k_i) &:= \left\lfloor P\cdot \h(k_i) / 2^{64}\right\rfloor,\\
  x &:= \big((P\cdot \h(k_i)) \bmod 2^{64}\big)/2^{64},\\
  \bucket(k_i) &:= \left\lfloor B\cdot \gamma(x)\right\rfloor.
\end{split}\label{eq:partbucket}
\end{align}

*Slots and pilots.*
Now, the goal and core of the data structure is to map the $n/P$ expected keys in each part to $S\approx
(n/P)/\alpha$ /slots/, where $\alpha\approx 0.99$ gives us $\approx 1\%$ extra slots to
play with. The pilot for each bucket controls to which slots its keys map.
PtrHash uses fixed-width $8$-bit /pilots/ $\{p_0, \dots,
p_{P\cdot B-1}\}$, one for each bucket. Specifically, key $k_i$ in bucket $b=\bucket(k_i)$ with pilot $p_b$
maps to slot
\begin{equation}
  \slot(k_i) := \part(k_i) \cdot S + \reduce(\h(k_i) \oplus \hp(p_b), S),\label{eq:slot}
\end{equation}
where $\reduce(\cdot, S)$ maps the random $64$ bit integer into $[S]$ as explained below.

Compared to PHOBIC and PTHash(-HEM) [cite:@pthash-2], there are two differences
here.
First, while we still split the input into parts, we assign each part the
/same/ number of slots, instead of scaling the number of slots with the
/actual/ size of each part. At query time, this removes the need to look up the
size of the key's part. Second, previous methods search for arbitrary large
pilot values that require some form of compression to store efficiently. Our
8-bit pilots can simply be stored in an array so that lookups are simple.

We now go over some specific details.

*Hash functions.*
The 8-bit pilots $p_b$ are hashed into pseudo-random $64$ integers by
using FxHash [cite:@fxhash] for $\hp$,
which simply multiplies the pilot with a /mixing constant/ $\C$:
\begin{equation}
\hp(p) := \C \cdot p.
\end{equation}

When the keys are $64$ bit integers, we use this same FxHash algorithm to hash
them ($\h(k) := \C\cdot k$), since multiplication by an odd constant is invertible modulo $2^{64}$ and
hence collision-free.
For other types of keys, the hash function depends on the number of elements. When the
number of elements is not too far above $10^9$, the probability of hash
collisions with a $64$ bit hash function is sufficiently small, and we use
the $64$ bit variant of xxHash [cite:@xxhash;@xxhash-rust].
When the number of keys goes beyond $2^{32} \approx 4\cdot 10^9$, the
probability of $64$ bit hash collisions increases. In this case, we use the
$128$ bit variant of xxHash.
The high $64$ bits determine the part and bucket in Equation \ref{eq:partbucket}, and the low
$64$ bits are used in Equation \ref{eq:slot} to determine the slot.

*The reduce function.* When $64$ bit hashes are used, we must ensure that all bits of
the hash are used to avoid collisions. A simple choice would be $\reduce(x,S) = x\bmod S$, which uses
all bits when $S$ is /not/ a power of $2$ and takes two multiplications using
'fast mod' [cite:@fast-mod]. Instead, we use $S=2^s$, so that $x\bmod 2^s$ is a simple bit-mask. Unfortunately, this only uses
the lower $s$ bits of the hash, while the $part$ and $bucket$ functions use the
high $\log_2(P\cdot B)$ bits, leaving some entropy in the middle bits unused.

As a solution, we first multiply $x$ by the mixing constant $\C$, and then take the low
$s$ bits of the high half. This uses all input bits and
only needs a single multiplication, giving a small speedup over fast mod:
\begin{equation}
  \reduce(x, 2^s) := \left\lfloor \C\cdot x/2^{64}\right\rfloor \bmod 2^s.
\end{equation}

*Remapping.* Since each part has slightly ($\approx 1\%$) more slots than keys, some keys will map to an
index $\geq n$, leading to a /non-minimal/ perfect hash function. To fix this,
those are /remapped/ back into the 'gaps' left behind in slots $<n$ using a
(possibly compressed) lookup table. This is explained in detail in [[#remapping]].

Whereas PTHash-HEM uses a separate remap /per part/, PtrHash only has a single
'global' remap table.

*Construction.* The main difficulty of PtrHash is during construction ([[#construction]]), where we must find values of the
pilots $p_j$ such that all keys indeed map to different slots.
Like other methods, PtrHash processes multiple parts in parallel.
Within each part, it sorts the buckets from large to
small and 'greedily' assigns them the smallest pilot value that maps the keys in
the bucket to slots that are still free.
Unlike other methods though, PtrHash only allows pilots up to $255$. When no
suitable pilot is found, we use a method similar to (blocked) cuckoo hashing
[cite:@cuckoo-hashing;@dary-cuckoo-hashing]: a pilot with a minimal number of collisions is chosen,
and the colliding buckets are 'evicted' and will have to search for a new pilot.

*Parameter values.*
In practice, we usually use $\alpha=0.99$.
Similar to PHOBIC, the number of buckets per part is set to $B = (\alpha\cdot
S)/\lambda$, where $\lambda$ is the expected size of each bucket and is around
$3$ to $4$.
The number of parts is $P=\lceil n/(\alpha S)\rceil$.
Smaller parts fit better in cache and hence are faster to construct, while too
small parts have too much variance in their size, causing some parts to possibly have
more than $S$ keys in them. Thus, we choose $S$ as the smallest size for which
the probability that any part is over-subscribed is sufficiently small.

*Streaming queries.* PtrHash supports /streaming/ queries, where multiple
queries are processed in parallel. This enables us to prefetch pilots from
memory, and thus increase throughput and better use the available memory bandwidth.

#+name: parameters
#+caption: Input parameters with typical values, and computed variables.
| Name                                                                  | Definition                                                        |
| $\alpha = 0.99$                                                       | Load factor. Expected number of keys per part is $\alpha\cdot S$. |
| $\lambda=4$                                                           | Expected number of elements per bucket.                           |
| $\gamma(x) = \frac{255}{256}\cdot (x^2+x^3)/2 + \frac{1}{256}\cdot x$ | Bucket function controlling relative bucket sizes.                |
| $n$                                                                   | Total number of keys.                                             |
| $S = 2^{18}$                                                          | Number of slots per part.                                         |
| $P = \lceil n/(\alpha \cdot S)\rceil$                                 | Number of parts.                                                  |
| $B = \lceil(\alpha \cdot S)/\lambda\rceil$                            | Number of buckets per part.                                       |

#+name: query-code
#+caption: Rust code for a simple implementation of the data structure and query function.
#+begin_src rust
struct PtrHash {
    n: usize,         // Number of elements
    P: usize,         // Number of parts
    B: usize,         // Buckets per parts
    S: usize,         // Slots per parts
    lgS: usize        // S = 2^lgS
    pilots: Vec<u8>,  // P*B pilots
    free: Vec<usize>, // P*S-n remap indices
}

/// Multiply a and b as if they are fractions of 2^64.
/// Compiles to taking the high 64 bits of the 64x64->128 multiplication.
fn mul(a: usize, b: usize) -> usize {
    ((a as u128 * b as u128) >> 64) as usize
}

impl PtrHash {
    fn query(&self, key: Key) -> usize {
        let h = self.hash(key);
        let part = mul(self.P, h);
        let bucket = mul(self.B, self.gamma(self.P * h));
        let pilot = self.pilots[bucket];
        let slot_in_part = mul(self.C, h ^ self.hash_pilot(pilot)) & (self.S - 1);
        let slot = (part << self.lgS) + slot_in_part;
        if slot < self.n {
            return slot
        } else {
            return self.free[slot - self.n]
        }
    }
}
#+end_src

** Construction
:PROPERTIES:
:CUSTOM_ID: construction
:END:
Both PTHash-HEM and PHOBIC first partition the keys into parts, and then build
an MPHF part-by-part, optionally in parallel on multiple threads.
Within each part, the keys are randomly partitioned into
/buckets/ of average size $\lambda$ ([[overview]]).
Then, the buckets are sorted from large to small, and one-by-one /greedily/ assigned a
/pilot/, such that the keys in the bucket map to /slots/ not yet covered by earlier buckets.

As more buckets are placed, there are fewer remaining empty slots, and searching for pilots becomes harder.
Hence, PTHash uses $n/\alpha > n$ slots
to ensure there sufficiently many empty slots for the last pilots. This speeds
up the search and reduces the values of the pilots.
PHOBIC, on the other hand, uses relatively small parts of size $2500$, so that
the search for the last empty slot usually shouldn't take much more than $2500$ attempts.
Nevertheless, a drawback of the greedy approach is that pilots values have an uneven
distribution, making it hard to compress them efficiently.

*Hash-evict[fn::We would have preferred to call this method hash-displace, as
/displace/ is the term used instead of /evict/ in e.g. the cuckoo  filter by [cite/t:@cuckoo-filter].
Unfortunately, /hash and displace/ is also the name of another MPHF introduced
by [cite/t:@hash-displace], that was then extended into /compressed
hand-and-displace/ (CHD) by [cite/t:@chd]. There, the
to-be-inserted key (rather than the existing key) is /displaced/ by applying a linear shift
to its initial position.].* In PtrHash, we instead use /fixed width/, single byte pilots. To achieve
this, we use a technique resembling cuckoo hashing [cite:@cuckoo-hashing], as
shown in [[construction-code]].
As before, buckets are greedily /inserted/ from large to small. For some buckets,
there may be no pilot in $[255]$ such that all its keys map to empty slots. When
this happens, a pilot is found with the lowest weighted number of /collisions/.
The weight of a collision with an element of a bucket of size $s$ is $s^2$, to prevent
/evicting/ large buckets, as those are harder to place.
The colliding buckets are evicted by emptying the slots they map to and
pushing them back onto the priority queue of remaining buckets.
Then, the new bucket is inserted.

#+name: construction-code
#+caption: Conceptual Rust code for determining the pilot values for each part. In practice, a number of optimizations are made.
#+begin_src rust
/// Given the buckets of hashed keys for a part, search for pilot values.
fn pilots_for_part(&self, buckets: Vec<&[Hash]>) -> Vec<u8> {
    let mut pilots = vec![0; self.B];                    // One pilot per bucket.
    let mut slots = vec![None; self.S];       // Bucket idx mapping to each slot.

    // A priority queue (max-heap) of buckets.
    let mut queue = BinaryHeap::from_iter(
        (0..buckets.len()).iter().map(|i| (buckets[i].len(), i))
    );

    while let Some((_, i)) = queue.pop() {       // Insert next-largest bucket i.
        pilots[i] = self.find_pilot(buckets[i], &mut slots);
        for &h in buckets[i] {
            let slot = self.slot_for_hashed_key(h, pilots[i]);
            if let Some(j) = slots[slot] {           // Evict colliding bucket j.
                for &h_j in buckets[j] {
                    let slot_j = self.slot_for_hashed_key(h_j, pilots[j]);
                    slots[slot_j] = None;
                }
                todo.push((buckets[j].len(), j));
            }
            slots[slot] = Some(i);
        }
    }

    pilots
}
#+end_src

[APPENDIX] *Optimizations.* In order to speed up the code to search for pilots, a number of
optimizations are made to the conceptual idea of [[construction-code]].
1. *=taken= bit mask.* Like PTHash and PHOBIC, Instead of determining whether a slot is free by
   checking the =slots= array for the optional index of the bucket mapping
   there, we keep a separate bit mask =taken= that takes only $1$ bit instead
   of $32$ bits per element. This allows for better caching and hence faster access.
2. *Collision-free hot path.* When searching for pilots, we first test if there
   is a pilot without any collisions. This is usually the case, and is faster
   since it only needs access to =taken=, not =slots=. Additionally, where there
   /is/ a collision, we know a pilot is optimal when it collides with exactly
   one bucket of minimal size.
3. *Avoiding loops.* To avoid repeated patterns of the same buckets evicting
   each other, the search for a pilot starts at a random number in $[256]$,
   rather than at $0$.
4. *Avoiding loops more.* Each time a bucket is placed that evicted some other
   bucket(s), it is added to a list of the $16$ most recently placed buckets.
   Buckets in this list are never evicted. This avoids short cycles, where for
   example two buckets keep evicting each other for the same slot.

*Analysis.* Unfortunately, we do not currently have a formal analysis showing
that the hash-evict method works with high probability given that certain
criteria are met. In [[*Results]], we will show some practical results.

** Remapping using CacheLineEF
:PROPERTIES:
:CUSTOM_ID: remapping
:END:
Like PTHash, PtrHash uses a parameter $0<\alpha\leq 1$ to use a total of
$n'=n/\alpha$ slots, introducing $n'-n$ additional free slots.
As a result of the additional slots, some, say $R$, of the keys will map to positions $n\leq
q_0<\dots< q_{R-1}< n'$, causing the perfect hash function to not be /minimal/.

*Remapping.* Since there are a total of $n$ keys, this means there are exactly $R$ empty
slots ('gaps') left behind in $[n]$, say at positions $L_0$ to $L_{R-1}$.
We /remap/ the keys that map to positions $\geq n$ to the empty slots at
positions $< n$ to obtain a /minimal/ perfect hash function.

A simple way to store the remap is as a plain array $\free$, such that
$\free[q_i-n] = L_i$.
PTHash encodes this array using Elias-Fano coding [cite:@elias;@fano], after setting undefined
positions of $\free$ equal to their predecessor.
The benefit of a plain $\free$ array is fast and cache-local lookups, whereas
Elias-Fano coding provides a more compact encoding that requires multiple
lookups to memory.

*CacheLineEF.* We propose using Elias-Fano coding on a per-cache line basis, so that each
lookup only requires a single read from memory.
First, the list of non-decreasing $\free$ positions is split into chunks of
$C=44$ values $\{v_0, \dots, v_{43}\}$, with the last chunk possibly containing fewer values.
Then, each chunk is encoded into $64$ bytes that can be stored as single cache
line, as shown in [[cacheline-ef]].

We first split all indices into their $8$ /low/ bits ($v_i \bmod 2^8$) and $32$
/high/ bits ($\lfloor v_i/2^8\rfloor$). Further, the high part is split into an
/offset/ (the high part of $v_0$) and the /relative/ high part:
\begin{equation}
v_i =
2^8\cdot\underbrace{\lfloor v_0/256\rfloor}_{\text{Offset}} +
2^8\cdot \underbrace{\left(\lfloor v_i/256\rfloor - \lfloor
v_0/256\rfloor\right)}_{\text{Relative high part}}
+\underbrace{(v_i\bmod 2^8)}_{\text{Low bits}}.
\label{eq:clef}
\end{equation}
This is stored as follows.
- First, the $32$ bit offset $\lfloor v_0/256\rfloor$ is stored.
- Then, the relative high parts are encoded into $128$ bits. For each $i\in[44]$, bit $i + \lfloor
  v_i/256\rfloor - \lfloor v_0/256\rfloor$ is set to 1.
  Since the $v_i$ are increasing, each $i$ sets a distinct bit, for a total of $44$ set bits.
- Lastly, the low $8$ bits of each $v_i$ are directly written to the $44$ trailing bytes.

#+name: cacheline-ef
#+caption: Overview of the CacheLineEF datastructure.
#+attr_html: :class inset large
[[file:./cacheline-ef.drawio.svg]]

*Lookup.* The value at position $i$ is found by summing the terms of Equation
\ref{eq:clef}. The offset and low bits can be read directly.
This relative high part can be found as $256\cdot(\select(i)-i)$, where $\select(i)$ gives
the position of the $i$'th 1 bit. In practice, this can be implemented
efficiently using the =PDEP= instruction provided by the BMI2 bit manipulation
instruction set [cite:@fast-select]:
[DROP?] this operation can /deposit/ the mask =1<<i= onto our bit pattern, so that the
1 ends up at the position of the $i$'th one of our pattern. Then, it suffices
to count the number of trailing zeros, which is provided by the =TZCNT=
instruction in BMI1.

*Limitations.* CacheLineEF uses $64/44\cdot 8 = 11.6$ bits per value, which is
more than the usual Elias-Fano, which for example takes $8+2=10$ bits per value for data
with an average stride of $256$.
Furthermore, values are limited to $40$ bits, covering $10^{12}$ items.
The range could be increased to $48$ bit numbers by storing $5$ bytes of the
offset, but this has not been necessary so far.
Lastly, each CacheLineEF can only span a range of around $(128-44)\cdot 256 =
21\ 504$, or an average stride of $500$.
This means that for PtrHash, we only use CacheLineEF when $\alpha\leq 0.99$, so that the
average distance between empty slots is $100$ and the average stride of $500$ is
not exceeded in practice. When $\alpha > 0.99$, a simple plain array can be used
without much overhead.

[DROP?] *Comparison.*
Compared to Elias-Fano coding, CacheLineEF stores the low order bits as exactly
a single byte, removing the need for unaligned reads. Further, the select
data structure on the high-order bits is replaced by a few local bit-wise operations.
CacheLineEF is also somewhat similar to the /(Uniform) Partitioned Elias-Fano Index/
of [cite/t:@partitioned-elias-fano], in that both split the data.
The uniform partitioned index also uses fixed part sizes, but encodes them with
variable widths, and adds a second level of EF
to encode the part offsets. Instead, CacheLineEF prefers simplicity and uses
fixed part sizes with a constant width encoding and simply stores the offsets directly.


#+name: cacheline-ef-code
#+caption: Code for constructing and querying CacheLineEF.
#+attr_html: :class inset large
#+begin_src rust
const L: usize = 44; // The number of elements per cache line.

#[repr(C)]
#[repr(align(64))]   // Align the 64byte object to cache lines.
pub struct CacheLineEF {
    high: [u64; 2],  // Encoding of the high bits.
    offset: u32,     // Offset of the first element.
    low: [u8; L],    // Low 8 bits of each element.
}

impl CacheLineEF {
    fn new(vals: &[u64; L]) -> Self {
        let offset = vals[0] >> 8;
        let mut low = [0u8; L];
        for (i, &v) in vals.iter().enumerate() {
            low[i] = (v & 0xff) as u8;
        }
        let mut high = [0u64; 2];
        for (i, &v) in vals.iter().enumerate() {
            let idx = i + ((v >> 8) - offset) as usize;
            high[idx / 64] |= 1 << (idx % 64);
        }
        Self {
            offset: offset as u32,
            high,
            low,
        }
    }

    fn get(&self, idx: usize) -> u64 {
        let p = self.high[0].count_ones() as usize;
        // Select the position of the 1 using the BMI2 PDEP instruction.
        let one_pos = if idx < p {
            self.high[0].select_in_word(idx)
        } else {
            64 + self.high[1].select_in_word(idx - p)
        };

        self.low[idx] as u64
            + 256 * self.reduced_offset as u64
            + 256 * (one_pos - idx) as u64
    }
}
#+end_src

** Bucket Assignment Functions
:PROPERTIES:
:CUSTOM_ID: bucket-fn
:END:

#+name: bucket-fn
#+caption: The left shows various bucket assignment functions $\gamma$, such as the piecewise linear function (skewed) used by FCH and PTHash, and the optimal function introduced by PHOBIC. Flatter slopes at $x=0$ create larger buckets, while steeper slopes at $x=1$ create more small buckets, as shown on the right, as the distribution of expected bucket sizes given by $(\gamma^{-1})'$ when the expected bucket size is $\lambda=4$.
| [[file:plots/bucket-fn.svg]] | [[file:plots/bucket-size.svg]] |

During construction, slots slowly fill up as more buckets are
placed. Because of this, the first buckets are much easier to place than the
later ones, when only few empty slots are left.
To compensate for this, we can introduce an uneven distribution of bucket
sizes, so that the first buckets are much larger and the last buckets
are smaller.
FCH [cite:@fch] accomplishes this by a /skew/ mapping that assigns $60\%$ of the
elements to $30\%$ of the
buckets, so that those $30\%$ are /large/ buckets while the remaining $70\%$
is /small/ ([[bucket-fn]]). This is also the scheme used by PTHash.

*The perfect bucket function.*
PHOBIC [cite:@phobic] provides a more thorough analysis and uses the optimal[fn::Under the
assumption that bucket sizes are continuous, and that the target load factor is
$\alpha=1$.] function
$\gamma_p(x) = x + (1-x)\ln (1-x)$. This function has derivative $0$ at $x=0$, so
that many $x$ values map close to $0$.
In practice, this causes the largest buckets to have size much larger than $\sqrt S$.
Such buckets are hard to place, because by the birthday paradox they are likely
to have multiple elements hashing to the same slot. To fix this, PHOBIC ensures the
slope of $\gamma$ is at least $\varepsilon=1/(5 \sqrt S)$ by using
$\gamma_\varepsilon(x) = x + (1-\varepsilon)(1-x)\ln(1-x)$ instead.
We fix $\varepsilon = 1/256$.
# Since this function is slow to compute in practice, a
# $2048$-piecewise linear approximation is used instead, using a lookup table and linear interpolation.

*Approximations.*
For PtrHash, we would like to only use simple computations and avoid lookups as
much as possible, to avoid the CPU becoming a bottleneck in query throughput.
To this end, we replace the $\ln (1-x)$ by its
first order Taylor approximation at $x=0$, $\ln(1-x) \approx -x$, giving
the quadratic $\gamma_2(x) = x^2$. Using the second order approximation $\ln(1-x) \approx
-x-x^2/2$ results in the cubic $\gamma(x) = (x^2+x^3)/2$. This version again
suffers from too large buckets, so in practice we use $\gamma_3(x) =
\frac{255}{256}\cdot (x^2+x^3)/2 + \frac{1}{256}\cdot x$.

These values can all be computed efficiently by using that the input and output
of $\gamma$ are $64$ bit unsigned integers representing a fraction of $2^{64}$,
so that e.g. $x^2$ can be computed as the upper $64$ bits of the widening $64\times64\to 128$ bit
product $x\cdot x$.

# [TODO: $\alpha$-adjusted perfect function.]


** Parallel Queries
*Throughput.*
In practice in bioinformatics applications such as SSHash, we expect many
independent queries to the MPHF. This means that queries can be answered in
parallel, instead of one by one. Thus, we should optimize for query /throughput/
(queries per second, but usually implicitly reported as /inverse throughput/ in amortized
seconds per query) rather than individual query latency (seconds per query).

*Out-of-order execution.*
An MPHF on $10^9$ keys requires memory at least $1.5\mathrm{bits}/\mathrm{key} \cdot 10^9
\mathrm{keys} = 188MB$, which is much larger than the L3 cache of size around
$16MB$. Thus, most queries require reading a pilot from main memory (RAM), which usually
has a latency around $80ns$.
Nevertheless, existing MPHFs such as FCH [cite:@fch] achieve an inverse throughput as
low as 35ns/query on such a dataset [cite:@pthash].
This is achieved by /pipelining/ and the /reorder buffer/.
For example, Intel Skylake CPUs can execute over 200 instructions ahead while waiting for memory
to become available [cite:@measuring-rob;@measuring-rob-skylake]. This allows the CPU to already start processing 'future'
queries and fetch the required cache lines from RAM while waiting for the
current query. Thus, when each iteration requires less than 100 instructions
and there are no branch-misses, this effectively makes up to two reads in
parallel. A large part of speeding up queries is then to reduce the length of
each iteration so that out-of-order execution can fetch memory more iterations ahead.

*Prefetching.*
Instead of relying on the CPU hardware to parallellize requests to memory, we can also
explicitly /prefetch/[fn::There are typically multiple types of prefetching
instructions that prefetch into a different level of the cache hierarchy. We
prefetch into all levels of cache using =prefetcht0=.] cache lines from our code.
Each prefetch requires a /line fill buffer/ to store the result before it is
copied into the L1 cache. Skylake has 12 line fill buffers
[cite:@line-fill-buffer-skylake], and hence can support up to 12 parallel
reads from memory.
In theory, this gives a maximal random memory throughput around $80/12 = 6.67$ns per read
from memory, but in practice experiments show that the limit is 7.4ns per read.
Thus, our goal is to achieve a query throughput of 7.4ns.

We consider two models to implement prefetching: batching and streaming.

#+name: streaming
#+caption: [DROP?] Simplified schematic of in-progress reads from main memory (RAM) when using two different prefetching approaches processing (up to) $8$ reads in parallel. Each horizontal line indicates the duration a read is in progress, from the moment it is prefetched (left vertical bar) to the moment it is available in L1 cache and its corresponding line fill buffer is free again (right vertical bar). Streaming (right) provides better parallelism than batching (left).
#+attr_html: :class inset
[[file:./streaming.drawio.svg]]

*Batching.*
In this approach, the queries are split into batches (chunks) of size
$B$, and are then processed one batch at a time ([[streaming]], left).
In each batch, two passes are made over all keys.
In the first pass, each key is hashed, its
bucket it determined, and the cache line containing the corresponding pilot is prefetched.
In the second pass, the hashes are iterated again, and the corresponding slots are
computed.

*Streaming.*
A drawback of batching is that at the start and end of each batch, the
memory bandwidth is not fully saturated.
Streaming fixes this by prefetching the cache line for the pilot $B$ iterations
ahead of the current one, and is able to sustain the maximum possible number of
parallel prefetches throughout, apart from at the very start and end ([[streaming]], right).


** Sharding

When the number of keys is large, say over $10^{10}$, their 64-bit (or 128-bit) hashes may not all fit
in memory at the same time, even though the final PtrHash datastructure (the
list of pilots) would fit. Thus, we can not simply sort all hashes in
memory to partition them. Instead, we split the set of all $n$ hashes into, say
$s=\lceil n/2^{32}\rceil$ /shards/ of $\approx 2^{32}$ elements each,
where the $i$'th shard corresponds to hash values in $s_i:=[2^{64}\cdot i/s,
2^{64}\cdot (i+1)/s)$.
Then, shards are processed one at a time. The hashes in each shard are
sorted and split into parts, after which the parts are constructed as usual.
This way, the shards only play a role during construction, and the final
constructed data structure is independent of which sharding strategy was used.

*In-memory sharding.*
The first approach to sharding is to iterate over the set of keys $s$ times.
In the $i$'th iteration, all keys are hashed, and only those hashes in the
corresponding interval $s_i$ are stored and processed.
This way, no disk space is needed for construction.

*On-disk sharding.*
A drawback of the first approach is that keys are potentially hashed many times.
This can be avoided by writing hashes to disk. Specifically, we can create one
file per shard and append hashes to their corresponding file.
These files are then read and processed one by one.

*Hybrid sharding.* A hybrid of the two approaches above only requires disk space
for $D<s$ shards. This iterates and hashes the keys $\lceil s/D\rceil$ times,
and in each iteration writes hashes for $D$ shards to disk. Those are then
processed one by one as before.

*On-disk PtrHash.*
When the number of keys is so large that even the pilots do not fit in memory, they
can also be stored to disk and read on-demand while querying. This is supported using $\varepsilon$-serde [cite:@epserde;@webgraph].


* Results
We now evaluate PtrHash construction and query throughput for
different parameters, and compare PtrHash to other minimal perfect hash functions.
All experiments are run on an Intel Core i7-10750H CPU with 6 cores and
hyper-threading disabled.
The frequency is pinned to 2.6GHz.
Cache sizes are 32KiB L1 and 256KiB L2 per core, and 12MiB shared L3 cache. Main
memory is 64GiB DDR4 at 3200MHz, split over two 32GiB banks.

*Input data.*
For construction, all experiments use $10^9$ keys, for which the pilots take
around 300MB and are much larger than L3 cache. Unless otherwise mentioned,
construction is in parallel using 6 cores.
For the query throughput experiments, we also test on
20 million keys, for which the pilots take around
6MB and easily fit in L3 cache.
To avoid the time needed for hashing keys, and since our motivating application
is indexing $k$-mers that fit in $64$ bits, we always use random $64$ bit integer keys, and hash them using FxHash.

** Construction
:PROPERTIES:
:CUSTOM_ID: construction-eval
:END:
*** Bucket Functions

# [TODO: Change 4.0 to 3.9 for more reliability.]

#+name: bucket-fn-plot
#+caption: Bucket size distribution (red) and average number of evictions (black) per additionally placed bucket during construction of the pilot table, for different bucket assignment functions. Parameters are $n=10^9$ keys, $S=2^{18}$ slots per part, and $\alpha=0.98$, and the red shaded load factor ranges from $0$ to $\alpha$. In the first five plots $\lambda=3.5$ so that the pilots take $2.29$ bits/key. For $\lambda=4.0$ (rightmost plot), the linear, skewed, and optimal bucket assignment functions cause endless evictions, and construction fails. The cubic function does work, resulting in $2.0$ bits/key for the pilots.
#+attr_html: :class full-width
| [[file:plots/bucket_fn_stats_l35.svg]] | [[file:plots/bucket_fn_stats_l40.svg]] |


In [[bucket-fn-plot]], we compare the performance of different bucket assignment
functions $\gamma$ in terms of the bucket size distribution and the number of
evictions for each additionally placed bucket.
We see that the linear $\gamma_1(x) = x$ has a lot of evictions for the last
buckets of size $3$ and $2$, but like all methods it is fast for the last
buckets of size $1$ due to the load factor $\alpha < 1$. The optimal
distribution of PHOBIC performs only slightly better than the skewed one of FCH and
PTHash, and can be seen to create more large buckets since the load factor
increases fast for the first buckets.
The cubic $\gamma_3$ is clearly much better than all other functions, and is
also tested with larger buckets of average size $\lambda = 4$, where all other
functions fail.

In the remainder, we will test the linear $\gamma_1$ for simplicity and lookup
speed, and the cubic $\gamma_3$ for space efficiency.

*** Tuning Parameters for Construction

#+name: construction
#+caption: This plot shows the construction time (blue and red, left axis) and datastructure size (black, green, and yellow, right axis) as a function of $\lambda$ for $n=10^9$ keys. Parallel construction time on 6 threads is shown for both the linear and cubic $\gamma$, and for various values of $\alpha$ (thickness). The curves stop because construction times out when $\lambda$ is too large. For each $\lambda$, the black line shows the space taken by the array of pilots. For larger $\lambda$ there are fewer buckets, and hence the pilots take less space. The total size including the remap table is shown in green (plain vector) and yellow (CacheLineEF) for various $\alpha$. The blue and red dots highlight the chosen /simple/ and /compact/ parameter configurations.
#+attr_html: :class inset
[[file:plots/size.svg]]

In [[construction]] we compare the multi-threaded construction time and space usage of PtrHash on
$n=10^9$ keys for
various parameter $\gamma\in \{\gamma_1, \gamma_3\}$, $2.7\leq \lambda\leq 4.2$,
$\alpha\in \{0.98, 0.99, 0.995, 0.998\}$, and plain remapping or CacheLineEF.
We see that for fixed $\gamma$ and $\alpha$, the construction time appears to
increase exponentially as $\lambda$ increases, until it times out due to a
never-ending chain of evictions.
Load factors $\alpha$ closer to $1$ (thinner lines) achieve smaller overall data
structure size, but take longer to construct and time out at smaller $\lambda$.
The cubic $\gamma_3$ is faster to construct than the identity $\gamma_1$ for
small $\lambda \leq 3.5$. Unlike $\gamma_1$, it also scales to much larger
$\lambda$ up to $4$, and thereby achieves significantly smaller overall size.

We note that for small $\lambda$, construction time does converge to around 19ns/key.
A rough time breakdown is that for each key, 1ns is spent on hashing, 5ns
on sorting all the keys, 12ns to find pilots, and lastly 1ns on remapping
to empty slots.

*Recommended parameters.*
Based on these results, we choose two sets of parameters for further
evaluation, as indicated with blue and red dots in [[construction]]:
- *Fast*: using the linear $\gamma_1$, $\lambda=3.0$, $\alpha=0.99$, and a plain
  vector for remapping.
  Construction takes only just over 20ns/key, close to the apparent lower
  bound, and space usage is 3.00bits/key. This can be used when $n$ is small, or
  more generally when memory usage is not a bottleneck.
- *Compact*: using the cubic $\gamma_3$, $\lambda=4.0$, $\alpha=0.99$, and
  CacheLineEF remapping. Construction now takes around 50ns/key, but the data
  structure only uses 2.12bits/key.

*** [Appendix?] Remap
#+name: remap
#+caption: Comparison of space usage and query throughput when using the recommended parameters with different remap structures. Query throughput is shown both for perfect hashing without remap, and for minimal perfect hashing with remap. Additionally, query throughput is shown both for a for-loop and for streaming with prefetching 32 iterations ahead.
| Parameters                                              |     Pilots | Query |    PHF | Remap       |      Remap | Query |   MPHF |
|                                                         | (bits/key) |  Loop | Stream |             | (bits/key) |  Loop | Stream |
|---------------------------------------------------------+------------+-------+--------+-------------+------------+-------+--------|
| Fast: $\alpha=0.99$, $\lambda=3.0$, linear $\gamma_1$   |       2.67 |  11.5 |    8.6 | Vec<u32>    |       0.33 |  12.5 |    8.8 |
|                                                         |            |       |        | CacheLineEF |       0.12 |  12.9 |    8.8 |
|                                                         |            |       |        | EF          |       0.09 |  14.2 |    9.7 |
| Compact: $\alpha=0.99$, $\lambda=4.0$, cubic $\gamma_3$ |       2.00 |  17.7 |    8.0 | Vec<u32>    |       0.33 |  20.3 |    8.6 |
|                                                         |            |       |        | CacheLineEF |       0.12 |  20.9 |    8.6 |
|                                                         |            |       |        | EF          |       0.09 |  21.7 |    9.7 |

In [[remap]], we compare the space usage and query throughput of the different remap
data structures for both the fast and compact parameters, for $n=10^9$ keys. We observe that
the overhead of CacheLineEF is $2.75\times$ smaller than a plain vector, and only $40\%$ larger
than true Elias-Fano encoding.

The speed of non-minimal (PHF) queries that do not remap does not depend
on the remap structure used.

For /minimal/ (MPHF) queries with the for loop, EF is significantly slower
(14.2ns) with the fast parameters than the plain vector (12.5ns), while
CacheLineEF (12.9ns) is only slightly slower.
The difference is much smaller with the compact parameters, because the
additional computations for the cubic $\gamma_3$ reduce the number of iterations
the processor can work ahead.
When streaming queries, for both parameter choices CacheLineEF is less than 0.1ns slower than the
plain vector, while EF is 1ns slower.

In the end, we choose CacheLineEF when using compact parameters, but prefer the
simpler and slightly faster plain vector for fast parameters.

*** Sharding
We tested the in-memory and hybrid sharding by constructing PtrHash on $5\cdot
10^{10}$ random integer keys on a laptop with only 64GB of memory, using 6 cores
in parallel.
All 64-bit hashes would take 400GB, so we use 24 shards of
around $2^{31}$ keys, that each take 16GB.  We use the compact parameters, but
with $\lambda=3.9$, as $\lambda=4.0$ turns out to be slightly too close to the limit for
reliable construction when the number of parts is large.
The final data structure takes 2.17 bits/key, or 13.6GB in total, and the
peak memory usage is around 50GB.

The in-memory strategy iterates through and hashes the integer keys 24 times, and takes
3996 seconds in total or 166s per shard. Of this, 65s (39%) is spent on hashing
the keys, 15s (9%) is spent sorting hashes into buckets, and 82s (49%) is spent
searching for pilots.

The hybrid strategy is allowed to use up to 128GB of disk space, and thus writes
hashes to disk in 3 batches of 8 shards at a time. This brings the total time
down to 3314s (17% faster), and uses 138s per shard. Of this, 24s is spent
writing hashes to disk, and 21s is spent reading hashes from disk, which
together is faster than the 65s that was previously spent on hashing all keys.

** Query Throughput

To our knowledge, all recent papers on (minimal) perfect hashing measure query
speed by first creating a list of keys, and then querying all keys in the list,
as in =for key in keys { ptr_hash.query(key); }=. One might think this measures the average
latency of a query, but that is not the case, as the CPU will execute
instructions from adjacent iterations at the same time.
Indeed, as can be seen in [[remap]], this loop can be as fast as $12
ns/key$ for $n=10^9$, which is over $6$ times faster than the RAM latency of
$\approx 80ns$ (for an input of size 300MB),
and thus, at least $6$ iterations are being processed in parallel.

Hence, we argue that existing benchmarks measure (and optimize for)
throughput and that they assume that the list of keys to query is known in advance.
We make this assumption explicit by changing the API to benchmark all queries at
once, as in =ptr_hash.query_all(keys)=. This way, we can explicitly process
multiple queries in parallel as described in [[*Parallel queries]].

We also argue that properly optimizing for throughput is relevant for
applications. SSHash, for example, queries all minimizers of a DNA sequence,
which can be done by first computing and storing those minimizers, followed by
querying them all at once.

We now explore the effect of the batch size and number of parallel threads on
query throughput.

*** [Appendix?] Batching and Streaming
#+name: batching
#+caption: Query throughput of prefetching via batching (dotted) and streaming (dashed) with various batch/lookahead sizes, compared to a plain for loop (solid), for $n=20\cdot 10^6$ (left) and $n=10^9$ (right) keys. Blue shows the results for the fast parameters, and red for the compact parameters. All times are measured over a total of $10^9$ queries, and for (non-minimal) perfect hashing only, /without/ remapping.
#+attr_html: :class inset
[[file:plots/query_batching.svg]]

In [[batching]], we compare the query throughput of a simple for loop with the
batching and streaming variants with various batch/lookahead sizes. We see that
both for small $n=20\cdot 10^6$ and large $n=10^9$, the simple parameters yield
higher throughput than the compact parameters when using a for loop. This is
because of the overhead of computing $\gamma_3(x)$. For small $n$, batching and
streaming do not provide much benefit, indicating that memory latency is not a
bottleneck. However, for large $n$, both batching and streaming improve over the
plain for loop. As expected, streaming is faster than batching here. For
streaming, throughput saturates when prefetching around 16 iterations ahead. At
this point, memory throughput is the bottleneck, and the difference between the
compact and simple parameters disappears. In fact, compact parameters with
$\gamma_3$ are slightly /faster/. This is because $\gamma_3$ has a more skew
distribution of bucket sizes with more large buckets. When the pilots for these
large buckets are cached, they are more likely to be hit by subsequent queries,
and hence avoid some accesses to main memory.

For further experiments we choose streaming over batching, and use a lookahead
of 32 iterations.
The final throughput of 8ns per query is very close to the optimal throughput of
7.4ns per random memory read.

*** [Appendix?] Multi-threaded Throughput
#+name: throughput
#+caption: In this plot we compare the throughput of a for loop (solid) versus streaming (dashed) for multiple threads, for both non-minimal (dimmed) and minimal (bright) perfect hashing. The left shows results for $n=20\cdot 10^6$, and the right shows results for $n=10^9$. On the right, the solid black line shows the maximum throughput based on 7.4ns per random memory access per thread, and the solid black line shows the maximum throughput based on the total memory bandwidth of 25.6GB/s.
#+attr_html: :class inset
[[file:plots/query_throughput.svg]]

In [[throughput]] we compare the throughput of the fast and compact parameters for
multiple threads. When $n=20\cdot 10^6$ is small and the entire datastructure
fits in L3 cache, the scaling to multiple threads is nearly perfect. As
expected, minimal perfect hashing (bright) tends to be slightly slower than
perfect hashing (dimmed), but the difference is small. The fast $\gamma_1$ is faster than
the compact $\gamma_3$, and streaming provides only a small benefit over a for
loop.
For large $n=10^9$, all methods converge towards the limit imposed by the full
RAM throughput of 25.6GB/s. Streaming variants hit this starting at around 4
threads, and remain faster than the for loop. As before, the compact version is
slightly faster because of its more efficient use of the caches, and is even
slightly better than the maximum throughput of random reads to RAM.
Minimal perfect hashing is only slightly slower than perfect hashing.

** Comparison to Other Methods

#+name: comparison
#+caption: Performance comparison of PtrHash against other methods, on 300 million random string keys of uniform length between 10 and 50. Construction time is shown when using 6 threads. Times marked with a * are single threaded and show the optimistic 6-fold faster time in parentheses. Near-optimal values in each column are shown in bold. The common parameters $\lambda$ (number of elements per bucket) and $\alpha$ (initial load factor) are shown separately.
#+attr_html: :class small
| Approach            | Configuration                               | Space@@html:<br/>@@bits/key | Construction@@html:<br/>@@ 6t, ns/key | Query @@html:<br/>@@ns/query |
| Brute-force         |                                             |                             |                                       |                              |
| SIMDRecSplit        | $n{=}5$, $b{=}5$                            |                        2.96 |                                  *26* |                          310 |
| SIMDRecSplit        | $n{=}8$, $b{=}100$                          |                      *1.81* |                                    66 |                          258 |
| Bip. ShockHash-Flat | $n{=}64$                                    |                      *1.62* |                           2140* (357) |                          201 |
|                     |                                             |                             |                                       |                              |
| Fingerprinting      |                                             |                             |                                       |                              |
| FMPH                | $\gamma{=}2.0$                              |                        3.40 |                                    44 |                          168 |
| FMPH                | $\gamma{=}1.0$                              |                        2.80 |                                    69 |                          236 |
| FMPHGO              | $s{=}4$, $b{=}16$, $\gamma{=}2.0$           |                        2.86 |                                   298 |                          160 |
| FMPHGO              | $s{=}4$, $b{=}16$, $\gamma{=}1.0$           |                        2.21 |                                   423 |                          212 |
| FiPS                | $\gamma{=}2.0$                              |                        3.52 |                            93* (*16*) |                          109 |
| FiPS                | $\gamma{=}1.5$                              |                        3.12 |                           109* (*18*) |                          124 |
|                     |                                             |                             |                                       |                              |
| Graphs              |                                             |                             |                                       |                              |
| SicHash             | $p_1{=}0.21$, $p_2{=}0.78$, $\alpha{=}0.90$ |                        2.41 |                                    48 |                          149 |
| SicHash             | $p_1{=}0.45$, $p_2{=}0.31$, $\alpha{=}0.97$ |                        2.08 |                                    63 |                          141 |
|                     |                                             |                             |                                       |                              |
| Bucket placement    |                                             |                             |                                       |                              |
| CHD                 | $\lambda{=}3.0$                             |                        2.27 |                           1059* (177) |                          542 |
| PTHash              | $\lambda{=}4.0$, $\alpha{=}0.99$, C-C       |                        3.19 |                                   403 |                           77 |
| + HEM               |                                             |                             |                                   173 |                              |
| PTHash              | $\lambda{=}5.0$, $\alpha{=}0.99$, EF        |                        2.17 |                                   765 |                          156 |
| + HEM               |                                             |                             |                                   323 |                              |
| PHOBIC              | $\lambda{=}3.9$, $\alpha{=}1.0$, IC-C       |                        4.14 |                                    62 |                          116 |
| PHOBIC              | $\lambda{=}4.5$, $\alpha{=}1.0$, IC-R       |                        2.34 |                                    80 |                          179 |
| PHOBIC              | $\lambda{=}6.5$, $\alpha{=}1.0$, IC-R       |                      *1.94* |                                   215 |                          163 |
| Phobic              | $\lambda{=}6.5$, $\alpha{=}1.0$, IC-C       |                        2.44 |                                   220 |                          108 |
| PHOBIC              | $\lambda{=}7.0$, $\alpha{=}1.0$, IC-R       |                      *1.86* |                                   446 |                          157 |
| PtrHash, fast       | $\lambda{=}3.0$, $\alpha{=}0.99$, Vec       |                        2.99 |                                  *26* |                         *38* |
| + streaming         |                                             |                             |                                       |                         *20* |
| PtrHash, compact    | $\lambda{=}4.0$, $\alpha{=}0.99$, CLEF      |                        2.12 |                                    62 |                         *42* |
| + streaming         |                                             |                             |                                       |                         *23* |

In [[comparison]] we compare the performance of PtrHash against other methods on
short, random strings.
In particular, we compare against methods that are reasonably fast to construct:
CHD [cite:@chd], FMPH and FMPHGO [cite:@fmph], FiPS [cite:@phf-thesis], SIMDRecSplit
[cite:@recsplit;@recsplit-gpu], SicHash [cite:@sichash], PTHash
[cite:@pthash;@pthash-2], and PHOBIC [cite:@phobic].
We also include Bipartite ShockHash-Flat [cite:@shockhash;@bipartite-shockhash],
which is able to use relatively little space with fast construction time.
The specific parameters are based on Table 1 of [cite:@phobic], Table 8.1 of
[cite:@phf-thesis], and Table 3 of [cite:@fmph], with some configurations that are slow to construct omitted.
These results were obtained using the excellent MPHF-Experiments library
[cite:@mphf-experiments] by Hans-Peter Lehmann. Construction is done on 6
threads in parallel. By default, the framework queries
one key at a time. For PtrHash with streaming queries, we modified this to query
all keys at once.

*Input.*
The input is 300 million random strings of random length between 10 and 50
characters. This input size is such that the MPHF data structures take around
75MB, which is much larger than the 12MB L3 cache.

*PtrHash.* As expected, the space usage of PtrHash matches the numbers of [[remap]].
In general, PtrHash can be slightly larger due to rounding in the number of
parts and slots per part, but for large inputs like here this effect is small.
Construction times per key are slightly slower than as predicted by
[[construction]], while we might expect slightly faster construction due to the
lower number of keys. Likely, the slowdown is caused by hashing the input strings.
The hashing of input strings has a much worse effect on query throughput. In
[[batching]], we obtained query throughput of 12ns and 18ns for the fast and compact
configurations when looping, and as low as 8ns when streaming queries. With
string inputs, these numbers more than double to 38ns resp. 42ns when looping,
and 20ns when streaming. A similar effect can be seen when comparing Tables 3
and 4 of [cite:@fmph]. [[#hashing]] further investigates this.

*Speed.*
We observe that PtrHash with fast parameters is the fastest to construct
alongside SIMDRecSplit (26 ns/key) and FiPS (16ns/key, assuming optimal scaling to
6 threads),  resulting in around 3bits/key for all three methods.
However, query throughput of PtrHash is $8\times$ (SIMDRecSplit) resp.
$2.8\times$ (FiPS) faster, going up to $15\times$ resp.
$5\times$ faster when streaming all queries at once.
Compared to the next-fastest method to query, PTHash-CC (HEM), PtrHash is twice
faster to query (or nearly $4\times$ when streaming), is $6.5\times$ faster to build, and
even slightly smaller.

*Space.*
PtrHash with the fast parameters is larger (2.99 bits/key) than some other methods, but
compensates by being significantly faster to construct and/or query.
When space is of importance, the compact version can be used (2.12 bits/key).
This takes $2.4\times$ longer to build at 62ns/key, and has only slightly slower queries.
Compared to methods that are smaller,
PtrHash is over $3\times$ faster to build than PHOBIC.
SIMDRecSplit and SicHash achieve smaller space of 1.81 and 2.08 bits/key in
comparable time (63ns to 66ns), but again are over $3\times$ slower to query, or
over $6\times$ compared to streaming queries. Bipartite ShockHash-Flat is even
smaller at 1.62bits/key, but also over $5\times$ slower to build and query.

* Conclusions and Future Work
We have introduced PtrHash, a minimal perfect hash function that builds on
PTHash and PHOBIC. Its main novelty is the used of fixed-width 8-bit pilots that
simplify queries. To make this possible, we use /hash-and-evict/, similar to
Cuckoo hashing: when there is no pilot that leads to a collision-free placement
of the corresponding keys, some other pilots are /evicted/ and have to search
for a new value.

The result is an MPHF with twice faster queries (38ns/key) than any other method
(at least 77ns/key) for datasets larger than L3 cache. Further,
due to its simplicity, queries can be processed in /streaming/ fashion, giving
another two times speedup (20ns/key). At this point, the hashing of string inputs becomes a
bottleneck. For integer keys, such as $k$-mers, much higher throughput of up to
8ns/key can be obtained, fully saturating the RAM throughput of each core or
even of the main memory (2.5ns/key) when using multiple cores.

*Future work.*
We see a few directions for future work.

First of all, a theoretical analysis of our method is currently missing. While
the hash-evict strategy generalizing ($d$-ary) cuckoo hashing works well in
practice, we currently have no relation between the bucket size $\lambda$, load
factor $\alpha$, and the number of evicts arising during construction.
Such an analysis could help to better understand the optimal bucket assignment
function, like PHOBIC [cite:@phobic] did for the case without
eviction.

Second, the size of pilots could possibly be improved by further parameter
tuning. In particular we use 8-bit pilots, while slightly fewer or more
bits may lead to smaller data structures. An experiment with 4-bit pilots
was not promising, however.

Lastly, to further improve the throughput, we suggest that more attention is
given to the exact input format. As already seen, hashing all queries at once
can provide significant performance gains via prefetching.  For string input
specifically, it is more efficient when the strings are consecutively packed in memory
rather than separately allocated, and it might be more efficient to explicitly
hash multiple strings in parallel.
To end, applications could investigate whether they can be rewritten to take
advantage of stream queries.

* Acknowledgements
:PROPERTIES:
:UNNUMBERED:
:END:
First, I thank Giulio Ermanno Pibiri for his ongoing feedback in
various stages of this project. Further, I thank Sebastiano Vigna for feedback
from trying to construct PtrHash on $10^{12}$ keys and integrating
$\varepsilon$-serde, and lastly I thank Hans-Peter Lehmann for feedback on an
early version of this paper.

* Funding
:PROPERTIES:
:UNNUMBERED:
:END:
This work was supported by ETH Research Grant ETH-1721-1 to Gunnar Rätsch.

#+LaTeX: \appendix
* Appendix
** Rust and Assembly Code for Streaming
[[streaming-code]] shows the Rust code for the streaming version of PtrHash, and
[[streaming-asm]] shows the corresponding assembly code with =perf record= results.

#+name: streaming-code
#+caption: Rust code for streaming indexing that prefetches $B$ iterations ahead.
#+begin_src rust
pub fn index_stream<'a, const B: usize, const MINIMAL: bool>(
    &'a self,
    keys: impl IntoIterator<Item = &'a Key> + 'a,
) -> impl Iterator<Item = usize> + 'a {
    // Append B values at the end of the iterator to make sure we wrap sufficiently.
    let mut hashes = keys.into_iter().map(|x| self.hash_key(x)).chain([0; B]);

    // Ring buffers to cache the hash and bucket of upcoming queries.
    let mut next_hashes: [Hx::H; B] = [Hx::H::default(); B];
    let mut next_buckets: [usize; B] = [0; B];

    // Initialize and prefetch first B values.
    for idx in 0..B {
        next_hashes[idx] = hashes.next().unwrap();
        next_buckets[idx] = self.bucket(next_hashes[idx]);
        crate::util::prefetch_index(self.pilots, next_buckets[idx]);
    }
    hashes.enumerate().map(move |(idx, next_hash)| {
        let idx = idx % B;
        let cur_hash = next_hashes[idx];
        let cur_bucket = next_buckets[idx];
        let pilot = self.pilots[cur_bucket];
        let mut slot = self.slot(cur_hash, pilot);
        if MINIMAL && slot >= self.n {
            slot = self.remap.index(slot - self.n) as usize;
        };

        // Prefetch B iterations ahead.
        next_hashes[idx] = next_hash;
        next_buckets[idx] = self.bucket(next_hashes[idx]);
        crate::util::prefetch_index(self.pilots, next_buckets[idx]);

        slot
    })
}
#+end_src

#+name: streaming-asm
#+caption: Assembly code of streaming indexing (without the final =remap=) that prefetches 32 iterations ahead, with =perf record= measurement of time time spent on each line. TODO: Update for latest version.
#+begin_src asm
  2.57 │ a0:   lea        (%r14,%rbp,1),%r12d
  0.95 │       mov        0x8(%rsp),%rdx
 16.93 │       mov        (%rdx,%r14,8),%rdx
  0.80 │       imul       %r11,%rdx
  2.30 │       and        $0x1f,%r12d
  0.90 │       mov        0x8(%rcx,%r12,8),%rsi
  1.36 │       mulx       %rbx,%r8,%r9
  2.24 │       mov        0x108(%rcx,%r12,8),%r10
  0.92 │       mov        %rdx,0x8(%rcx,%r12,8)
  0.48 │       mov        %r8,%rdx
  2.99 │       mulx       %r8,%rdx,%rdx
  1.03 │       mov        0x20(%rsp),%r8
  1.44 │       mulx       %r8,%rdx,%rdx
  2.15 │       imul       0x18(%rsp),%r9
  1.08 │       add        %rdx,%r9
  0.83 │       mov        %r9,0x108(%rcx,%r12,8)
 46.61 │       prefetcht0 (%r15,%r9,1)            ; Nearly half the time is spent here.
  1.39 │       movzbl     (%r15,%r10,1),%r8d
  0.54 │       imul       %r11,%r8
  0.31 │       xor        %rsi,%r8
  2.34 │       mov        %rsi,%rdx
  1.43 │       mulx       %rbx,%rdx,%rdx
  0.30 │       shlx       %r13,%rdx,%rdx
  2.43 │       add        %rdx,%rax
  0.87 │       mov        %r8,%rdx
  0.72 │       mulx       %r11,%rdx,%rdx
  2.37 │       and        %rdi,%rdx
  0.98 │       add        %rdx,%rax
  0.51 │       inc        %r14
       │       cmp        %r14,0x28(%rsp)
  0.23 │     ↑ jne        a0
#+end_src



* DONE Failed Ideas
- always compute remap to avoid branch:
  - Instead, an additional layer of prefetching helps a bit, but too complicated
    and annoying.
- rattle kicking?
- 4bit pilots with buckets of half the size -> doesn't work.

* Appendix
** TODO Choosing the Part Size

** Input Types and Hash Functions
:PROPERTIES:
:CUSTOM_ID: hashing
:END:

#+name: hashes
#+caption: MPHF query throughput of PtrHash with fast parameters for $n=10^8$ keys. (Not $10^9$ because this takes too much memory.)  Hashing plain ints with FxHash is fastest, followed by hashing Boxed ints. XxHash is much slower, already when hashing ints. In fact, xxHash is faster at hashing strings than integers! The string length does not have a very big impact, but variable-length strings are consistently slightly slower.
| input     | loop |      |       | stream |      |       |
|           |   fx | xx64 | xx128 |     fx | xx64 | xx128 |
| u64       | 11.1 | 24.4 |  29.9 |    7.2 |  9.1 |  10.5 |
| Box<u64>  | 12.7 | 30.1 |  31.2 |    8.7 | 11.1 |  12.4 |
| &[u8; 10] | 19.4 | 27.7 |  32.9 |   10.1 | 12.5 |  14.2 |
| &[u8; 50] | 34.1 | 28.6 |  32.8 |   16.5 | 12.7 |  14.1 |
| &[u8]     | 39.2 | 37.0 |  50.9 |   27.2 | 17.8 |  23.1 |
| Vec<u8>   | 40.2 | 40.6 |  52.7 |   28.3 | 20.2 |  25.3 |

Setup:
- TODO cite fx and xx.
- xx64 calls =xxhash_rust::xxh3::xxh3_64_with_seed=
- xx128 calls =xxhash_rust::xxh3::xxh3_128_with_seed=
- String slices are all packed into a single large vector, so they are
  effectively streaming and at predictable locations in memory.
- Box<u64> and Vec<u8> are allocated in query order, so are likely somewhat
  ordered in memory as well.

Results:
- xx64 always faster than xx128
- Fx faster than xx64 for ints and short fixed-length strings.
- xx64 wins for long and arbitrary-length strings, especially when streaming.
- Packed string input is faster than separately allocated string input.
- Looking at streaming xx64:
  - 7ns lookup (fx)
  - +2ns int hashing overhead (xx)
  - +3.5ns for hashing fixed-length strings. Not much dependent on length 10 or 50.
  - +5ns for variable-length strings, for the branch-miss.
  - +3ns for indirection of arbitrary allocations.

* Graveyard
#+name: query-throughput-1
#+caption: The typical code used to benchmark (minimal) perfect hash functions takes a list of keys, and measures the time it takes to =query= them one by one. This implicitly processes multiple queries in parallel. =black_box= is a Rust standard library function that ensures the query is not optimized away.
#+begin_src rust
fn benchmark(&self, keys: &Vec<Key>) -> Duration {
    let start = Instant::now()
    for key in keys {
        black_box(self.query(key));
    }
    start.elapsed()
}
#+end_src

#+name: query-throughput-2
#+caption: To allow explicit parallel processing of queries using prefetching, we change the benchmark to a single =query_all= function.
#+begin_src rust
fn benchmark(&self, keys: &Vec<Key>) -> Duration {
    let start = Instant::now()
    black_box(self.query_all(keys));
    start.elapsed()
}
#+end_src


#+print_bibliography:
