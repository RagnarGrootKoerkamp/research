#+title: [WIP] Minimizers
#+filetags: @thesis minimizers wip
#+HUGO_LEVEL_OFFSET: 0
#+OPTIONS: ^:{} num:2 H:4
#+hugo_front_matter_key_replace: author>authors
#+toc: headlines 3
#+hugo_paired_shortcodes: %notice
#+date: <2024-11-05 Tue>

$$
\newcommand{\O}{\mathcal O}
\newcommand{\order}{\mathcal O}
\newcommand{\Ok}{\mathcal O_k}
\newcommand{\Ot}{\mathcal O_t}
\newcommand{\Os}{\mathcal O_s}
\newcommand{\S}{\Sigma}
\newcommand{\Dk}{\mathcal D_k}
\newcommand{\Dtk}{\tilde{\mathcal D}_k}
\newcommand{\P}{\mathbb P}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\poly}{poly}
\DeclareMathOperator*{\rc}{rc}
\DeclareMathOperator*{\sp}{sparsity}
\newcommand{\ceil}[1]{\left\lceil{#1}\right\rceil}
\newcommand{\floor}[1]{\left\lfloor{#1}\right\rfloor}
\newcommand{\c}{\mathrm{c}}
\newcommand{\boldremuval}{\mathbf{ReM}_{\mathbf{u}}\mathbf{val}}
\newcommand{\remuval}{\mathrm{ReM}_{\mathrm{u}}\mathrm{val}}
$$

#+attr_shortcode: summary
#+begin_notice
- New lower bound
- New 'pure' schemes
#+end_notice

#+attr_shortcode: attribution
#+begin_notice
- lower bound [cite:@sampling-lower-bound], with Bryce
- mod-mini [cite:@modmini] with Giulio, especially the background section
- open-closed mod-mini [cite:@oc-modmini-preprint]
- SIMD-minimizers [cite:@simd-minimizers-preprint], for canonical minimizers
- some new/unpublished minimizer schemes

Also, the following blog posts
- bd anchors: [[../bd-anchors/bd-anchors.org]]
- mod-mini: [[../mod-minimizers/mod-minimizers.org]]
- lower bound: [[../minimizer-lower-bound/minimizer-lower-bound.org]]
- simd minimizers: [[../fast-minimizers/fast-minimizers.org]]
- practical minimizers: [[../practical-minimizers/practical-minimizers.org]]
#+end_notice


*Motivation.*
- k-mers
- compression
- faster analysis
- locally consistent
- Jaccard similarity

*Brief definition.*
- w, k, density

*Problem statement.*
- Mention random minimizer
- low density

*Overview.*
We split content into three topics
- General theory
- Lower bounds
- Selection

*Applications.*
- Locality sensitive hashing
- Similarity estimation [cite:@minimizer-biased;@minhash;@minmers]
- DeBruin graph partitioning/glueing [cite:@compacting-dbg]

#+begin_quote
[At RECOMB 2022, discussing DeepMinimizer]

Why would we even care about better minimizer? We have this simple and fast
random minimizer that's only at most $2\times$ away from optimal. Why would
anyone invest time in optimizing this by maybe $25\%$?
There are so much bigger gains possible elsewhere.
#+end_quote

** Introduction

- Lots of DNA data
- Most algorithms deal with k-mers.
- k-mers overlap, and hence considering all of them is redundant.
- Thus: sample a subset of the kmers.
- Must be 'locally consistent' and deterministic to be useful.
- Enter random minimizers.
- Parameter $w$: guarantee that at least one k-mer is sampled out of every
  window of $w$ k-mers.
- Density $d$: (expected) overall fraction of sampled k-mers.
- Obviously, $d\geq 1/w$
- For random mini, $d=2/(w+1)$.
- Lower density => fewer k-mers, smaller indices, faster algorithms.
- *Question:* How small density can we get for given $k$ and $w$?

*** Previous reviews
- [cite/t:@minimizer-sketches]
- [cite/t:@minimizer-review-2]

** Copied:
Minimizers can be used for various purposes, such as:
- Compressing sequences, such as in minimizer-space De Bruijn graphs [cite:@mdbg].
- Similarity estimation,
- /Locality preserving hashing/ of kmers for different types of /clustering/.
  1. [cite:@sshash;@lphash].
     - Here a large $k \geq \log_\sigma N$ is used so that minimizers are unique keys.
     - The objective is to have as few minimizers as possible in any sequence, to
       reduce the number of cache-misses due to non-locality.
  2. Similarity estimation [cite:@minimap].
     - Here the goal is to cluster similar reads. Ideally reads with a few small
       mutations have the same minimizer.

Locality preserving hashing and similarity estimation both cluster kmers on
similarity, but there are some differences:
- In LPH, we want /consecutive/ kmers to share a minimizer. I.e. we want to
  partition the De Bruijn graph into long paths.
- In similarity estimation, we want /similar/ kmers to share a minimizer, where
  /similar/ explicitly includes small mutations. I.e. we want to partition the
  De Bruijn graph into 'blobs' covering many variants.

* Theory of sampling schemes
The theory of minimizer schemes started with two independent papers proposing
roughly the same idea: winnowing [cite:@winnowing] in 2003 and minimizers [cite/t:@minimizers] in 2004.
At the core, the presented ideas are very similar: to deterministically select a k-mer out of each
window of w consecutive k-mers by choosing the ''smallest'' one, according to
either a random of lexicographic order.
The /window guarantee/ is a core property of minimizers: it guarantees that
consecutive minimizers are never too far away from each other.
Further, these
schemes are /local/: whether a k-mer is sampled as a minimizer only depends on a
small surrounding context of $w-1$ characters, and not on any external context.
This enables the use of minimizers for locality sensitive hashing
[cite:@lphash;@sshash], since the minimizer is a deterministic key (hash) that
is often shared between adjacent windows.

While the winnowing paper was published first, the 'minimizer' terminology is the one
that appears to be used most these days. Apart from terminology, also notations
tend to differ between different papers. Here we fix things as follows.

*Notation.*
Throughout this chapter, we use the following notation.
For $n\in \mathbb N$, we write $[n]:=\{0, \dots, n-1\}$.
The alphabet is $\S = [\sigma]$ and has size $\sigma =2^{O(1)}$, so that each character can
be represented with a constant number of bits.
Given a string $S\in \S^*$, we write $S[i..j)$ for the sub-string starting at
the $i$'th character, up to (and not including) the $j$'th character, where both
$i$ and $j$ are $0$-based indices.
A k-mer is any (sub)string of length $k$.

In the context of minimizer schemes, we have a /window guarantee/ $w$ indicating
that at least one every $w$ k-mers must be sampled.
A /window/ is a string containing exactly $w$ k-mers, and hence consists of
$\ell:=w+k-1$ characters.
We will later also use /contexts/, which are sequences containing two windows
and thus of length $w+k$.

** Types of sampling schemes
#+begin_definition Window
Given parameters $w$ and $k$, a /window/ is a string containing exactly $w$
k-mers, i.e., of length $\ell = w+k-1$.
#+end_definition

#+begin_definition Local sampling scheme
For $w\geq 1$ and $k\geq 0$, a /local scheme/ is a function $f: \S^\ell \to [w]$.
Given a window $W$, it /samples/ the k-mer $W[f(W)..f(W)+k)$.
#+end_definition

In practice, we usually require $w\geq 2$ and $k\geq 1$, as some theorems break
down at either $w=1$ or $k=0$.
When $k \geq w$, such a scheme ensures that every single character in the input
is covered by at least one sampled k-mer.

#+begin_definition Forward sampling scheme
A local scheme is /forward/ when for any /context/ $C$ of length $\ell+1$
containing windows $W=C[0..\ell)$ and $W'=C[1..\ell+1)$, it holds that $f(W) \leq f(W')+1$.
#+end_definition

Forward scheme have the property that as the window $W$ slides through an input
string $S$, the position in $S$ of the sampled k-mer never decreases.

#+begin_definition Order
An order $\Ok$ on k-mers is a function $\Ok : \S^k \to \mathbb R$, such
that for $x,y\in \S^k$, $x\leq _{\Ok} y$ if and only if $\Ok(x) \leq \Ok(y)$.
#+end_definition

#+begin_definition Minimizer scheme
A /minimizer scheme/ is defined by a total order $\Ok$ on k-mer and samples the
leftmost minimal k-mer in a window $W$, which is called the /minimizer/:

$$
f(W) := \argmin_{i\in [w]} \Ok(W[i..i+k)).
$$
#+end_definition

Minimizer schemes are always forward, and thus we have the following hierarchy

$$
\textrm{minimizer schemes} \subseteq \textrm{forward schemes} \subseteq
\textrm{local schemes}.
$$

There are two particularly common minimizer schemes, the /lexicographic/
minimizer [cite:@minimizers] and the /random/ minimizer [cite:@winnowing].

#+begin_definition Lexicographic minimizer
The /lexicographic minimizer/ is the minimizer scheme that sorts all k-mers lexicographically.
#+end_definition

#+begin_definition Random minimizer
The /random minimizer/ is the minimizer scheme with a uniform random total
order $\Ok$.
#+end_definition

Following [cite/t:@small-uhs], we also define a /selection/ scheme, as opposed
to a /sampling/ scheme. Note though that this distinction is not usually made in
other literature.

#+begin_definition Selection scheme
A /selection scheme/ is a sampling scheme with $k=1$, and thus samples any
position in a window of length $w+k-1=w$.
Like sampling schemes, selection schemes can be either local or forward.
#+end_definition

We will consistently use /select/ when $k=1$, and /sample/ when $k$ is arbitrary.
When $k=1$, we also call the sampled position an /anchor/, following bd-anchors [cite:@bdanchors].
Note that a /minimizer selection scheme/ is not considered, as sampling the
smallest character can not have density below $1/\sigma$.

#+begin_definition Particular density
Given a string $S$ of length $n$, let $W_i := S[i..i+\ell)$ for $i\in [n-\ell+1]$.
A sampling scheme $f$ then samples the k-mers starting at positions $M:=\{i+f(W_i)
\mid i\in [n-\ell+1]\}$. The /particular density/ of $f$ on $S$ is the fraction
of sampled k-mers: $|M|/(n-k+1)$.
#+end_definition

#+begin_definition Density
The /density/ of a sampling $f$ is defined as the expected particular density on
a string $S$ consisting of i.i.d. random characters of $\Sigma$ in the limit
where $n\to\infty$.
#+end_definition

Since all our schemes must sample at least one k-mer from every $w$ consecutive
positions, they naturally have a lower bound on density of $1/w$.

As we will see, for sufficiently large $k$ the density of the random minimizer is $2/(w+1) + o(1/w)$.
There is also the notion of /density factor/ [cite:@improved-minimizers], which
is defined as $(w+1)\cdot d(f)$. Thus, random minimizers
have a density factor of $2$. While this is convenient, we refrain from using
density factors here, because it would be more natural to relate the density to
the lower bound of $1/w$ instead, and use $w\cdot d(f)$. Specifically, as
defined, the density factor can never reach the natural lower bound of $1$,
because $(w+1)\cdot \frac 1w = 1+1/w > 1$.

Now that we have defined the density, the natural question to ask is:
#+begin_problem Optimal density
What is the lowest density that can be achieved by a minimizer, forward, or
local scheme?
#+end_problem
Since the classes of forward and local schemes are larger, they can
possibly achieve lower densities, but by how much?
The ideal is to answer some of these questions by proving a lower bound and
providing a scheme that has density equal to this lower bound, ideally for all
parameters, but otherwise for a subset.
We can also ask what happens when $w\to
\infty$ (for $k$ fixed), or when $k\to\infty$ (for $w$ fixed)?
And can how does this depend on the alphabet size?
Or maybe we can not quite make schemes that /exactly/ match the lower bound, but we /can/ make schemes
that are within $1\%$ of the lower bound, or that are asymptotically a factor
$1+o(1)$ away.

There are also different parameter regimes to consider: small $k=1$ or
$k<\log_\sigma(w)$, slightly larger $k\leq 10$, and more practical $k$ up to
$\approx 30$, or even larger $k$ in theory. Similarly, we can consider small $w\leq 10$,
but also $w\approx 1000$ is used in practice. The alphabet size will usually be
$\sigma=4$, but also this can vary and can be $\sigma=256$ for ASCII input.

If we do find (near) optimal schemes, we would
like these to be /pure/ in some way: ideally we can provide a simple analysis of
their density, as opposed to only being able to compute it without any
additional understanding. This somewhat rules out solutions found by brute force
approaches, as they often do not provide insight into why they work well.
This motivates the following definition.

#+begin_definition Pure sampling scheme
A sampling scheme is /pure/ when it can be implemented in $O(\poly(w+k))$ time
and space.
#+end_definition

There is also the problem to minimize the particular density on a given input
string. We do not discuss this here, but some works in this direction are
[cite/t:@deepminimizer] and [cite/t:@polar-set-minimizers].

** Computing the density
The density of a sampling scheme is defined as the expected particular density
on an infinitely long string. In practice, we can approximate it closely by
simply computing the particular density on a sufficiently long random string of,
for example, 10 million characters.

When $\sigma^{w+k}$, the following theorem forms the basis for computing the density of
schemes exactly [cite:Lemma 4 @improved-minimizers;@miniception]:

#+begin_definition (Charged) context
For forward schemes, a /context/ is a string of length $c = w+k$, consisting of
two overlapping windows.

For a sampling scheme $f$, a context $C$ is /charged/ when two different positions
are sampled from the first and second window, i.e., $f(C[0..w+k-1)) \neq 1+f(C[1..w+k))$.
#+end_definition

For a /local/ scheme, a context has length $2w+k-1$ instead [cite:Section 3.1
@small-uhs;Section 3.2 @sampling-lower-bound], and is charged when
the last window samples a k-mer not sampled by /any/ of the previous contained
windows. This larger context is necessary because a local scheme can jump
backwards. In practice, this

As a small variant on this, in [cite/t:@winnowing], a /window/ is charged when it
is the first window to sample a k-mer.

#+begin_theorem Computing density (context)
The density of a forward scheme equals the probability that,
in a uniform random context of length $c=w+k$, two different k-mers are sampled
from the two windows.

Thus, the density can be computed exactly by iterating over all $\sigma^{w+k}$ contexts.
#+end_theorem

We can also approximate the density by sampling sufficiently many random
contexts.
A somewhat more efficient method is to use a De Bruijn sequence instead.
A De Bruijn sequence of order $c$ is any circular sequence of length
$\sigma^c$ that contains every sequence of length $c$ exactly once [cite:@debruijnseq].
We have the following theorem, again by [cite:Lemma 4 @improved-minimizers]:

#+begin_theorem Computing density (De Bruijn sequence)
The density of any forward scheme equals its particular density on an order
$c=w+k$ De Bruijn sequence.
For /local/ schemes, the order $c=2w+k-2$ De Bruijn graph must be used instead.
#+end_theorem

Another approach, that follows from the first, is by considering cycles of
length $c$, rather than just strings of length $c$.
#+begin_newtheorem Computing density (cycles)
The density of any forward scheme equals its average particular density over all
cyclic strings of order $c=w+k$ for forward schemes and $c=2w+k-2$ for local schemes.
#+end_newtheorem

** The density of random minimizers
As a warm-up, we will compute the density of the random minimizer.
We mostly follow the presentation of [cite/t:@miniception].

We start by analysing when a context is charged [cite:Lemma 1 @miniception].

#+begin_theorem Charged contexts of minimizers
For a minimizer scheme, a context is charged if and only if the smallest k-mer
in the context is either the very first, at position $0$, or the very last, at
position $w$.
#+end_theorem

#+begin_proof
The context contains $w+1$ k-mers, the first $w$ of which are in the first
window, say $W$, and the last $w$ of which are in the second window, say $W'$.

When the (leftmost) overall smallest k-mer is either the very first or very last
k-mer, the
window containing it chooses that k-mer, and the other window must necessarily
sample a different k-mer.
On the other hand, when the smallest k-mer is not the very first or very last,
it is contained in both windows, and both windows will sample it.
#+end_proof

Before computing the actual density, we need to bound the probability that a
window contains two identical k-mers [cite:Lemma 9 @miniception].

#+begin_theorem Duplicate k-mers
For any $\varepsilon > 0$, if $k > (3+\varepsilon) \log_\sigma (c)$, the
probability that a random context of $c$ k-mers contains two identical k-mers is $o(1/c)$.
#+end_theorem
#+begin_proof_sketch
For any two non-overlapping k-mers in the window, the probability that they are
equal is $\sigma^{-k} \leq 1/c^{3+\varepsilon} = o(1/c^3)$.
It can be seen that the same holds when two k-mers overlap by $d>0$ characters.

There are $c^2$ pairs of k-mers, so by the union bound, the probability that any
two k-mers are equal is $o(1/c)$.
#+end_proof_sketch

In practice, $k > (2+\varepsilon) \log_\sigma(c)$ seems to be
sufficient, but this has not been proven yet. Even stronger, for most
applications of the lemma, $k>(1+\varepsilon)\log_\sigma(c)$ appears sufficient.

This leads us to the density of the random minimizer [cite:Theorem 3
@miniception], which is a more refined version of the simple density of
$2/(w+1)$ computed in both [cite/t:@winnowing] and [cite/t:@minimizers].

#+begin_theorem Random minimizer density
For $k>(3+\varepsilon)\log_\sigma(w+1)$, the density of the random minimizer is

$$
\frac{2}{w+1} + o(1/w).
$$
#+end_theorem
#+begin_proof
Consider a uniform random context $C$ of $w+k$ characters and $w+1$ k-mers.
When all these k-mers are distinct, the smallest one is the first or last with
probability $2 / (w+1)$. When the k-mers are not all distinct, this happens with
probability $o(1/w)$, so that the overall density is bounded by $2/(w+1) + o(1/w)$.
#+end_proof

Using a more precise analysis, it can be shown that for sufficiently large $k$,
the random minimizer has, in fact, a density slightly /below/ $2/(w+1)$.
In [cite/t:@improved-minimizers] this is shown using universal hitting sets.
In [cite/t:Theorem 4 @random-mini-density], it is shown that the density of the random
minimizer is less than $2$ for all sufficiently large $k\geq w\geq w_0$, where
$w_0$ is a constant that may depend on the alphabet size $\sigma$.

It was originally conjectured that the density of $2/(w+1)$ is the best one can
do [cite:@winnowing], but this has been refuted by newer methods, starting with
DOCKS [cite:@docks;@improved-minimizers]. (Although it must be remarked that the
original conjecture is for a more restricted class of ''local'' schemes
than as defined here.)

** Universal hitting sets
Universal hitting sets are an alternative way to generate minimizer schemes.
They were first introduced by [cite:@docks-wabi;@docks].
#+begin_definition Universal hitting set
A /Universal hitting set/ (UHS) $U$ is an ''unavoidable'' set of k-mers, so
that every window of length $\ell=k+w-1$ contains at least one k-mer from the set.
#+end_definition

Universal hitting sets are an example of a /context-free/ scheme
[cite:@syncmers], where each k-mer is sampled only if it is part of the UHS:

#+begin_definition Context free scheme
A /context-free/ scheme decides for each k-mer independently (without
surrounding context) whether to sample it or not.
#+end_definition

There is a tight correspondence between universal hitting
sets and minimizer schemes [cite:Section 3.3 @improved-minimizers; Section 2.1.5 @asymptotic-optimal-minimizers;@small-uhs]:

#+begin_definition Compatible minimizer scheme
Given a universal hitting set $U$ on k-mers, a /compatible/ minimizer scheme
uses an order $\Ok$ that orders all elements of $U$ before all elements not
in $U$.
#+end_definition

The density of a compatible minimizer scheme is closely related to the size of
the universal hitting set [cite:Lemma 1 @asymptotic-optimal-minimizers].

#+begin_theorem Compatible minimizer density
When a minimizer scheme $f$ is compatible with a UHS $U$, its density satisfies

$$
d(f) \leq |U|/\sigma^k.
$$
#+end_theorem
#+begin_proof_sketch
Consider a De Bruijn sequence of order $c=w+k$. This contains each $c$-mer
exactly once, and each $k$-mer exactly $\sigma^w$ times.
Thus, the number of k-mers in $U$ in the De Bruijn sequence is $|U| \cdot \sigma^w$.

Suppose the minimizer scheme samples $s$ distinct k-mers in the De Bruijn sequence. Since $U$ is
an UHS, $s \leq |U| \cdot \sigma^w$. The density of $f$ is the fraction of
sampled k-mers,

$$
d(f) = s / \sigma^c \leq |U| \cdot \sigma^w / \sigma^{w+k} = |U| / \sigma^k.
$$
#+end_proof_sketch

From this, it follows that creating smaller universal hitting sets typically
leads to better minimizer schemes.

Lastly, [cite/t:@improved-minimizers] introduces the /sparsity/ of a universal
hitting set $U$ as the fraction of contexts of $w+k$ characters that contain exactly
one k-mer from $U$. Then, the density of a corresponding minimizer scheme can be
computed as $(1-\sp(U))\cdot \frac{2}{w+1}$.

*Minimum decycling set.*
Where a universal hitting set is a set of k-mers such that every length $w+k-1$
window contains a k-mer in the UHS, a /minimum decycling set/ (MDS) is a smallest set of k-mers
that hits every /infinitely long/ string. Equivalently, if we take the complete De
Bruijn graph of order $k$ and remove all nodes in the MDS from it, this should leave a
graph without cycles. It can be seen that the number of /pure cycles/ in the De
Bruijn graph is a lower bound on the size of an MDS, and indeed this lower bound
can be reached.

*Mykkeltveit MDS.*
One construction of an MDS is by Mykkeltveit [cite:@mykkeltveit].
To construct this set $\Dk$, k-mers are first embedded into the complex plane via a
character-weighted sum of the $k$ $k$'th roots of unity $\omega_k$: a k-mer $X$ is mapped
to $x=\sum_i X_i\cdot\omega_k^i$.
This way,
shifting a k-mer by one position corresponds to a rotation, followed by the
addition or subtraction of a real number.
Based on this, $\Dk$ consists of those k-mers whose embedding
$x$ corresponds to the first clockwise rotation with positive imaginary part, i.e.,
such that $\pi-2\pi/k\leq \arg(x)<\pi$.

** Asymptotic results
In [[asymptotics]], we summarize a few theoretical results on the asymptotic density of
minimizer, forward, and local schemes as $k\to\infty$ or $w\to\infty$.
Some of these results will be covered more in-depth later.

#+name: asymptotics
#+caption: Summary of asymptotic density results.
| Class     | $k\to\infty$                | $w\to\infty$ lower bound           | $w\to\infty$ best |
| Minimizer | $1/w$, rot-mini, *mod-mini* | $1/\sigma^k$                       | $1/\sigma^k$      |
| Forward   | $1/w$, rot-mini, *mod-mini* | $2/(w+k)$  (was $1/w$)             | $(2+o(1))/w$ (was $O(\ln(w)/w)$) |
| Local     | $1/w$, rot-mini, *mod-mini* | $1.5/(w+\max(k-2, 1))$ (was $1/w$) | $(2+o(1))/w$ (was $O(1/w)$) |

TODO: Use more precise lower bound for $k\to\infty$: $\ceil{(w+k)/w}/(w+k)$?

When $k\to\infty$, both the rot-minimizer [cite:@asymptotic-optimal-minimizers]
and the new mod-minimizer (section [[#modmini]]) achieve optimal density $1/w$.

Slightly simplified, the *rot-minimizer* ranks k-mers by the sum of the
characters in positions $0\pmod w$, so that for $w=2$, it would sum every other
character of the k-mer. Then, it selects the k-mer for which this sum is maximal.

When $w\to\infty$, minimizer schemes have a big limitation. Since they only
consider the k-mers, when $w\gg \sigma^k$, almost every window will contain the
smallest k-mer. Thus, we obtain [cite/t:theorem 2 @asymptotic-optimal-minimizers]:

#+begin_theorem Large-$w$ minimizer scheme
For any /minimizer/ scheme $f$, the density is at least $1/\sigma^k$, and
converges to this as $w\to\infty$.
#+end_theorem

This implies that as $w\to\infty$, fixed-$k$ minimizer schemes can never reach
the optimal density of $1/w$.
On the other hand, this lower bound does not hold for forward and local schemes.
For forward schemes, we can use the lower bound of [cite:Theorem 1
@sampling-lower-bound] to get $2/(w+k)$ (see section [[#near-tight-lb]]). For local schemes, Remark 7 applies and with $k' = \max(k,3)$ we
get the bound $1.5/(w+\max(k-2, 1))$.

From the other side, Proposition 7 of [cite/t:@asymptotic-optimal-minimizers] shows
that:
#+begin_theorem Forward-density for $w\\to\\infty$ (1)
There exists a forward scheme with density $O(1/\sqrt w)$ for $k$ fixed and $w\to\infty$.
#+end_theorem
#+begin_proof_sketch
Consider $k' = \log_\sigma{\sqrt w}$. For sufficiently large $w$ we have $k'
\geq k$ and we consider any minimizer scheme on $k'$-mers with window size
$w'=w+k-k'\leq w$. Asymptotically, this has density $O(1/\sqrt w)$.
#+end_proof_sketch

Later, this was improved to [cite:Theorem 2 @small-uhs]:

#+begin_theorem Forward-density for $w\\to\\infty$ (2)
There exists a forward scheme with density $O(\ln(w) / w)$ for $k$ fixed and $w\to\infty$.
#+end_theorem
#+begin_proof_sketch
Let $w' = k' =  w/2$, so that $w'+k'-1 = w-1 \leq w+k-1$. We'll build a UHS on
$k'$-mers with window guarantee $w'$.
Set $d = \floor{\log_\sigma(k'/\ln k'))}-1$.
Let $U$ be the set of $k'$-mers that either start with $0^d$, or else do not
contain $0^d$ at all.
The bulk of the proof goes into showing that this set has size $O(\ln(k')/k')
\cdot \sigma^{k'}$.
Every string of length $w'+k'-1=w-1$ will either contain $0^d$ somewhere in its
first $w'$ positions, or else the length-$k'=w'$
prefix does not contain $0^d$ and is in $U$. Thus, $U$ is a UHS with window
guarantee $w'$. We conclude that the density of a compatible minimizer scheme is
bounded by $O(\ln(k')/k') = O(\ln(w)/w)$.
#+end_proof_sketch

But this is still not optimal: reduced bd-anchors [cite:Lemma 6 @bdanchors] (see section
[[*Bd-anchors]]) are a local scheme with $k=1$ and density $O(1/w)$.

We further improve on this using SUS-anchors (section [[*NEW: SUS-anchors]]), which
is a forward scheme with density $(2+o(1))/w$ as $w\to\infty$.
TODO: Prove this.
TODO: What about $k$?

While it may seem from [[asymptotics]] that local schemes are not better than forward
schemes, there /are/ parameters for which local schemes achieve strictly better
density [cite:@asymptotic-optimal-minimizers;@sampling-lower-bound].
Unfortunately, there currently is not good theory of local schemes, and these
improved schemes were found by solving an integer linear program (ILP) for small
parameters.
Lower bounds on local scheme density for small $k$ and $w$ are also not nearly
as tight as for forward schemes.

** Variants

There are several variations on sampling schemes that generalize in different
ways.

/Global/ schemes drop the requirement that whether a k-mer is sampled only
depends on a local context. Examples are minhash [cite:@minhash] and more general
FracMinHash [cite:@fracminhash], both sampling the smallest k-mers of an entire
string.

On strings with many repeated characters, all k-mers have the same hash, and
hence all k-mers are sampled. /Robust winnowing/ [cite:@winnowing] prevents
this by sampling the rightmost minimal k-mer by default, unless the minimizer of
the previous window has the same hash, in which case that one is ''reused''.

/Min-mers/ [cite:@minmers] are a second variant, where instead of choosing a
single k-mer from a window, $s$ k-mers are chosen instead, typically from a
window that is $s$ times longer.

/Finimizers/ [cite:@finimizers] are /variable length/ minimizers that ensure
that the frequency of the minimizers is below some threshold.

For DNA, it is often not know to which strand a give sequence belongs.
Thus, any analysis should be invariant under taking the reverse complement.
In this case, /canonical minimizers/ can be used.
#+begin_definition Canonical sampling scheme
A sampling scheme $f$ is /canonical/ when for all windows $W$ and their reverse
complement $\rc(W)$, it holds that

$$f(\rc(W)) = w-1-f(W).$$
#+end_definition

One way to turn any minimizer scheme into a canonical minimizer scheme is by
using the order $\Ok^{\rc}(x) = \min(\Ok(x), \Ok(\rc(x)))$
[cite:@minimizers;@nthash] or $\Ok^{\rc}(x) = \Ok(x) + \Ok(\rc(x))$ [cite:@nthash2;@simd-minimizers-preprint].
Still, this leaves the problem of whether to select the leftmost or rightmost
occurrence of a kmer in case of ties. This can be solved using the technique of
the /refined minimizer/ [cite:@refined-minimizer;@simd-minimizers-preprint]: ensure that $w+k-1$ is odd,
and pick the strand with the highest count of =GT= bases.
A way to encode canonical k-mers that saves one bit is presented in [cite/t:@encoding-canonical-kmers].
Lastly, [cite/t:@knonical-reverse-complements] provides a way to turn
context-free methods into a canonical version.

Lastly, [cite/t:@syncmers] introduces the /conservation/ of a scheme
as the expected fraction of bases covered by sampled k-mers.
In [cite/t:@local-kmer-selection],
sampling schemes are generalized to /k-mer selection methods/ that are allowed to sample /any
subset/ of k-mers from the input string, and /local selection methods/ that
return any /subset/ of k-mers from a window.
Both these papers focus on context-free schemes, as such k-mers are
better preserved.


* Lower bounds
The starting point for this section is the trivial lower bound:
#+begin_theorem Trivial lower bound
For any local, forward, or minimizer scheme $f$, the density is at least $1/w$.
#+end_theorem
Naturally, all proofs of tighter lower bounds use the fact that at least one
k-mer must be sampled every $w$ positions. All theorems apply it in a slightly
different context though.

This was first improved by Schleimer et al. [cite:@winnowing] to approximately $1.5/(w+1)$, although using assumptions that are too strong in
practice (Section
[[*Schleimer et al.'s bound]]).
Marcais et al. [cite:@asymptotic-optimal-minimizers] give a weaker version that
/does/ hold for all forward schemes, of just above $1.5/(w+k)$ (Section [[*Marcais
et al.'s bound]]). At the core, it considers two windows spaced apart by $w+k$
positions. The first window than has a minimizer, and with probability $1/2$, a
second additional minimizer is needed to 'bridge the gap' to the second window.
In the appendix of [cite/t:@modmini], Groot Koerkamp and Pibiri improve this
further to $1.5/(w+k-0.5)$ by using a slightly more precise analysis (Section [[*Improving and extending Marcais et al.'s bound]]). Because of the similarity of these three proofs, we
only provide sketches of the first two, followed by a full proof of the strongest
version.

Still, these bounds appeared very far from tight, given that e.g. for $k=1$ the
best schemes do not go below $2/(w+1)$, which is much larger than $1.5/(w+0.5)$.
For a large part, Kille and Groot Koerkamp et al. [cite:@sampling-lower-bound]
resolved this by a new near-tight lower bound of $\ceil{(w+k)/k}/(w+k)$
(Section [[#near-tight-lb]]).
This bound looks at cycles of length $w+k$, and uses that at least
$\ceil{(w+k)/k}$ minimizers must be sampled on such a cycle.
They also prove a slightly improved version that is the first lower bound to be
/exactly/ tight for a subset of parameters.


** Schleimer et al.'s bound
The first improvement over the trivial lower bound was already given in the
paper that first introduced minimizers [cite:theorem 1 @winnowing ]:

#+begin_theorem Lower bound when hashing k-mers
Consider a $w$-tuple of uniform random independent hashes of the k-mers in a tuple.
Now let $S$ be any function that selects a k-mer based on these $w$ hashes.
Then, $S$ has density at least

$$
d(S) \geq \frac{1.5 + \frac{1}{2w}}{w+1}.
$$
#+end_theorem

#+begin_proof_sketch
Let $W_i$ and $W_{i+w+1}$ be the windows of $w$ k-mers starting at positions $i$
and $i+w+1$ in a long uniform random string.
Since $W_i$ and $W_{i+w+1}$ do not share any k-mers, the hashes of the k-mers in
$W_i$ are independent of the hashes of the k-mers in $W_{i+w+1}$.
Now, we can look at the probability distributions $X$ and $X'$ of the sampled
position in the two windows. Since the hashes are independent, these
distributions are simply the same, $X \sim X'$.
There are $(i+w+1+X') - (i+X) - 1 = w+(X'-X)$ ''skipped'' k-mers between the two
sampled k-mers. When $X\leq X'$, this is $\geq w$, which means that at least one
additional k-mer must be sampled in this gap. It is easy to see that $\P[X\leq
X'] \geq 1/2$, and using Cauchy-Schwartz this can be improved to $\P[X\leq X']\geq
1/2 + 1/(2w)$. Thus, out of the expected $w+1$ k-mers from position $i+X$ to $i+w+1+X'$
(exclusive), we sample at least $1 + 1/2 + 1/(2w)$ in expectation, giving the result.
#+end_proof_sketch

Unfortunately, this lower-bound assumes that k-mers are hashed before being processed
them further using a potentially ''smart'' algorithm $S$. This class of schemes
was introduced as /local algorithms/, and thus caused some confusion (see e.g. [cite/t:@improved-minimizers]) in that it
was also believed to be a lower bound on the more general /local schemes/ as we
defined them. This inconsistency was first noticed in
[cite/t:@asymptotic-optimal-minimizers], which introduces a ''fixed'' version of
the theorem.


** Marcais et al.'s bound
In [cite/t:@asymptotic-optimal-minimizers], the authors give a weaker variant of
the theorem of [cite/t:@winnowing] that /does/ hold for all forward schemes:
#+begin_theorem Lower bound for forward schemes
Any forward scheme $f$ has density at least

$$
d(f) \geq \frac{1.5 + \max\left(0, \left\lfloor\frac{k-w}{w}\right\rfloor\right) +
\frac 1{2w}}{w+k}.
$$
#+end_theorem
#+begin_proof_sketch
The proof is very comparable to the one of [cite/t:@winnowing].
Again, we consider two windows in a long uniform random string.
This time, however, we put them $w+k+1$ positions
apart, instead of just $w+1$. This way, the windows do not share any characters (rather
than not sharing any k-mers) and thus, the probability distributions $X$ and $X'$
of the position of the k-mers sampled from $W_i$ and $W_{i+w+k+1}$ are
independent again.

They again consider the positions $s_1=i+X$ and $s_2=i+w+k+1+X'$, and lower bound
the expected number of sampled k-mers in this range.
The length of the range is $w+k$, leading to the denominator, and the
$1.5+1/(2w)$ term arises as before. The additional $\left\lfloor
\frac{k-w}{w}\right\rfloor$ term arises from the fact that when $k$ is large,
just sampling one additional k-mer in between $s_1$ and $s_2$ is not sufficient
to ensure a sample every $w$ positions.
#+end_proof_sketch

** Improving and extending Marcais et al.'s bound
It turns out that the theorem TODO REF is slightly inefficient. In
[cite/t:@modmini], we improve it.

#+begin_newtheorem Improved lower bound
The density of any /local/ scheme $f$ satisfies

$$
d(f) \geq \frac{1.5}{w+k-0.5}.
$$
#+end_newtheorem
#+begin_proof
TODO: Copy over full proof?
#+end_proof
#+begin_proof_sketch
Again, we highlight here the differences compared to the previous proof.
The full proof is replicated in Appendix TODO.

First, the $+\left\lfloor\frac{k-w}{w}\right\rfloor$ term only contributes
anything when $k\geq w$. It turns out that for $k> (w+1)/2$, the lower bound is
provably less than the trivial bound of $1/w$. Thus, we may as well drop this term.

Second, we can slightly improve the analysis of $\P[X\leq X']$.
Instead of considering a single interval of two consecutive windows $w+k$ apart,
we can instead consider /three/ disjoint windows at positions $i$, $i+w+k-1$, and
$i+2w+2k-1$. Let $X$, $X'$, and $X''$ be the positions of the sampled k-mers.
Then we sample at least the k-mers at positions $s_1=i+X$ and $s_2=i+w+k-1+X'$.
When $X<X'$, the number of bases between $s_1$ and $s_2$ is at least $s_2-s_1-1
= w+k-2+(X'-X) \geq w+k-1$. Thus, an additional k-mer must be sampled from this
window with probability $\P[X<X']$. Similarly, an additional k-mer must be
sampled between $s_2$ and $s_3=i+2w+2k-1+X''$ with probability $\P[X'\leq X'']$. Since $X\sim X' \sim
X''$ and since the three distributions are fully independent, we have $\P[X'\leq
X''] = \P[X'\leq X] = 1 - \P[X < X']$. Thus, in expectation we need to sample at least one
additional k-mer. We then get a lower bound of

$$
\frac{1 + \P[X < X'] + 1 + \P[X'\leq X'']}{2w+2k-1} = \frac{3}{2w+2k-1} = \frac{1.5}{w+k-0.5}.
$$

Lastly, we note that this lower bound does not use the fact that $f$ is forward,
and thus, it holds for local schemes as well.
#+end_proof_sketch

In TODO PLOT we can see that this new version indeed provides a small
improvement over the previous lower bound. Nevertheless, a big gap remains
between the lower bound and, say, the density of the random minimizer.

It is also clear that this proof is far from tight. It uses that an additional
k-mer must be sampled when a full window of $w+k-1$ characters fits between $s_1$ and $s_2$, while in
practice an additional k-mer is already needed when the distance between them is
larger than $w$. However, exploiting this turns out to be difficult: we
can not assume that the sampled positions in overlapping windows are
independent, nor is it easy to analyse a probability such as $\P[X \leq X''-k]$.

** A near-tight lower bound on the density of forward sampling schemes
:PROPERTIES:
:CUSTOM_ID: near-tight-lb
:END:
In [cite/t:@sampling-lower-bound], we prove a nearly tight lower bound on the
density of /forward/ schemes.
Here, we first present a slightly simplified version. The full version can be
found in (TODO REF).

#+begin_newtheorem Near-tight lower bound (simple)
Any forward scheme $f$ has a density at least

$$
d(f) \geq \frac{\ceil{\frac{w+k}{w}}}{w+k}.
$$
#+end_newtheorem
#+begin_proof
The density of a forward scheme can be computed as
the probability that two consecutive windows in a random length $w+k$ context
sample different k-mers [cite:Lemma 4 @improved-minimizers].  From this, it follows that we can also
consider /cyclic strings/ (cycles) of length $w+k$, and compute the expected
number of sampled k-mers along the cycle. The density is then this count divided
by $w+k$.

Because of the window guarantee, at least one out of every $w$ k-mers along the
length $w+k$ cycle must be sampled. Thus, at least $\lceil (w+k)/w\rceil$ k-mers
must be sampled in each cycle. After dividing by the number of k-mers in the
cycle, we get the result.
#+end_proof

The full and more precise version is as follows [cite:Theorem 1 @sampling-lower-bound].

#+begin_theorem Near-tight lower bound (improved)
Let $M_\sigma(p)$ count the number of aperiodic necklaces of length $p$ over an
alphabet of size $\sigma$. Then, the density of any forward sampling scheme $f$ is
at least

$$
d(f) \geq g_\sigma(w,k) :=  \frac{1}{\sigma^{w+k}} \sum_{p | (w+k)} M_\sigma(p) \left\lceil \frac
pw\right\rceil \geq \frac{\left\lceil\frac{w+k}{w}\right\rceil}{w+k} \geq \frac 1w,
$$

where the middle inequality is strict when $w>1$.
#+end_theorem
#+begin_proof_sketch
The core of this result is to refine the proof given above.
While indeed we know that each cycle will have at least $\ceil{(w+k)/w}$
sampled k-mers, that lower bound may not be tight. For example, if the cycle
consists of only zeros, each window samples position $i + f(000\dots 000)$, so that
in the end every position is sampled.

We say that a cycle has /period/ $p$ when it consists of $(w+k)/p$
copies of some pattern $P$ of length $p$, and $p$ is the maximum number for which this holds.
In this case, we can consider the cyclic string of $P$, on which we must sample
at least $\ceil{p/w}$ k-mers. Thus, at least $\frac{w+k}{p}\ceil{\frac pw}$
k-mers are sampled in total, corresponding to a particular density along the
cycle of at least $\frac{1}{p}\ceil{\frac pw}$.

Since $p$ is maximal, the pattern $P$ itself must be /aperiodic/. When
$M_\sigma(p)$ counts the number of aperiodic cyclic strings of length $p$,
the probability that a uniform random cycle has period $p$ is $p\cdot M_\sigma(p) /
\sigma^{w+k}$, where the multiplication by $p$ accounts for the fact that each pattern
$P$ gives rise to $p$ equivalent cycles that are simply rotations of each other.
Thus, the overall density is simply the sum over all $p\mid (w+k)$:

$$
d(f)
\geq \sum_{p | (w+k)} \frac{p\cdot M_\sigma(p)}{\sigma^{w+k}}\cdot \frac{1}{p} \left\lceil \frac pw\right\rceil
=\frac 1{\sigma^{w+k}} \sum_{p | (w+k)} M_\sigma(p)  \left\lceil \frac pw\right\rceil.
$$

The remaining inequalities follow by simple arithmetic.
#+end_proof_sketch

As can be seen in TODO PLOT, this lower bound jumps up at values $k\equiv 1 \pmod w$.
In practice, if some density $d$ can be achieved for parameters $(w,k)$, it can
also be achieved for any larger $k'\geq k$, by simply ignoring the last $k'-k$
characters of each window. Thus, we can ''smoothen'' the plot via the following
corollary.

#+begin_theorem Near-tight lower bound (monotone)
Any forward scheme $f$ has density at least

$$
d(f)
\geq g'_\sigma(w,k) := \max\big(g_\sigma(w,k), g_\sigma(w,k')\big)
\geq \max\left(\frac 1{w+k}\ceil{\frac{w+k}w}, \frac1{w+k'}\ceil{\frac{w+k'}w}\right),
$$

where $k'$ is the smallest integer $\geq k$ such that $k' \equiv 1 \pmod w$.
#+end_theorem

At this point, one might assume that a smooth ''continuation'' of this bound
(that exactly goes through the ''peaks'') also holds
(TODO REF FIG), but this turns out to not be the case, as for example
decycling-based minimizers break it [cite:@minimum-decycling-set].

*Searching optimal schemes.*
For small parameters $\sigma$, $w$, and $k$, we can search for optimal schemes
using an integer linear program (ILP) [cite:@sampling-lower-bound]. In short,
we define an integer variable $x_W=f(W) \in [w]$ for every window $W \in
\sigma^{w+k-1}$, that indicates the position of the k-mer sampled from this
window.
For each context containing consecutive windows $W$ and $W'$, we add a boolean
variable $y_{(W, W')}$ that indicates whether this context is charged.
Additionally, we impose that $f(W') \geq f(W)-1$ to ensure the scheme is forward.
The objective is to minimize the number of charged edges, i.e., to minimize the
number of $y$ that is true.
In practice, the ILP can be sped up by imposing constraints equivalent to our
lower bound: for every cycle of length $w+k$, at least $\ceil{(w+k)/w}$ of the
contexts must be charged. This helps especially when $k\equiv 1\pmod w$, in
which case it turns out that the ILP /always/ finds a forward scheme matching
the lower bound, and hence can finish quickly. In other cases,
we can also use length $w+k'$ cycles instead, with $k$ as in TODO ref.

*** TODO Discussion
- Small params
  - Minimum is reached whenever $k\equiv 1\pmod w$, in particular whenever $k=1$.
  - minimum is also reached for $w=\sigma=2$ and any $k$.
  - For $\sigma=2$ and $1<k<w$,


*Local schemes.* The lower bounds discussed so far can also be extended to local
schemes by replacing $c=w+k$ by $c=2w+k-2$. Sadly, this does not lead to a good
bound. In practice, the best local schemes appear to be only marginally better than
the best forward schemes, while the currently established theory requires us to
increase the context size significantly, thereby making all inequalities
much more loose. Specifically, the tightness of the bound is mostly due to the
rounding up in
$\frac{1}{c}\ceil{\frac{c}{k}}=\frac{1}{w+k}\ceil{\frac{w+k}{k}}$, and the more
we increase $c$, the smaller the effect of the rounding will be.

#+begin_openproblem Local scheme density
In practice, local schemes are only slightly better than forward schemes, while
the current best lower-bounds for local schemes are much worse. Can we prove a
lower bound that is close to that of forward schemes?
Or can we bound the improvement that local schemes can make over forward schemes?
#+end_openproblem

*** TODO Commentary
Bryce Kille and myself independently discovered the basis of this theorem during
the summer of 2024. In hindsight, I am very surprised it took this long (over 20
years!) for this theorem to be found. Minimizers were originally defined in
2003-2004, and only in 2018 the first improvement (or fix, rather) of Schleimer
et al.'s original bound was found in [cite/t:@asymptotic-optimal-minimizers].
Specifically, all ingredients for the proof have been around for quite some time
already:
- The density of the random minimizer is $2/(w+1)$, which
  ''clearly'' states: out of every $w+1$ consecutive k-mers, at least $2$ must
  be sampled. We just have to put those characters into a cycle.
- The density of any forward scheme can be computed using an order $w+k$ De
  Bruijn sequence, so again, it is only natural that looking at strings of length at
  least $w+k$ is necessary. Cyclic strings are a simple next step.
- And also, partitioning the De Bruijn graph into cycles is something that was
  done before by Mykkeltveit [cite:@mykkeltveit].

* Sampling schemes
We now turn our attention from lower bounds and towards low-density sampling schemes.
We first consider various existing schemes, that we go over in three groups.
In Section [[#lexmin]] we consider some simple
variants of lexicographic minimizers.
In Section [[*UHS-inspired schemes]], we consider some schemes that build on
universal hitting sets, either by explicitly constructing one or by using
related theory. We also include here the greedy minimizer, which is also
explicitly constructed using a brute force search.
Then, in Section [[*Syncmer-based schemes]], we cover some schemes based on
syncmers.

We end with two new schemes.
First, the /open-closed minimizer/ [cite:@oc-modmini-preprint] (Section [[*Open-closed minimizer]]), which extends
the miniception by first preferring the smallest open syncmer, falling back to
the smallest closed syncmer, and then falling back to the smallest k-mer
overall. This simple scheme achieves near-best density for $k\leq w$.

Second, we introduce the /(extended) mod-minimizer/ and the /open-closed
mod-mini/ [cite:@modmini;@oc-modmini-preprint]. These schemes significantly
improve over all other schemes when $k>w$ and converge to density $1/w$ as
$k\to\infty$. Additionally, we show that they have optimal density when $k\equiv
1\pmod w$ and the alphabet is large.

** Variants of lexicographic minimizers
:PROPERTIES:
:CUSTOM_ID: lexmin
:END:
The lexicographic minimizer is known to have relatively bad density because it
is prone to sampling multiple consecutive k-mers when there is a run of =A= characters.
Nevertheless, they achieve density $O(1/w)$ as $k=\floor{\log_\sigma(w/2)}-2$
and $w\to\infty$ [cite:@miniception].

This can be fixed by using an /alternating/ order [cite:@minimizers]:
we can use lexicographic order for character in /even/ positions, including the first, and /reverse/
lexicographic order for all /odd/ positions, including the second. Thus, the
smallest string would be =AZAZAZ...=. This way, long runs of equal characters
are usually avoided, unless the entire window consists only of a single character.

A second variant is the /abb/-order [cite:@minimally-overlapping-words]. This
compares the first character lexicographically, and then uses order
~B = C = ... = Z < A~ from the second position onward, so that the smallest string is =ABBBB=,
where the number of non-=A= characters following the first =A= is maximized.

Another way to prevent over-sampling consecutive k-mers is to order k-mers by
their frequency in the input data,
favouring rare k-mers over more common ones [cite:@debruijngraph-representation].

** UHS-inspired schemes

*DOCKS.* In [cite/t:@docks-wabi;@docks], the authors introduce an algorithm to generate
small universal hitting sets. It works in two steps.
It starts by using Mykkeltveit's minimum decycling set [cite:@mykkeltveit] such that every infinitely long
string contains a k-mer from the decycling set. Then, it repeatedly adds the
k-mer to the UHS that is contained in the largest number of length $\ell=w+k-1$
windows that does not yet contain a k-mer in the UHS.
In practice, the exponential runtime in $k$ and $w$ is a bottleneck. A first
speedup is to consider the k-mer contained in the largest number of paths of
/any/ length. A second method for larger $k' > k$, called /naive extension/, is to simply ignore
the last $k'-k$ characters of each k-mer and then use a UHS for $k$-mers.
DOCKS can generate UHSes up to around $k=13$, and for $k=10$ and $w=10$, it has
density down to $1.737/(w+1)$ [cite:@improved-minimizers], thereby being the first
scheme that breaks the conjectured $2/(w+1)$ lower bound.

*$\boldremuval$* [cite:@practical-uhs] is a method that builds on DOCKS.
Starting with some $(w,k-1)$ UHS generated by DOCKS,
first uses naive extension to get a $(w, k)$ UHS $U'$. Then, it tries to reduce the
size of this new UHS by removing some k-mers. In particular, if a k-mer only
ever occurs in windows together with another k-mer of $U'$, then this k-mer may
be removed from $U'$. Instead of greedily dropping k-mers, and ILP is built to
determine an optimal set of k-mers to drop. This process is repeated until the
target $k$ is reached, which can be up to at least $200$, as long as $w\leq 21$ is
sufficiently small.

*PASHA* [cite:@pasha] also builds on DOCKS and focuses on improving the
construction speed. It does this by parallellizing the search for k-mers to
add to the UHS, and by adding multiple k-mers at once (each with some
probability) rather than only the one with the highest count of un-covered
windows containing it. These optimizations enable PASHA to generate schemes up
to $k=16$, while having density comparable to DOCKS.

*Decycling-based minimizer.* An improvement to the brute force constructions of
DOCKS, $\remuval$, and PASHA came with a minimizer scheme based directly on
minimum decycling sets [cite:@minimum-decycling-set]:
In any window, prefer choosing a k-mer in $\Dk$, if
there is one, and break ties using a random order. They also introduce
the /double decycling/ scheme. This uses the /symmetric/ MDS $\Dtk$ consisting
of those k-mers for which $-2\pi/k\leq \arg(x)<0$. It then first prefers
k-mers in $\Dk$, followed by k-mers in $\Dtk$, followed by k-mers that are in neither.

It is easy to detect whether a k-mer is in the MDS or not without any memory, so
that this method scales to large $k$.
Surprisingly, not only is this scheme conceptually simpler, but it also
has significantly lower density than DOCKS, PASHA, and miniception. Apparently, the simple greedy
approach of preferring smaller k-mers works better than the earlier brute force
searches for small universal hitting sets. Especially for $k$ just below $w$,
its density is much better than any other scheme.

*GreedyMini.*
Unlike the previous UHS schemes, GreedyMini [cite:@greedymini-preprint] directly
constructs a low-density minimizer scheme using a brute force approach.
As we saw, the density of a minimizer scheme equals the probability that the
smallest k-mer in a $w+k$ long context is at the start or end.
The GreedyMini builds a minimizer scheme by one-by-one selecting the
next-smallest k-mer.
It starts with the set of all $w+k$ contexts, and finds the k-mer for which the
number of times it appears as the first or last k-mer in a context, as a fraction
of its total number of appearances, is lowest.
It then discards all contexts this k-mer appears in,
and repeats the process until a minimizer has been determined for all contexts.
To improve the final density, slightly
submoptimal choices are also tried occasionally, and a local search and random restarts are
used.
To keep the running time manageable, the schemes are only built for a $\sigma=2$
binary alphabet and up to $k\leq 20$. This is extended to larger $k$ using naive
extension and to larger alphabets by simply ignoring some of the input bits.

The resulting schemes achieve density very close to the lower bound, especially when $k$ is around
$w$. In these regions, the greedymini has lower density than all other schemes,
and it is able to find optimal schemes for some small cases when $k\equiv 1\pmod
w$. This raises the question whether it is also optimal for other $k$, where the
lower bound may not be tight yet.
A drawback is that this scheme is not pure: it must explicitly store the chosen
order of k-mers.

** Syncmer-based schemes
As we saw, universal hitting sets belong to a more general class of context-free
schemes that only look at individual k-mers to decide whether or not to sample
them.
A well-known category of context-free schemes are /syncmers/ [cite:@syncmers].
In general, syncmer variants consider the position of the smallest s-mer inside
a k-mer, for some $1\leq s\leq k$ and according to some order $\Os$. Here we
consider two well-known variants: /closed/ and /open/ syncmers.

#+begin_definition Closed syncmer
A k-mer is a /closed syncmer/ when the (leftmost) smallest contained s-mer, according to
some order $\Os$, is either the first s-mer at position $0$ or the last s-mer at position $k-s$.
#+end_definition

Closed syncmers satisfy a window guarantee of $k-s$, meaning that there is at
least one closed syncmer in any window of $w\geq k-s$ consecutive k-mers.
When the order $\Os$ is random, closed syncmers have a density of $2/(k-s+1)$,
which is the same as that of a random minimizer when $k>w$ and $s=k-w$. Indeed,
syncmers were designed to improve the /conservation/ metric rather than the
density. See [cite/t:@syncmers] for details.

#+begin_definition Open syncmer
A k-mer is an /open syncmer/ whe the smallest contained s-mer (according to
$\Os$) is at a specific offset $v\in [k-s+1]$. In practice, we always use $v = \floor{(k-s)/2}$.
#+end_definition
The choice of $v$ to be in the middle was shown to be optimal for conservation
by [cite/t:@local-kmer-selection]. For this $v$, open syncmers satisfy a
/distance guarantee/ (unlike closed syncmers): two consecutive open syncmers are
always at least $\floor{(k-s)/2}+1$ positions apart.

Miniception is a minimizer scheme that builds on top of closed syncmers [cite:@miniception].
The name stands for ''minimizer inception'', in that it first uses an order
$\Os$ and then an order $\Ok$.
#+begin_definition Miniception
Let $w$, $k$, and $s$ be given parameters and $\Ok$ and $\Os$ be orders.
Given a window $W$ of $w$ k-mers, /miniception/ samples the smallest closed
syncmer if there is one. Otherwise, it samples the smallest k-mer.
#+end_definition
Because of the window guarantee of closed syncmers, miniception /always/ samples
a closed syncmer when $w\geq k-s$. When $k$ is sufficiently larger than $w$ and
$s = k-w+1$,
it is shown that miniception has density bounded by $1.67/w + o(1/w)$. In
practice, we usually use $s = k-w$ when $k$ is large enough.
Unlike UHS-based schemes, miniception does not require large memory, and it is
the first such scheme that improves the $2/(w+1)$ density when $k\approx
w$.



** Open-closed minimizer
As we saw, Miniception samples the smallest k-mer that is a closed syncmer.
The open-closed minimizer is a natural extension of this
[cite:@oc-modmini-preprint]:


#+begin_newdefinition Open-closed minimizer
Given parameters $w$, $k$, and $1\leq s\leq k$, and orders $\Ok$ and $\Os$,
the open-closed minimizer samples the smallest (by $\Ok$) k-mer in a window that is an open
syncmer (by $\Os$), if there is one. Otherwise, it samples the smallest k-mer
that is a closed syncmer. If also no closed syncmer is present, the overall
smallest k-mer is sampled.
#+end_newdefinition

The rationale behind this method is that open syncmers have a distance /lower/
bound [cite:@syncmers], i.e., any two open syncmers are at least
$\floor{(k-s)/2}+1$ positions apart. This is in contrast to closed syncmers,
that do not obey a similar guarantee (but instead have an /upper/ bound on the
distance between them). As it turns out, by looking at TODO REF PLOT, the distance lower bound of open
syncmers gives rise to lower densities than the upper bound of closed syncmers.

In [cite/t:@oc-modmini-preprint], we give a polynomial algorithm to compute the
exact density of the open-closed minimizer scheme, assuming that no duplicate
k-mers occur. At a high level, this works by considering the
position of the smallest $s$-mer in the window, and then recursing on the
prefix or suffix before/after it, until an s-mer is found that is sufficiently
far from the boundaries to induce an open syncmer.

** Mod-minimizer
:PROPERTIES:
:CUSTOM_ID: modmini
:END:

So far, all the schemes we have seen in this section work well up to around
$k\approx w$, but then fail to further decrease in density as $k$ grows to
infinity.
The rot-minimizer [cite:@asymptotic-optimal-minimizers] /does/ converge to
density $1/w$, but in its original form it only does so very slowly.

Here we present the /mod-minimizer/ [cite:@modmini], which turns out to converge
to $1/w$ nearly as fast as the lower bound TODO REF.

We start with a slightly more general definition.
To avoid breaking the flow, proofs are in Section [[#modmini-proofs]].

#+begin_newdefinition Mod-sampling
Let $W$ be a window of $w+k-1$ characters, let $1\leq t\leq k$ be a parameter,
and let $\Ot$ be a total order on t-mers.
Let $x$ be the position of the smallest t-mer in the window according to $\Ot$.
Then, /mod-sampling/ samples the k-mer at position $x \bmod w$.
#+end_newdefinition

As it turns out, this scheme is only forward for some choices of $t$ [cite:Lemma 8 @modmini].

#+begin_newtheorem Forward
Mod-sampling is forward if and only if $t\equiv k\pmod w$ or $t\equiv k+1\pmod w$.
#+end_newtheorem

It turns out that mod-sampling has local minima in density when $t\equiv k\pmod
w$ [cite:Figure 4 and Lemma 12 @modmini], thus, we restrict our attention to this case only.
TODO copy fig.

Furthermore, we can show that for $t\equiv k \pmod w$, mod-sampling is not only
forward, but also a minimizer scheme [cite:Lemma 13 @modmini]:

#+begin_newtheorem Minimizer
Mod-sampling is a minimizer scheme when $t\equiv k\pmod w$.
#+end_newtheorem

TODO PROOF

This now allows us to define the mod-minimizer.

#+begin_newdefinition Random mod-minimizer
Let $r = O(\log_\sigma(w))$ be a small integer lower bound on $t$. For any $k\geq r$, choosing $t=
r+((k-r)\bmod w)$ in combination with a uniform random order $\Ot$ gives /the mod-minimizer/.
#+end_newdefinition

It turns out this definition can be extended to wrap /any/ sampling scheme,
rather than just random minimizers [cite:@oc-modmini-preprint].

#+begin_newdefinition Extended mod-minimizer
Let $w$, $k$, and $t\equiv k\pmod w$ be given parameters, and
let $f: \Sigma^{w+k-1} \to [w+k-t]$ be any sampling scheme with parameters $(w', k') = (w+k-t, t)$.
Then, given a window $W$ of length $w+k-1$, the /extended mod-minimizer/ of $f$
samples position $f(W)\bmod w$.
#+end_newdefinition

Naturally, the extended mod-minimizer can be applied to the open-closed
minimizer, to obtain the oc-mod-mini.

*Density.*

When we restrict $f$ to be a /minimizer/ scheme specifically, we can compute the
density of the extended mod-minimizer [cite:Theorem 1 @oc-modmini-preprint].

#+begin_newtheorem Extended mod-minimizer density
Let $w$, $k$, and $t\equiv k\pmod w$ be given parameters, and
let $f$ be a /minimizer/ scheme on t-mers with order $\Ot$.
Then, the density of the extended mod-minimizer is given by the probability that,
in a context of length $w+k$, the smallest t-mer is at a position $0\pmod w$.
#+end_newtheorem

Before we compute the density of the mod-minimizer, we first re-state Lemma 9 of
[cite:@modmini], which is a slightly
modified version of Lemma 9 of [cite:@miniception] that we saw earlier in
Section [[*The density of random minimizers]]. The proof is nearly
identical.

#+begin_newtheorem Duplicate k-mers
For any $\varepsilon > 0$, if $t > (3+\varepsilon) \log_\sigma(\ell)$, the
probability that a random window of $\ell-t+1$ t-mers contains two identical
t-mers is $o(1/\ell)$. Given that $\ell = w+k-1$, $o(1/\ell) \to 0$ for $k\to\infty$.
#+end_newtheorem

From the above two results, we obtain the density of the random mod-minimizer
[cite:Corollary 17 @modmini;Theorem 2 @oc-modmini-preprint].

#+begin_newtheorem Random mod-minimizer density
If $t\equiv k\pmod w$ satisfies $t > (3+\varepsilon) \log_\sigma(\ell)$ for some
$\varepsilon > 0$, the
density of the random mod-minimizer is

$$
\frac{2+\frac{k-t}{w}}{w+k-t+1} + o(1/(w+k-1)).
$$

When $w$ is fixed and $k\to\infty$, this density tends to $1/w$.
#+end_newtheorem

#+begin_proof
Given the bound on $t$, the probability that a context of $w+k$ characters contains duplicate t-mers
is $o(1/\ell) = o(1/(w+k-1))$. Otherwise, the context contains $w+k-t+1$ t-mers,
of which the ones at positions $\{0, w, 2w, \dots, w+k-t\}$ cause the context to
be charged, which is a fraction of $\frac{2+\frac{k-t}{w}}{w+k-t+1}$ of all t-mers.
#+end_proof

#+begin_newtheorem Optimality of the mod-minimizer
The random mod-minimizer has optimal density when $w$ is fixed, $r=t=1$, $k\equiv 1\pmod
w$, and $\sigma\to\infty$.
#+end_newtheorem
#+begin_proof
First note that the probability of duplicate k-mers in a window goes to $0$ as
$\sigma\to\infty$, and hence the error term in the density computed above
disappears.
Substituting variables, we get
$$
\frac{2+\lfloor\frac{k-1}{w}\rfloor}{w+\lfloor\frac{k-1}{w}\rfloor w+1}
= \frac{2+\frac{k-1}{w}}{w+\frac{k-1}{w} w+1}
= \frac{\frac{k+2w-1}w}{k+w}
= \frac{\lceil\frac{k+w}w\rceil}{k+w}.
$$
#+end_proof

- TODO: Oc-mod-mini results and commentary
  - simply & efficient
  - good density
  - 'pure', unlike greedymini

*** Proofs
:PROPERTIES:
:CUSTOM_ID: modmini-proofs
:END:
We now cover the proofs for the theorems above. These are taken from
[cite/t:@modmini] and [cite/t:@oc-modmini-preprint].

TODO: backrefs to thms.

FORWARD:

#+begin_proof
Consider two consecutive windows $W$ and $W'$.
Let $x$ be the position of the smallest t-mer in window $W$
and $x'$ that of the smallest t-mer in $W'$.
mod-sampling is forward when
$(x \bmod w) - 1 \leq (x' \bmod w)$ holds for all $x$ and $x'$.
Given that the two windows are consecutive, this trivially holds when $x=0$ and
when $x' = x-1$.
Thus, the only position $x'$ that could violate the forwardness condition is
when $W'$ introduces a new smallest t-mer at position
$x'=w+k-t-1$. In this case, we have $x' \bmod w = (w+k-t-1) \bmod w = (k-t-1) \bmod w$.
The rightmost possible position of the sampled k-mer in $W$ is $x\bmod w = w-1$.
Hence, if the scheme is forward, then it must hold that $(w-1)-1=w-2\leq(k-t-1) \bmod w$.
Vice versa, if $w-2\leq(k-t-1) \bmod w$ always
holds true, then the scheme is forward. % since $x \bmod w \leq w-1$.

Now, note that $(k-t-1) \bmod w \geq w-2
\iff qw-2 \leq k-t-1 < qw
\iff k-qw \leq t \leq k-qw+1$ for some $0 \leq q \leq \lfloor k/w \rfloor$.
In conclusion, the scheme is forward if and only if
$t=k-qw$ or $t=k-qw+1$, i.e., when $t \equiv k \pmod w$ or $t \equiv k+1 \pmod w$.
#+end_proof

MINIMIZER:

#+begin_proof
Our proof strategy explicitly defines an order $\order_k$
and shows that mod-sampling with $t \equiv k \pmod w$
corresponds to a minimizer scheme using $\order_k$, i.e.,
the k-mer sampled by mod-sampling is the leftmost smallest
k-mer according to $\order_k$.

Let $\order_t$ be the order on t-mers used by mod-sampling
Define the order $\order_k(X)$ of the k-mer $X$
as the order of its smallest t-mer, chosen among the t-mers
occurring in positions that are a multiple of $w$:
$$
\order_k(X) = \min_{i \in \{0,w, 2w,\dots, k-t\}} \order_t(X[i..i+t))
$$
where $k-t$ is indeed a multiple of $w$ since $t\equiv k\pmod w$.
Now consider a window $W$ of consecutive k-mers $X_0,\ldots,X_{w-1}$.
Since each k-mer starts at a different position in $W$,
$\order_k(X_i)$ considers different sets of positions relative to $W$ than
$\order_k(X_j)$ for all $i \neq j$.
However, t-mers starting at different positions in $W$ could be identical, i.e.,
the smallest t-mer of $X_i$ could be identical to that of $X_j$.
In case of ties,
$\order_k$ considers the k-mer containing the leftmost occurrence
of the t-mer to be smaller.

Suppose the leftmost smallest t-mer is at position $x \in [w+k-t]$.
Then mod-sampling samples the k-mer $X_p$ at position $p = x \bmod w$.
We want to show that $X_p$ is the leftmost smallest k-mer according to $\order_k$.
If $\order_t(W[x..x+t))=o$, then % also $\order_k(X_i)=o$.
\begin{align*}
\order_k(X_p) = \order_k(W[p..p+k)) &= \min_{j\in \{0,w, 2w,\dots,
k-t\}} \order_t(W[p+j..p+j+t)) \\
&= \min_{j\in \{x-p\}} \order_t(W[p+j..p+j+t))
= o.
\end{align*}
Since $o$ is minimal, any other k-mer $X_j$ must have order $\geq o$.
Also, since $o$ is the order of the leftmost occurrence of the smallest t-mer,
$X_p$ is the leftmost smallest k-mer according to $\order_k$.
#+end_proof

EXTENDED MOD-MINI DENSITY

#+begin_proof
Consider two consecutive windows $W$ and $W'$ of length $w+k-1$ of a
uniform random string.
Let $x$ and $x'$ be the position of the smallest t-mer in $W$ and $W'$
respectively, and let $p=x\bmod w$ and $p'=x'\bmod w$ be the positions of the
sampled k-mers
Let $y\in\{x, x'+1\}$ be the absolute position of the smallest t-mer in the
two windows.

Since $A$ is a forward scheme, we can compute its density as the probability
that a different k-mer is sampled from $W$ and $W'$.
First note that the two consecutive windows contain a total of $w+k-t+1$ t-mers,
and thus, $0\leq y\leq w+k-t$, where $w+k-t$ is divisible by $w$ since
$t\equiv k\pmod w$.

When $y\not\equiv 0\pmod w$, this implies $0<y<w+k-t$, and thus, the two
windows share their smallest t-mer. Thus, $p=x\bmod w = y\bmod w$ and
$p'+1=x'\bmod w+1=(y-1)\bmod w+1$. Since $y\not\equiv 0\pmod w$, this gives
$p'+1=y\bmod w$, and thus, the two windows sample the same k-mer.

When $y\equiv 0\pmod w$, there are two cases.
When $y=x$ (and thus $y<w+k-t$), we have $p=x\bmod w=y\bmod w=0$, and since the k-mer starting at
position $0$ is not part of $W'$, the second window must necessarily sample a
new k-mer.
Otherwise, we must have $y=(x'+1)\equiv 0\pmod w$, which implies $p'=x'\bmod w=(y-1)\bmod w=w-1$, and since the k-mer starting at
position $w-1$ in $W'$ is not part of $W$, again the second window must necessarily sample a
new k-mer.

To conclude, the two windows sample distinct k-mer if and only if the smallest t-mer
occurs in a position $y\equiv 0\pmod w$.
#+end_proof

* TODO Selection schemes
This section is complete TODO. I should probably prove some results on SUS-anchors before
this is worth including.
** Bd-anchors
*Bidirectional anchors* (bd-anchors) are a variant on minimizers that take the minimal
lexicographic /rotation/ instead of the minimal k-mer substring [cite:@bdanchors-esa;@bdanchors;@anchors-are-all-you-need].
I wrote above them before in [[file:../bd-anchors/bd-anchors.org::*Paper overview][this post]].

*Reduced bd-anchors* restrict this rotation to not start in the 'unstable' last
$r=4\log_\sigma(\ell)$ positions.

*Density:* Reduced bd-anchors have a density of $2/(\ell+1-r)$ for large
alphabet, and somewhat larger for small $\sigma$.

Bd-anchors have a slightly different purpose than minimizers, in that they are keyed by their
position in the text, rather than by the corresponding string itself. Thus, a
suffix array is built on suffixes and reverse-prefixes starting/ending there.

For random strings, reduced bd-anchors are a dense subset of the $k=r+1$ minimizers.

Given the bd-anchors, two suffix arrays are built. One of suffixes starting at
anchors, and one on reverse prefixes ending at anchors.

*Note:* bd-anchors are not a so-called /forward/ scheme. That is, it is possible
for the window to shift right, but the selected position to jump backwards.
[[file:../bd-anchors/bd-anchors.org::*Paper overview][Example here]].

*Optimization:*
When querying an $\ell$-mer, in practice only the longer of the
prefix and suffix is actually looked up in the corresponding suffix array. Thus,
we don't need to two suffix arrays over /all/ bd-anchors:
- The forward SA over suffixes only needs to contains bd-anchors occurring in
  the left half of some $\ell$-mer.
- The reverse SA over suffixes only needs to contains bd-anchors occurring in
  the right half of some $\ell$-mer.
This makes things slightly sparser.
** Maximal non-overlapping string sets

- Papers on maximal non-overlapping string sets (see below).


- [cite/t:@max-non-overlapping-codes]
  - Shows a bound on max number of non-overlapping words of
    $$\frac 1k \left(\frac{k-1}{k}\right)^{k-1} \sigma^k$$
- [cite/t:@non-overlapping-codes]
  - divide alphabet into two parts. Then patterns =abbbb= and e.g. =aab?b?b?b=
    are non-overlapping. (=b=: any non-=a= character)
  - For DNA, optimal solution (max number of pairwise non-overlapping words) for $k=2$ is =[AG][CT]=, while for
    $k\in\{3,4,5,6\}$, an optimal solution is given by =A[CTG]+=.
  - Re-prove upper bound on number of non-overlapping words $\sigma^k/(2k-1)$.
  - Re-prove upper bound of Levenshtein above.
  - Show existing scheme with size
    $$\frac{\sigma-1}{e\sigma} \frac{\sigma^k}{k}$$
  - New scheme: not $0$ and ${>}0$, but arbitrary partition. And prefix is in
    some set $S$, while suffix is $S$-free.
    - When $k$ divides $\sigma$, choose $|I| = \sigma/k$ and $|J| =
      \sigma-\sigma/k$, and consider strings =IIIIIIJ=. These are optimal.
    - The set $S$ is needed to avoid rounding errors when $\sigma$ is small.
    - Conjecture: a suffix of =JJ= or longer is never optimal.
- [cite/t:@minimally-overlapping-words]
  - /minimally overlapping words/ are anti-clustered, hence good for sensitivity.
  - =cg=-order: alternate small and large characters, as [cite:@minimizers]
  - =abb=-order: compare first character normal, the rest by ~t=g=c<a~.
- [cite/t:@searching-max-non-overlapping-codes]
  - ILP to solve the problem for more $(k, \sigma)$ pairs.
- [cite/t:@optimal-sampling-frith]
  - Test various word-sets for their sparsity and specificity.
- [cite/t:@unavoidable-sets]

** NEW: SUS-anchors

TODO: Show density of $(2+o(1))/w$ as $w\to\infty$?

*** Commentary
- Ideally, prove density $O(1/w)$, and answer the question of [cite:@small-uhs]
  that yes, perfect selection schemes exist. Then also update the asymptotic table.

* Evaluation and discussion

** Open questions
- How much are local schemes better than forward schemes?
- How much are forward schemes better than minimizer schemes? Only for small $k$?
- How close to optimal is greedy minimizer?


# * OLD



# *** Overview

# #+caption: An overview of the papers this post discusses, showing authors and categories of each paper.
# #+attr_html: :class inset large
# [[file:papers.svg]]

# ** Theory of sampling schemes

# - [cite/t:@minhash]
#   - Take the $s$ kmers with smallest $s$ hashes, then estimate jaccard
#     similarity based on this.
# - [cite/t:@winnowing]
#   - $k$: /noise threshold/
#   - $\ell$: /guarantee threshold/
#   - /winnowing/: Definition 1: Select minimum hash in each window.
  # - Charged contexts to prove a $2/(w+1)$ density, assuming no duplicate hashes
    # (and $k$-mers)
  # - /local algorithm/: Function on k-mer hashes, rather than on window itself:
    # $S(h_i, \dots, h_{i+w-1})$.
  # - Local algorithms have density at least $(1.5+1/2w)/(w+1)$.
  # - Conjecture that $2/(w+1)$ is optimal.
  # - Robust Winnowing: smarter tie-breaking: same as previous window in case of
    # tie if possible, otherwise rightmost.
  # - 'threshold' $t=w+k-1$
  # - order via hash
# - [cite/t:@minimizers]
  # - /interior minimizers/: Length $w+k-1$ in common, then share minimizer
  # - Same heuristic argument for $2/(w+1)$ density, assuming distinct kmers.
  # - $w\leq k$ guarantees no gaps (uncovered characters) between minimizers
  # - /end minimizers/: minimizers of a prefix/suffix of the string of length $<\ell$.
  # - lexicographic ordering is bad on consecutive zeros.
  # - 'Alternating' order: even positions have reversed order.
  # - Increase chance of 'rare' k-mers being minimizers.
  # - Reverse complement-stable minimizers: $ord(kmer) = min(kmer, rev-kmer)$.
  # - Some heuristic argument that sensitivity goes as $k+w/2$.
  # - $k<\log_\sigma(N)$ may have bad sensitivity.
# - [cite/t:@improved-minimizers]
  # - Main goal is to disprove the $2/(w+1)$ conjectured lower bound.
  # - States that [cite/t:@winnowing] defines a /local scheme/ as only having
    # access to the sequence within a window, but actually, it only has access to
    # the hashes.
  # - UHS to obtain ordering with lower density than lex or random.
  # - DOCKS goes below $1.8/(w+1)$, so the conjecture doesn't hold.
  # - Random order has density slightly below $2/(w+1)$.
  # - Defines /density factor/ $d_f = d\cdot(w+1)$.[fn::I am not a fan of this,
  #   since the lower bound is $1/w$, no scheme can actually achieve density
  #   factor $1$. Calibrating the scale to the (somewhat arbirary) random
  #   minimizer, instead of to the theoretical lower bound does not really make
  #   sense to me.]
  # - UHS /sparsity/ $SP(U)$: the fraction of contexts containing exactly one k-mer from
  #   the $U$.
  #   - $d = 2/(w+1) \cdot (1-SP(U))$
  # - The density of a minimizer scheme can be computed on a De Bruin sequence of
  #   order $k+w$.
  # - The density of a local scheme can be less than $2/(w+1)$.
  # - Does not refute the $(1.5+1/2w)/(w+1)$ lower bound.
# - [cite/t:@asymptotic-optimal-minimizers]
  # - Properly introduces $local \supseteq forward\supseteq minimizers$.
  # - Realizes that $(1.5+1/2w)/(w+1)$ lower bound is only for /randomized local schemes/.
  # - Studies asymptotic behaviour in $k$ and $w$
  # - For $k\to\infty$, a minimizer scheme with density $1/w$.
  # - For $w\to\infty$, a $1/\sigma^k$ lower bound on minimizer schemes.
  #   - Forward schemes can achieve density $O(1/\sqrt w)$ instead, by using $k' = \log_\sigma(\sqrt{w})$ instead.
  # - A lower bound on forward schemes of $\frac{1.5 + 1/2w + \max(0, \lfloor(k-w)/w\rfloor)}{w+k}$.
  #   - Proof looks at two consecutive windows and the fact that half the time,
  #     the sampled kmers leave a gap of $w$ in between, requiring an additional
  #     sampled kmer.

  # - Local schemes can be strictly better than forward, found using ILP.
  # - New lower bound on forward schemes.
  # - For local schemes, a De Bruijn sequence of order $2w+k-2$ can be used to
  #   compute density.
  # - UHS-minimizer compatibility.
  # - Naive extension for UHS: going from $k$ to $k+1$ by ignoring extra characters.
  # - Construction of asymptotic in $k\to\infty$ scheme is complex, but comes down
  #   to roughly: for each $i\in [w]$, sum the characters in positions $i\pmod w$.
  #   Take the k-mer the position $i$ for which the sum is maximal. (In the paper
  #   it's slightly different, in that a context-free version is defined where a
  #   k-mer is 'good' if the sum of it's $0\pmod w$ characters is larger than the
  #   sums for the other equivalence classes, and then there is an argument that
  #   good kmers close to a UHS, and turning them into a real UHS only requires
  #   'few' extra kmers.)
  # - $d(k, w)$ is decreasing in $w$.

# - [cite/t:@syncmers]
#   - Introduces open syncmers, closed syncmers
#   - /context free/: each kmer is independently selected or not
#   - Conservation: probability that a sampled kmer is preserved under mutations.
#   - context-free sampled kmers are better conserved.
# - [cite/t:@local-kmer-selection]
#   - Formalizes /conservation/: the fraction of bases covered by sampled kmers.
#   - k-mer /selection method/: samples any kind of subset of kmers
#   - $q$-local /selection method/: $f$ looks at a $k+q-1$-mer, and returns some
#     /subset/ of kmers.
#   - /word-based method/: a 'context free' method where for each k-mer it is
#     decided independently whether it is sampled or not.
# - [cite/t:@minimizer-biased]
#   - The jaccard similarity based on random minimizers is biased.
# - [cite/t:@random-mini-density]
  # - The random minimizer has density just below $2/(w+1)$ when $k>w$ and $w$ is
  #   sufficiently large.
  # - $O(w^2)$ method to compute the /exact/ density of random minimizer.
  # - The $2/j$ and $1/j$ fractions were observed before in [cite:@improved-minimizers]
# - [cite/t:@sampling-lower-bound]
  # - Lower bound on density of $\frac1{w+k}\lceil\frac{w+k}w\rceil$.
  # - Tighter version by counting pure cycles of all lengths.
  # - Instead of $k$, can also use the bound for $k'\geq k$ with $k\equiv 1\pmod w$.
# - [cite/t:@small-uhs]
  # - UHS-minimizer compatibility; remaining path length $L \leq \ell$
  # - $d \leq |U|/\sigma^k$.
  # - Mentions decycling set of [cite/t:@mykkeltveit]
  # - Theorem 2: Forward sampling scheme with density $O(\ln(w) / w)$ (where $k$ is
  #   small/constant), and a corresponding UHS.
  # - /selection scheme/: selects /positions/ rather than /kmers/, i.e., $k=1$.
  # - Assumes $w\to\infty$, so anyway $k=O(1)$ or $k=1$ are kinda equivalent.
  # - Theorem 1: local scheme implies $(2w-1)$-UHS, forward scheme implies $(w+1)$-UHS.
  # - Theorem 3: Gives an upper and lower bound on the remaining path length of the
  #   Mykkeltveit set: it's between $c_1\cdot w^2$ and $c_2\cdot w^3$.
  # - Local schemes: $w-1$ 'looking back' context for $2w+k-2$ total context size.
  #   - The charged contexts are a UHS.
  # - $O(\ln(w)/w)$ forward scheme construction:
  #   - Definition 2 / Lemma 2: The set of words that either start with $0^d$ or do not contain $0^d$ at
  #     all is a UHS. Set $d = \log_\sigma(w /\ln w)-1$. This has longest
  #     remaining path length $w-d$.
  #   - Then a long proof that the relative size is $O(\ln(w) / w)$.
  #   - (In hindsight: this is a variant of picking the smallest substring, as
  #     long as it is sufficiently small.)
  # - Questions:
  #   - We can go from a scheme $f$ to a UHS. Can we also go back?
  #   - Does a perfect selection scheme exist?
# - [cite/t:@miniception]
  # - For $w\to\infty$, minimizer schemes can be optimal (have density $O(1/w)$) if and only if $k
  #   \geq \log_\sigma(w) - O(1)$. In fact, the lexicographic minimizer is optimal.
  # - When $k\geq (3+\varepsilon)\log_\sigma(w)$, the random minimizer has
  #   expected density $2/(w+1)+o(1/w)$, fixing the proof by [cite:@winnowing].
  # - When $\varepsilon>0$ and $k>(3+\varepsilon)\log_\sigma w$, the probability
  #   of duplicate k-mers in a window is $o(1/w)$.
  #   - TODO: Hypothesis: the $3$ could also be a $2$, or actually even a $1$?
  # - turn charged contexts of a minimizer scheme into a $(w+k)$-UHS. (skipped)
  # - Relative size of UHS is upper bound on density of compatible minimizer.
# - [cite:@debruijngraph-representation]
  # - Order k-mers by their frequency in the dataset.

# *** Questions
# *Main question:* What is the lowest possible density for given $(k, w)$?

# The first questions:
# - What is a scheme

# type:
# - sampling scheme: sample k-mer
# - selection scheme: sample position ($k=1$)

# This question is then approached from two sides:
# - Lower bounds on density for $(k,w,\sigma)$?
# - Tight lower bounds for /some/ parameters?
# - Tight lower bounds, asymptotic in parameters (e.g., $\sigma\to\infty$)?
# - Can we make tight lower bounds for all practical parameters?
# - If not, can we understand why the best schemes found (using ILP) do not reach
#   know bounds?

# And:
# - What is the empirical density of existing schemes?
# - Can we model existing schemes and compute their density exactly?
# - Can we make near-optimal schemes (say, within $1\%$ from optimal) for
#   practical parameters?
# - Can we make exactly optimal schemes, for asymptotic parameters?
# - Can we make optimal schemes for practical parameters?
# - Can we make 'pure' optimal schemes, that do not require exponential memory?
# - If we can not make pure optimal schemes, can we bruteforce search for them instead?
# *** Types of schemes
# scope:
# - global (frac-sampling, mod-sampling
   # sampling every $n$-th kmer)
# - local
# - forward
# - minimizer

# *** Parameter regimes
# - small $k$: $k < \log_\sigma(w)$
# - large $k$: $k\gg w$ or $k\to \infty$.
# - 'practical': $4\leq k \leq 2w$ with $w\leq 20$ or so; depends on the application.
# - binary/DNA alphabet $\sigma\in\{2,4\}$.
# - large/infinite alphabet, $\sigma=256$ or $\sigma\to\infty$.

# *** Different perspectives
# - charged contexts of length $w+1$.
# - pure cycles of length $w+k$.
# - long random strings.


# *** UHS vs minimizer scheme
# - UHS is a minimizer scheme where everything has hash/order $0$ or $1$.
# *** (Asymptotic) bounds
# *** Lower bounds

# ** Minimizer schemes
# *** Orders
# *** UHS-based and search-based schemes
# - [cite/t:@docks-wabi;@docks]
  # - Introduces UHS
  # - DOCKS finds a UHS
  # - Finding optimal UHS is hard when a set of strings to be hit is given. (But
    # here we have a DBg, which may be easier.)
  # - The size of a UHS may be much smaller than the set of all possible minimizers.
  # - DOCKS UHS density is close to optimal (?)
  # - Step 1: Start with the Mykkeltveit embedding
  # - Step 2: repeatedly find a vertex with maximal 'hitting number' of
    # $\ell$-long paths going through it, and add it to the UHS (and remove it
    # from the graph.)
  # - DOCKSany: compute number of paths of /any/ length, instead of length $\ell$.
  # - DOCKSanyX: remove the top $X$ vertices at a time.
  # - Applies 'naive extension' to work for larger $k$.
  # - Runs for (many) hours to compute UHS for $k=11$ already.
  # - An ILP to improve UHSes found by DOCKS; improves by only a few percent at best.
  # - DOCKS selects far fewer distinct kmers compared to random minimizers, and
    # has slightly lower density.
  # - Does **not** use a compatible minimizer order.
# - [cite/t:@practical-uhs]
#   - Extends UHS generated by DOCKS
#   - larger $k$ up to $200$, but $L\leq 21$.
#   - Merges UHS with random minimizer tiebreaking.
#   - Mentions sparsity
#   - Starts with UHS for small $k$ and grows one-by-one to larger $k$. Full
#     process is called =reMuval=.
#     - First, naive extension
#     - Second, an ILP to reduce the size of the new UHS and
#       increase the number of /singletons/: windows containing exactly one kmer.
#       (Since density directly correlates with sparsity.)
#   - Naive extension can decrease density
#   - Remove kmers from the UHS that always co-occur with another k-mer in every window.
#   - ILP is on whether each kmer is retained in the UHS or not, such that every
#     window preserves at least one element of the UHS.
#   - Also does sequence-specific minimizers
# - [cite/t:@pasha]
#   - Improves DOCKS using randomized parallel algorithm for set-cover.
#   - Faster computation of hitting numbers.
#   - Scales to $k\leq 16$.
# - [cite/t:@deepminimizer]
#   - Learns a total order, instead of a UHS.
#   - Continuous objective, rather than discrete.
#   - UHSes are 'underspecified' since the order withing each component is not
#     given. Determining the permutation directly is more powerful.
#   - Around $5\%$ better than PASHA.
# - [cite/t:@greedymini-preprint]
#   - Unlike UHS-based methods that optimize UHS size, this directly optimizes
#     minimizer density by minimizing the number of charged context:
#     - Repeatedly pick the next kmer as smallest that is in the smallest fraction
#       of charged contexts.
#     - Then do some noise (slightly submoptimal choices), and local search with
#       random restarts on top.
#   - Builds scheme for alphabet size $\sigma'=2$ and $k'\leq 20$ which is extended to $\sigma=2$
#     and to larger $k$ if $k>20$.
#   - Achieves very low density. Open question how close to optimal.
#   - Not 'pure': requires the memory to store the order of kmers.
# - [cite/t:@polar-set-minimizers]
#   - Polar set intersects each $w$-mer /at most/ once.
#   - Two kmers in a polar set are at least $(w+1)/2$ apart.
#   - Lemma 4: Formula for probability that a window is charged, in terms of
#     number of unique kmers.
#   - Progressively add 'layers' to the polar set to fill gaps.
#   - Heuristic: greedily try to pick kmers that are exactly $w$ apart, by
#     choosing a random offset $o\in [w]$, and adding all those kmers as long as
#     they aren't too close to already chosen kmers.
#     - Up to 7 rounds in practice.
#   - Filter too frequent kmers.
#   - Significantly improved density over other methods.
#   - Requires explicitly storing an order.

# *** Pure schemes
# - [cite/t:@miniception]
#   - Considers all closed syncmers in a window. Picks the smallest one.
#   - Parameter $k_0$ (we call it $s$): the length of the hashed 'inner' slices.
#   - For $k > w + O(\log_\sigma(w))$, has density below $1.67/w + o(1/w)$.
#     - This requires a long proof.
#   - First scheme with guaranteed density $<2/(w+1)$ when $k\approx w$ (instead
#     $k\gg w$).
#   - Does not require expensive heuristics for precomputation; no internal storage.
#   - Charged contexts or a $(w_0, k_0)$ minimizer are the UHS of the $(w,
#     k=w_0+k_0)$ minimizer, as long as $w\geq w_0$.
# - [cite/t:@minimum-decycling-set]
  # - MDS: a set of k-mers that hits every cycle in the DBg.
  # - Mykkeltveit embedding: map each k-mer to a complex number. Take those k-mers
    # with argument (angle) between $0$ and $2\pi/k$ as context-free hitting set.
  # - Take a compatible minimizer.
  # - Even better: prefer argument in $[0, 2\pi/k)$, and otherwise prefer argument
    # $[\pi, \pi+2\pi/k)$.
  # - Great density for $k$ just below $w$.
  # - MDS orders outperform DOCKS and PASHA.
  # - Scales to larger $k$
# - [cite/t:@modmini]
#   - For $k > w$, look at $t=k\bmod w$-mers instead. If the smallest $t$-mer is
#     at position $x$, sample the $k$-mer at position $x\bmod w$.
  # - Asymptotic optimal density as $w\to\infty$.
  # - Close to optimal for large alphabet when $k\equiv 1\pmod w$.
# - [cite/t:@oc-modmini-preprint]
  # - Extend miniception to open syncmers, and open followed by closed syncmers.
  # - Extend modmini to wrap any other sampling scheme.
  # - Simple and very efficient scheme, for any $k$.
  # - Greedymini has lower density, but is more complex.

# *** Other variants
# - [cite/t:@minmers]
#   - Sample the smallest $s$ k-mers from each $s\cdot w$ consecutive k-mers.
# - [cite/t:@fracminhash]
  # - Sample all kmers with hash below $max\cdot f$.
# - [cite:@debruijngraph-representation]
#   - Frequency aware minimizers TODO
# - [cite/t:@finimizers]
#   - /frequency bounded minimizers/, with frequency below $t$
#   - Prefers rare kmers as minimizers
#   - variable length scheme.
#   - /Shortest unique finimizers/
#   - Uses SBWT to work around 'non-local' property.
#   - Useful for SSHash-like indices.
#   - Defines DSPSS: Disjoint spectrum preserving string set.
#   - For each kmer, find the shortest contained substring that occurs at most $t$
#     times in the DBg of the input.
#   - (TODO: I'm getting a bit lost on the technicalities with the SBWT.)

# **** Selection schemes
# These have $k=1$
# - [cite/t:@bdanchors-esa;@bdanchors]
#   - In each window, sample the position that starts the lexicographically
#     smallest rotation.
#   - Avoid sampling the last $r\approx \log_\sigma(w)$ positions, as they cause
#     'unstable' anchors.
# **** Canonical minimizers
# - [cite/t:@refined-minimizer]
#   - Choose the strandedness via higher CG-content.
# - [cite/t:@encoding-canonical-kmers]
#   - TODO
# - [cite/t:@knonical-reverse-complements]
#   - TODO


* Checks
- select -> samples
- Marcais -> proper spelling
- symbol vs character
- Fix $k$-mer and $t$-mer and $s$-mer and plurals

#+print_bibliography:
