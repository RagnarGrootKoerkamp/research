#+title: [WIP] Minimizers
#+filetags: @thesis minimizers wip
#+HUGO_LEVEL_OFFSET: 0
#+OPTIONS: ^:{} num:2 H:4
#+hugo_front_matter_key_replace: author>authors
#+toc: headlines 3
#+hugo_paired_shortcodes: %notice
#+date: <2024-11-05 Tue>

$$
\newcommand{\O}{\mathcal O}
\newcommand{\Ok}{\mathcal O_k}
\newcommand{\S}{\Sigma}
\newcommand{\P}{\mathbb P}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\poly}{poly}
\DeclareMathOperator*{\rc}{rc}
\newcommand{\ceil}[1]{\left\lceil{#1}\right\rceil}
\newcommand{\floor}[1]{\left\lfloor{#1}\right\rfloor}
$$

#+attr_shortcode: summary
#+begin_notice
- New lower bound
- New 'pure' schemes
#+end_notice

#+attr_shortcode: attribution
#+begin_notice
- lower bound [cite:@sampling-lower-bound], with Bryce
- mod-mini [cite:@modmini] with Giulio, especially the background section
- open-closed mod-mini [cite:@oc-modmini-preprint]
- SIMD-minimizers [cite:@simd-minimizers-preprint], for canonical minimizers
- some new/unpublished minimizer schemes

Also, the following blog posts
- bd anchors: [[../bd-anchors/bd-anchors.org]]
- mod-mini: [[../mod-minimizers/mod-minimizers.org]]
- lower bound: [[../minimizer-lower-bound/minimizer-lower-bound.org]]
- simd minimizers: [[../fast-minimizers/fast-minimizers.org]]
- practical minimizers: [[../practical-minimizers/practical-minimizers.org]]
#+end_notice


*Motivation.*
- k-mers
- compression
- faster analysis
- locally consistent
- Jaccard similarity

*Brief definition.*
- w, k, density

*Problem statement.*
- Mention random minimizer
- low density

*Overview.*
We split content into three topics
- General theory
- Lower bounds
- Selection

* Theory of sampling schemes
-TODO: Cite winnowing and minimizers papers.


Throughout this chapter, we use the following notation.
For $n\in \mathbb N$, we write $[n]:=\{0, \dots, n-1\}$.
The alphabet is $\S = [\sigma]$ and has size $\sigma =2^{O(1)}$, so that each character can
be represented with a constant number of bits.
Given a string $S\in \S^*$, we write $S[i..j)$ for the sub-string starting at
the $i$'th character, up to (and not including) the $j$'th character, where both
$i$ and $j$ are $0$-based indices.
A k-mer is any (sub)string of length $k$.

In the context of minimizer schemes, we have a /window guarantee/ $w$ indicating
that at least one every $w$ k-mers must be sampled.
A /window/ is a string containing exactly $w$ k-mers, and hence consists of
$\ell:=w+k-1$ characters.
We will later also use /contexts/, which are sequences containing two windows
and thus of length $w+k$.

** Types of sampling schemes
TODO:
- Briefly introduce what a sampling scheme does.
- Mention global scheme, and why we care about local
- Mention minimizers vs winnowing vs sampling schemes terminology.
- Define context, cite [cite:@small-uhs] more.
- Define charged


#+begin_definition Local sampling scheme
For $w\geq 1$ and $k\geq 0$, a /local scheme/ is a function $f: \S^\ell \to [w]$.
Given a window $W$, it /samples/ the k-mer $W[f(W)..f(W)+k)$.
#+end_definition

In practice, we usually require $w\geq 2$ and $k\geq 1$, as some theorems break
down at either $w=1$ or $k=0$.

#+begin_definition Forward sampling scheme
A local scheme is /forward/ when for any /context/ $C$ of length $\ell+1$
containing windows $W=C[0..\ell)$ and $W'=C[1..\ell+1)$, it holds that $f(W) \leq f(W')+1$.
#+end_definition

Forward scheme have the property that as the window $W$ slides through an input
string $S$, the position in $S$ of the sampled k-mer never decreases.

#+begin_definition Order
An order $\Ok$ on k-mers is a function $\Ok : \S^k \to \mathbb R$, such
that for $x,y\in \S^k$, $x\leq _{\Ok} y$ if and only if $\Ok(x) \leq \Ok(y)$.
#+end_definition

#+begin_definition Minimizer scheme
A /minimizer scheme/ is defined by a total order $\Ok$ on k-mer and samples the
leftmost minimal k-mer in a window $W$, which is called the /minimizer/:

$$
f(W) := \argmin_{i\in [w]} \Ok(W[i..i+k)).
$$
#+end_definition

#+begin_definition Random minimizer
The /random minimizer/ is the minimizer scheme with a uniform random total
order $\Ok$.
#+end_definition

#+begin_definition Particular density
Given a string $S$ of length $n$, let $W_i := S[i..i+\ell)$ for $i\in [n-\ell+1]$.
A sampling scheme $f$ then samples the k-mers starting at positions $M:=\{i+f(W_i)
\mid i\in [n-\ell+1]\}$. The /particular density/ of $f$ on $S$ is the fraction
of sampled k-mers: $|M|/(n-k+1)$.
#+end_definition

#+begin_definition Density
The /density/ of a sampling $f$ is defined as the expected particular density on
a string $S$ consisting of i.i.d. random characters of $\Sigma$ in the limit
where $n\to\infty$.
#+end_definition

As we will see, the density of the random minimizer is $2/(w+1) + o(1/w)$.

#+begin_definition Pure sampling scheme
A sampling scheme is /pure/ when it can be implemented in $O(\poly(w+k))$ time
and space.
#+end_definition

** Variants

There are several variations on sampling schemes that generalize in different
ways.

On strings with many repeated characters, all k-mers have the same hash, and
hence all k-mers are sampled. /Robust winnowing/ [cite:@winnowing] prevents
this by sampling the rightmost minimal k-mer by default, unless the minimizer of
the previous window has the same hash, in which case that one is ''reused''.

/Min-mers/ [cite:@minmers] are a second variant, where instead of choosing a
single k-mer from a window, $s$ k-mers are chosen instead, typically from a
window that is $s$ times longer.

For DNA, it is often not know to which strand a give sequence belongs.
Thus, any analysis should be invariant under taking the reverse complement.
In this case, /canonical minimizers/ can be used.
#+begin_definition Canonical sampling scheme
A sampling scheme $f$ is /canonical/ when for all windows $W$ and their reverse
complement $\rc(W)$, it holds that

$$f(\rc(W)) = w-1-f(W).$$
#+end_definition

TODO: [cite:@local-kmer-selection]


*Universal hitting sets.*
A /Universal hitting set/ (UHS) $U_{k,w}$ is an ''unavoidable'' set of k-mers, so
that every
window of $w$ k-mers contains at least on k-mer in the universal hitting set (UHS)
[cite:@docks-wabi;@docks;@pasha;@small-uhs;@practical-uhs;@improved-minimizers].
This then implies a /context-free/ scheme: a k-mer is sampled if and only if it
is part of the UHS.

A minimizer scheme $f$ is /compatible/ with a UHS $U_{k,w}$ when every k-mer in the UHS is
smaller than every k-mer not in the UHS. In this case, the density of $f$
satisfies $d(f) \leq |U|/\sigma^k$ [cite:@asymptotic-optimal-minimizers].
Thus, the goal is to construct small universal hitting sets,
which typically have a lower density of sampled k-mers. Both DOCKS
[cite:@docks-wabi;@docks] and PASHA [cite:@pasha] construct a small UHS greedily
adding the ''best'' k-mer to it, according to some heuristic.


** Computing density
The density of a sampling scheme is defined as the expected particular density
on an infinitely long string. In practice, we can approximate it closely by
simply computing the particular density on a sufficiently long random string of,
for example, 10 million characters.

When $\sigma^{w+k}$, the following theorem forms the basis for computing the density of
schemes exactly [cite:Lemma 4 @improved-minimizers]:

#+begin_theorem Computing density (context)
The density of a forward scheme equals the probability that,
in a uniform random /context/ of length $c=w+k$, two different k-mers are sampled
from the two windows.

Thus, the density can be computed exactly by iterating over all $\sigma^{w+k}$ contexts.
#+end_theorem

We can also approximate the density by sampling sufficiently many random
contexts.
A somewhat more efficient method is to use a De Bruijn sequence instead.
A De Bruijn sequence of order $c$ is any circular sequence of length
$\sigma^c$ that contains every sequence of length $c$ exactly once [cite:@debruijnseq].
We have the following theorem, again by [cite:Lamma 4 @improved-minimizers]:

#+begin_theorem Computing density (De Bruijn sequence)
The density of any forward scheme equals its particular density on an order
$c=w+k$ De Bruijn sequence.
For /local/ schemes, the order $c=2w+k-2$ De Bruijn graph must be used instead.
#+end_theorem

Another approach, that follows from the first, is by considering cycles of
length $c$, rather than just strings of length $c$.
#+begin_newtheorem Computing density (cycles)
The density of any forward scheme equals its average particular density over all
cyclic strings of order $c=w+k$ for forward schemes and $c=2w+k-2$ for local schemes.
#+end_newtheorem

** Random minimizer
As a warm-up, we will analyse the density of the random minimizer.


- $2/(w+1)$
- optimality conjecture and counter example
- Density just below $2/(w+1)$ for large k [cite:@random-mini-density]
- Small $k < \log_\sigma n$ is bad

** Lower bound for minimizers
[cite:theorem 2 @asymptotic-optimal-minimizers]:
#+begin_theorem Large w
For any /minimizer/ scheme $f$, the density is at least $1/\sigma^k$, and
converges to this as $w\to\infty$.
#+end_theorem

This implies that as $w\to\infty$, fixed-$k$ minimizer schemes can never be
optimal.
It does not hold for forward and local schemes though.


* Lower bounds
The starting point is the following trivial lower bound.
#+begin_theorem Trivial lower bound
For any local, forward, or minimizer scheme $f$, the density is at least $1/w$.
#+end_theorem

** Schleimer et al.'s bound
The first improvement over the trivial lower bound was already given in the
paper that first introduced minimizers [cite:theorem 1 @winnowing ]:

#+begin_theorem Lower bound when hashing k-mers
Consider a $w$-tuple of uniform random independent hashes of the k-mers in a tuple.
Now let $S$ be any function that selects a k-mer based on these $w$ hashes.
Then, $S$ has density at least

$$
d(S) \geq \frac{1.5 + \frac{1}{2w}}{w+1}.
$$
#+end_theorem

#+begin_proof_sketch
Let $W_i$ and $W_{i+w+1}$ be the windows of $w$ k-mers starting at positions $i$
and $i+w+1$ in a long uniform random string $S$.
Since $W_i$ and $W_{i+w+1}$ do not share any k-mers, the hashes of the k-mers in
$W_i$ are independent of the hashes of the k-mers in $W_{i+w+1}$.
Now, we can look at the probability distributions $X$ and $X'$ of the sampled
position in the two windows. Since the hashes are independent, these
distributions are simply the same, $X \sim X'$.
There are $(i+w+1+X') - (i+X) - 1 = w+(X'-X)$ ''skipped'' k-mers between the two
sampled k-mers. When $X\leq X'$, this is $\geq w$, which means that at least one
additional k-mer must be sampled in this gap. It is easy to see that $\P[X\leq
X'] \geq 1/2$, and using Cauchy-Schwartz this can be improved to $\P[X\leq X']\geq
1/2 + 1/(2w)$. Thus, out of the $w+1$ k-mers starting at positions $i$ to $i+w$
(inclusive), we sample at least $1 + 1/2 + 1/(2w)$ in expectation, giving the result.
#+end_proof_sketch

Unfortunately, this lower-bound assumes that k-mers are hashed before processing
them further using a potentially ''smart'' algorithm $S$. This class of schemes
was introduced as /local algorithms/, and thus caused some confusion (see e.g. [cite:@improved-minimizers]) in that it
was also believed to be a lower bound on the more general /local schemes/ as we
defined them. This inconsistency was first noticed in
[cite:@asymptotic-optimal-minimizers], which introduces a ''fixed'' version of
the theorem.


** Marcais et al.'s bound
In [cite/t:@asymptotic-optimal-minimizers], the authors give a weaker variant of
the theorem of [cite:@winnowing] that does hold for all forward schemes:
#+begin_theorem Lower bound for forward schemes
Any forward scheme $f$ has density at least

$$
d(f) \geq \frac{1.5 + \max\left(0, \left\lfloor\frac{k-w}{w}\right\rfloor\right) +
\frac 1{2w}}{w+k}.
$$
#+end_theorem
#+begin_proof_sketch
The proof is very comparable to the one of [cite/t:@winnowing].
Again, we consider two windows in a long uniform random string $S$.
This time, however, we put them $w+k+1$ positions
apart, instead of just $w+1$. This way, the windows do not share any characters, rather
than not sharing any k-mers, and thus, the probability distributions $X$ and $X'$
of the position of the k-mers sampled from $W_i$ and $W_{i+w+k+1}$ are
independent again.

They again consider the positions $s_1=i+X$ and $s_2=i+w+k+1+X'$, and lower bound
the expected number of sampled k-mers in this range.
The length of the range is $w+k$, leading to the denominator, and the
$1.5+1/(2w)$ term arises as before. The additional $\left\lfloor
\frac{k-w}{w}\right\rfloor$ term arises from the fact that when $k$ is large,
just sampling one additional k-mer in between $s_1$ and $s_2$ is not sufficient
to ensure a sample every $w$ positions.
#+end_proof_sketch

** NEW: Improved version
It turns out that the theorem TODO REF is slightly inefficient. In
[cite/t:@modmini], we improve it.

#+begin_newtheorem Improved lower bound
The density of any /local/ scheme $f$ satisfies

$$
d(f) \geq \frac{1.5}{w+k-0.5}.
$$
#+end_newtheorem
#+begin_proof
TODO: Copy over full proof?
#+end_proof
#+begin_proof_sketch
Again, we highlight here the differences compared to the previous proof.
The full proof is replicated in Appendix TODO.

First, the $+\left\lfloor\frac{k-w}{w}\right\rfloor$ term only contributes
anything when $k\geq w$. It turns out that for $k> (w+1)/2$, the lower bound is
provably less than the trivial bound of $1/w$. Thus, we may as well drop this term.

Second, we can slightly improve the analysis of $\P[X\leq X']$.
Instead of considering a single interval of two consecutive windows $w+k$ apart,
we can instead consider /three/ disjoint windows at positions $i$, $i+w+k-1$, and
$i+2w+2k-1$. Let $X$, $X'$, and $X''$ be the positions of the sampled k-mers.
Then we sample at least the k-mers at positions $s_1=i+X$ and $s_2=i+w+k-1+X'$.
When $X<X'$, the number of bases between $s_1$ and $s_2$ is at least $s_2-s_1-1
= w+k-2+(X'-X) \geq w+k-1$. Thus, an additional k-mer must be sampled from this
window with probability $\P[X<X']$. Similarly, an additional k-mer must be
sampled between $s_2$ and $s_3=i+2w+2k-1+X''$ with probability $\P[X'\leq X'']$. Since $X\sim X' \sim
X''$ and since the three distributions are fully independent, we have $\P[X'\leq
X''] = \P[X'\leq X] = 1 - \P[X < X']$. Thus, in expectation we need to sample at least one
additional k-mer. We then get a lower bound of

$$
\frac{1 + \P[X < X'] + 1 + \P[X'\leq X'']}{2w+2k-1} = \frac{3}{2w+2k-1} = \frac{1.5}{w+k-0.5}.
$$

Lastly, we note that this lower bound does not use the fact that $f$ is forward,
and thus, it holds for local schemes as well.
#+end_proof_sketch

In TODO PLOT we can see that this new version indeed provides a small
improvement over the previous lower bound. Nevertheless, a big gap remains
between the lower bound and, say, the density of the random minimizer.

It is also clear that this proof is far from tight. It uses that an additional
k-mer must be sampled when a full window of $w+k-1$ characters fits between $s_1$ and $s_2$, while in
practice an additional k-mer is already needed when the distance between them is
larger than $w$. However, exploiting this turns out to be difficult: we
can not assume that the sampled positions in overlapping windows are
independent, nor is it easy to analyse a probability such as $\P[X \leq X''-k]$.

** NEW: A near-tight lower bound
In [cite/t:@sampling-lower-bound], we prove a nearly tight lower bound on the
density of /forward/ schemes.
Here, we first present a slightly simplified version. The full version can be
found in (TODO REF).

#+begin_newtheorem Near-tight lower bound (simple)
Any forward scheme $f$ has a density at least

$$
d(f) \geq \frac{\left\lceil\frac{w+k}{w}\right\rceil}{w+k}.
$$
#+end_newtheorem
#+begin_proof
The density of a forward scheme can be computed as
the probability that two consecutive windows in a random length $w+k$ context
sample different k-mers [cite:Lemma 4 @improved-minimizers].  Form this, it follows that we can also
consider /cyclic strings/ (cycles) of length $w+k$, and compute the expected
number of sampled k-mers along the cycle. The density is then this count divided
by $w+k$.

Because of the window guarantee, at least one out of every $w$ k-mers along the
length $w+k$ cycle must be sampled. Thus, at least $\lceil (w+k)/w\rceil$ k-mers
must be sampled in each cycle. After dividing by the number of k-mers in the
cycle, we get the result.
#+end_proof

The full and more precise version is as follows [cite:Theorem 1 @sampling-lower-bound].

#+begin_theorem Near-tight lower bound (improved)
Let $M_\sigma(p)$ count the number of aperiodic necklaces of length $p$ over an
alphabet of size $\sigma$. Then, the density of any forward sampling scheme $f$ is
at least

$$
d(f) \geq g_\sigma(w,k) :=  \frac{1}{\sigma^{w+k}} \sum_{p | (w+k)} M_\sigma(p) \left\lceil \frac
pw\right\rceil \geq \frac{\left\lceil\frac{w+k}{w}\right\rceil}{w+k} \geq \frac 1w,
$$

where the middle inequality is strict when $w>1$.
#+end_theorem
#+begin_proof_sketch
The core of this result is to refine the proof given above.
While indeed we know that each cycle will have at least $\ceil{(w+k)/w}$
sampled k-mers, that lower bound may not be tight. For example, if the cycle
consists of only zeros, each window samples position $i + f(000\dots 000)$, so that
in the end every position is sampled.

We say that a cycle has /period/ $p$ when it consists of $(w+k)/p$
copies of some pattern $P$ of length $p$, and $p$ is the maximum number for which this holds.
In this case, we can consider the cyclic string of $P$, on which we must sample
at least $\ceil{p/w}$ k-mers. Thus, at least $\frac{w+k}{p}\ceil{\frac pw}$
k-mers are sampled in total, corresponding to a particular density of at least $\frac{1}{p}\ceil{\frac pw}$.

Since $p$ is maximal, the pattern $P$ itself must be /aperiodic/. When
$M_\sigma(p)$ counts the number of aperiodic cyclic strings of length $p$,
the probability that a uniform random cycle has period $p$ is $p\cdot M_\sigma(p) /
\sigma^{w+k}$, where the multiplication by $p$ accounts for the fact that each pattern
$P$ gives rise to $p$ equivalent cycles that are simply rotations of each other.
Thus, the overall density is simply the sum over all $p\mid (w+k)$:

$$
d(f)
\geq \sum_{p | (w+k)} \frac{p\cdot M_\sigma(p)}{\sigma^{w+k}}\cdot \frac{1}{p} \left\lceil \frac pw\right\rceil
=\frac 1{\sigma^{w+k}} \sum_{p | (w+k)} M_\sigma(p)  \left\lceil \frac pw\right\rceil.
$$

The remaining inequalities follow by simple arithmetic.
#+end_proof_sketch

TODO: Describe ILP and plot

As can be seen in TODO PLOT, this lower bound jumps up at values $1 \pmod w$.
In practice, if some density $d$ can be achieved for parameters $(w,k)$, it can
also be achieved for any larger $k'\geq k$, by simply ignoring the last $k'-k$
characters of each window. Thus, we can ''smoothen'' the plot via the following
corollary.

#+begin_theorem Near-tight lower bound (smooth)
Any forward scheme $f$ has density at least

$$
d(f)
\geq g'_\sigma(w,k) := \max\big(g_\sigma(w,k), g_\sigma(w,k')\big)
\geq \max\left(\frac 1{w+k}\ceil{\frac{w+k}w}, \frac1{w+k'}\ceil{\frac{w+k'}w}\right),
$$

where $k'$ is the smallest integer $\geq k$ such that $k' \equiv 1 \pmod w$.
#+end_theorem

*Local schemes.* The lower bounds above can also be extended to local
schemes by replacing $c=w+k$ by $c=2w+k-2$. Sadly, this does not lead to a good
bound. In practice, the best local schemes appear to be only marginally better than
the best forward schemes, while the currently established theory requires us to
increase the context size significantly, thereby making all inequalities
much more loose. Specifically, the tightness of the bound is mostly due to the
rounding up in
$\frac{1}{c}\ceil{\frac{c}{k}}=\frac{1}{w+k}\ceil{\frac{w+k}{k}}$, and the more
we increase $c$, the smaller the effect of the rounding will be.

*TODO: Contextualization. Where are we now?*

* Sampling schemes
** Previous work
- Character orders:
  - alternating
- Miniception
- (Double) decycling
- PASHA, DOCKS
- GreedyMini
- Open/closed syncmers
- Asympotically optimal UHS
** NEW: Mod-minimizer
** NEW: Open-closed minimizer
** NEW: SUS-anchor and variants
- Papers on maximal non-overlapping string sets (see below).

* Summary

* Proofs


* OLD

** Introduction

- Lots of DNA data
- Most algorithms deal with k-mers.
- k-mers overlap, and hence considering all of them is redundant.
- Thus: sample a subset of the kmers.
- Must be 'locally consistent' and deterministic to be useful.
- Enter random minimizers.
- Parameter $w$: guarantee that at least one k-mer is sampled out of every
  window of $w$ k-mers.
- Density $d$: (expected) overall fraction of sampled k-mers.
- Obviously, $d\geq 1/w$
- For random mini, $d=2/(w+1)$.
- Lower density => fewer k-mers, smaller indices, faster algorithms.
- *Question:* How small density can we get for given $k$ and $w$?

*** Previous reviews
- [cite/t:@minimizer-sketches]
- [cite/t:@minimizer-review-2]


*** Overview

#+caption: An overview of the papers this post discusses, showing authors and categories of each paper.
#+attr_html: :class inset large
[[file:papers.svg]]

** Theory of sampling schemes
#+begin_quote
[At RECOMB 2022, discussing DeepMinimizer]

Why would we even care about better minimizer? We have this simple and fast
random minimizer that's only at most $2\times$ away from optimal. Why would
anyone invest time in optimizing this by maybe $25\%$?
There are so much bigger gains possible elsewhere.
#+end_quote

- [cite/t:@minhash]
  - Take the $s$ kmers with smallest $s$ hashes, then estimate jaccard
    similarity based on this.
- [cite/t:@winnowing]
  - $k$: /noise threshold/
  - $\ell$: /guarantee threshold/
  - /winnowing/: Definition 1: Select minimum hash in each window.
  - Charged contexts to prove a $2/(w+1)$ density, assuming no duplicate hashes
    (and $k$-mers)
  - /local algorithm/: Function on k-mer hashes, rather than on window itself:
    $S(h_i, \dots, h_{i+w-1})$.
  - Local algorithms have density at least $(1.5+1/2w)/(w+1)$.
  - Conjecture that $2/(w+1)$ is optimal.
  - Robust Winnowing: smarter tie-breaking: same as previous window in case of
    tie if possible, otherwise rightmost.
  - 'threshold' $t=w+k-1$
  - order via hash
- [cite/t:@minimizers]
  - /interior minimizers/: Length $w+k-1$ in common, then share minimizer
  - Same heuristic argument for $2/(w+1)$ density, assuming distinct kmers.
  - $w\leq k$ guarantees no gaps (uncovered characters) between minimizers
  - /end minimizers/: minimizers of a prefix/suffix of the string of length $<\ell$.
  - lexicographic ordering is bad on consecutive zeros.
  - 'Alternating' order: even positions have reversed order.
  - Increase chance of 'rare' k-mers being minimizers.
  - Reverse complement-stable minimizers: $ord(kmer) = min(kmer, rev-kmer)$.
  - Some heuristic argument that sensitivity goes as $k+w/2$.
  - $k<\log_\sigma(N)$ may have bad sensitivity.
- [cite/t:@improved-minimizers]
  - Main goal is to disprove the $2/(w+1)$ conjectured lower bound.
  - States that [cite/t:@winnowing] defines a /local scheme/ as only having
    access to the sequence within a window, but actually, it only has access to
    the hashes.
  - UHS to obtain ordering with lower density than lex or random.
  - DOCKS goes below $1.8/(w+1)$, so the conjecture doesn't hold.
  - Random order has density slightly below $2/(w+1)$.
  - Defines /density factor/ $d_f = d\cdot(w+1)$.[fn::I am not a fan of this,
    since the lower bound is $1/w$, no scheme can actually achieve density
    factor $1$. Calibrating the scale to the (somewhat arbirary) random
    minimizer, instead of to the theoretical lower bound does not really make
    sense to me.]
  - UHS /sparsity/ $SP(U)$: the fraction of contexts containing exactly one k-mer from
    the $U$.
    - $d = 2/(w+1) \cdot (1-SP(U))$
  - The density of a minimizer scheme can be computed on a De Bruin sequence of
    order $k+w$.
  - The density of a local scheme can be less than $2/(w+1)$.
  - Does not refute the $(1.5+1/2w)/(w+1)$ lower bound.
- [cite/t:@asymptotic-optimal-minimizers]
  - Properly introduces $local \supseteq forward\supseteq minimizers$.
  - Realizes that $(1.5+1/2w)/(w+1)$ lower bound is only for /randomized local schemes/.
  - Studies asymptotic behaviour in $k$ and $w$
  - For $k\to\infty$, a minimizer scheme with density $1/w$.
  - For $w\to\infty$, a $1/\sigma^k$ lower bound on minimizer schemes.
    - Forward schemes can achieve density $O(1/\sqrt w)$ instead, by using $k' = \log_\sigma(\sqrt{w})$ instead.
  - A lower bound on forward schemes of $\frac{1.5 + 1/2w + \max(0, \lfloor(k-w)/w\rfloor)}{w+k}$.
    - Proof looks at two consecutive windows and the fact that half the time,
      the sampled kmers leave a gap of $w$ in between, requiring an additional
      sampled kmer.

  - Local schemes can be strictly better than forward, found using ILP.
  - New lower bound on forward schemes.
  - For local schemes, a De Bruijn sequence of order $2w+k-2$ can be used to
    compute density.
  - UHS-minimizer compatibility.
  - Naive extension for UHS: going from $k$ to $k+1$ by ignoring extra characters.
  - Construction of asymptotic in $k\to\infty$ scheme is complex, but comes down
    to roughly: for each $i\in [w]$, sum the characters in positions $i\pmod w$.
    Take the k-mer the position $i$ for which the sum is maximal. (In the paper
    it's slightly different, in that a context-free version is defined where a
    k-mer is 'good' if the sum of it's $0\pmod w$ characters is larger than the
    sums for the other equivalence classes, and then there is an argument that
    good kmers close to a UHS, and turning them into a real UHS only requires
    'few' extra kmers.)
  - $d(k, w)$ is decreasing in $w$.

- [cite/t:@syncmers]
  - Introduces open syncmers, closed syncmers
  - /context free/: each kmer is independently selected or not
  - Conservation: probability that a sampled kmer is preserved under mutations.
  - context-free sampled kmers are better conserved.
- [cite/t:@local-kmer-selection]
  - Formalizes /conservation/: the fraction of bases covered by sampled kmers.
  - k-mer /selection method/: samples any kind of subset of kmers
  - $q$-local /selection method/: $f$ looks at a $k+q-1$-mer, and returns some
    /subset/ of kmers.
  - /word-based method/: a 'context free' method where for each k-mer it is
    decided independently whether it is sampled or not.
- [cite/t:@minimizer-biased]
  - The jaccard similarity based on random minimizers is biased.
- [cite/t:@max-non-overlapping-codes]
  - Shows a bound on max number of non-overlapping words of
    $$\frac 1k \left(\frac{k-1}{k}\right)^{k-1} \sigma^k$$
- [cite/t:@non-overlapping-codes]
  - divide alphabet into two parts. Then patterns =abbbb= and e.g. =aab?b?b?b=
    are non-overlapping. (=b=: any non-=a= character)
  - For DNA, optimal solution (max number of pairwise non-overlapping words) for $k=2$ is =[AG][CT]=, while for
    $k\in\{3,4,5,6\}$, an optimal solution is given by =A[CTG]+=.
  - Re-prove upper bound on number of non-overlapping words $\sigma^k/(2k-1)$.
  - Re-prove upper bound of Levenshtein above.
  - Show existing scheme with size
    $$\frac{\sigma-1}{e\sigma} \frac{\sigma^k}{k}$$
  - New scheme: not $0$ and ${>}0$, but arbitrary partition. And prefix is in
    some set $S$, while suffix is $S$-free.
    - When $k$ divides $\sigma$, choose $|I| = \sigma/k$ and $|J| =
      \sigma-\sigma/k$, and consider strings =IIIIIIJ=. These are optimal.
    - The set $S$ is needed to avoid rounding errors when $\sigma$ is small.
    - Conjecture: a suffix of =JJ= or longer is never optimal.
- [cite/t:@minimally-overlapping-words]
  - /minimally overlapping words/ are anti-clustered, hence good for sensitivity.
  - =cg=-order: alternate small and large characters, as [cite:@minimizers]
  - =abb=-order: compare first character normal, the rest by ~t=g=c<a~.
- [cite/t:@searching-max-non-overlapping-codes]
  - ILP to solve the problem for more $(k, \sigma)$ pairs.
- [cite/t:@optimal-sampling-frith]
  - Test various word-sets for their sparsity and specificity.
- [cite/t:@random-mini-density]
  - The random minimizer has density just below $2/(w+1)$ when $k>w$ and $w$ is
    sufficiently large.
  - $O(w^2)$ method to compute the /exact/ density of random minimizer.
  - The $2/j$ and $1/j$ fractions were observed before in [cite:@improved-minimizers]
- [cite/t:@sampling-lower-bound]
  - Lower bound on density of $\frac1{w+k}\lceil\frac{w+k}w\rceil$.
  - Tighter version by counting pure cycles of all lengths.
  - Instead of $k$, can also use the bound for $k'\geq k$ with $k\equiv 1\pmod w$.
- [cite/t:@small-uhs]
  - UHS-minimizer compatibility; remaining path length $L \leq \ell$
  - $d \leq |U|/\sigma^k$.
  - Mentions decycling set of [cite/t:@mykkeltveit]
  - Theorem 2: Forward sampling scheme with density $O(\ln(w) / w)$ (where $k$ is
    small/constant), and a corresponding UHS.
  - /selection scheme/: selects /positions/ rather than /kmers/, i.e., $k=1$.
  - Assumes $w\to\infty$, so anyway $k=O(1)$ or $k=1$ are kinda equivalent.
  - Theorem 1: local scheme implies $(2w-1)$-UHS, forward scheme implies $(w+1)$-UHS.
  - Theorem 3: Gives an upper and lower bound on the remaining path length of the
    Mykkeltveit set: it's between $c_1\cdot w^2$ and $c_2\cdot w^3$.
  - Local schemes: $w-1$ 'looking back' context for $2w+k-2$ total context size.
    - The charged contexts are a UHS.
  - $O(\ln(w)/w)$ forward scheme construction:
    - Definition 2 / Lemma 2: The set of words that either start with $0^d$ or do not contain $0^d$ at
      all is a UHS. Set $d = \log_\sigma(w /\ln w)-1$. This has longest
      remaining path length $w-d$.
    - Then a long proof that the relative size is $O(\ln(w) / w)$.
    - (In hindsight: this is a variant of picking the smallest substring, as
      long as it is sufficiently small.)
  - Questions:
    - We can go from a scheme $f$ to a UHS. Can we also go back?
    - Does a perfect selection scheme exist?
- [cite/t:@miniception]
  - For $w\to\infty$, minimizer schemes can be optimal (have density $O(1/w)$) if and only if $k
    \geq \log_\sigma(w) - O(1)$. In fact, the lexicographic minimizer is optimal.
  - When $k\geq (3+\varepsilon)\log_\sigma(w)$, the random minimizer has
    expected density $2/(w+1)+o(1/w)$, fixing the proof by [cite:@winnowing].
  - When $\varepsilon>0$ and $k>(3+\varepsilon)\log_\sigma w$, the probability
    of duplicate k-mers in a window is $o(1/w)$.
    - TODO: Hypothesis: the $3$ could also be a $2$, or actually even a $1$?
  - turn charged contexts of a minimizer scheme into a $(w+k)$-UHS.
  - Relative size of UHS is upper bound on density of compatible minimizer.

- [cite:@debruijngraph-representation]
  - Order k-mers by their frequency in the dataset.

*** Questions
*Main question:* What is the lowest possible density for given $(k, w)$?

The first questions:
- What is a scheme

This question is then approached from two sides:
- Lower bounds on density for $(k,w,\sigma)$?
- Tight lower bounds for /some/ parameters?
- Tight lower bounds, asymptotic in parameters (e.g., $\sigma\to\infty$)?
- Can we make tight lower bounds for all practical parameters?
- If not, can we understand why the best schemes found (using ILP) do not reach
  know bounds?

And:
- What is the empirical density of existing schemes?
- Can we model existing schemes and compute their density exactly?
- Can we make near-optimal schemes (say, within $1\%$ from optimal) for
  practical parameters?
- Can we make exactly optimal schemes, for asymptotic parameters?
- Can we make optimal schemes for practical parameters?
- Can we make 'pure' optimal schemes, that do not require exponential memory?
- If we can not make pure optimal schemes, can we bruteforce search for them instead?
*** Types of schemes
scope:
- global (frac-sampling, mod-sampling [cite:@debruijngraph-representation;@compacting-dbg]
  (TODO, TODO),
  minhash, sampling every $n$-th kmer)
- local
- forward
- minimizer

type:
- sampling scheme: sample k-mer
- selection scheme: sample position ($k=1$)

*** Parameter regimes
- small $k$: $k < \log_\sigma(w)$
- large $k$: $k\gg w$ or $k\to \infty$.
- 'practical': $4\leq k \leq 2w$ with $w\leq 20$ or so; depends on the application.
- binary/DNA alphabet $\sigma\in\{2,4\}$.
- large/infinite alphabet, $\sigma=256$ or $\sigma\to\infty$.

*** Different perspectives
- charged contexts of length $w+1$.
- pure cycles of length $w+k$.
- long random strings.


*** UHS vs minimizer scheme
- UHS is a minimizer scheme where everything has hash/order $0$ or $1$.
*** (Asymptotic) bounds
*** Lower bounds

** Minimizer schemes
*** Orders
*** UHS-based and search-based schemes
- [cite/t:@docks-wabi;@docks]
  - Introduces UHS
  - DOCKS finds a UHS
  - Finding optimal UHS is hard when a set of strings to be hit is given. (But
    here we have a DBg, which may be easier.)
  - The size of a UHS may be much smaller than the set of all possible minimizers.
  - DOCKS UHS density is close to optimal (?)
  - Step 1: Start with the Mykkeltveit embedding
  - Step 2: repeatedly find a vertex with maximal 'hitting number' of
    $\ell$-long paths going through it, and add it to the UHS (and remove it
    from the graph.)
  - DOCKSany: compute number of paths of /any/ length, instead of length $\ell$.
  - DOCKSanyX: remove the top $X$ vertices at a time.
  - Applies 'naive extension' to work for larger $k$.
  - Runs for (many) hours to compute UHS for $k=11$ already.
  - An ILP to improve UHSes found by DOCKS; improves by only a few percent at best.
  - DOCKS selects far fewer distinct kmers compared to random minimizers, and
    has slightly lower density.
  - Does **not** use a compatible minimizer order.
- [cite/t:@practical-uhs]
  - Extends UHS generated by DOCKS
  - larger $k$ up to $200$, but $L\leq 21$.
  - Merges UHS with random minimizer tiebreaking.
  - Mentions sparsity
  - Starts with UHS for small $k$ and grows one-by-one to larger $k$. Full
    process is called =reMuval=.
    - First, naive extension
    - Second, an ILP to reduce the size of the new UHS and
      increase the number of /singletons/: windows containing exactly one kmer.
      (Since density directly correlates with sparsity.)
  - Naive extension can decrease density
  - Remove kmers from the UHS that always co-occur with another k-mer in every window.
  - ILP is on whether each kmer is retained in the UHS or not, such that every
    window preserves at least one element of the UHS.
  - Also does sequence-specific minimizers
- [cite/t:@pasha]
  - Improves DOCKS using randomized parallel algorithm for set-cover.
  - Faster computation of hitting numbers.
  - Scales to $k\leq 16$.
- [cite/t:@deepminimizer]
  - Learns a total order, instead of a UHS.
  - Continuous objective, rather than discrete.
  - UHSes are 'underspecified' since the order withing each component is not
    given. Determining the permutation directly is more powerful.
  - Around $5\%$ better than PASHA.
- [cite/t:@greedymini-preprint]
  - Unlike UHS-based methods that optimize UHS size, this directly optimizes
    minimizer density by minimizing the number of charged context:
    - Repeatedly pick the next kmer as smallest that is in the smallest fraction
      of charged contexts.
    - Then do some noise (slightly submoptimal choices), and local search with
      random restarts on top.
  - Builds scheme for alphabet size $\sigma'=2$ and $k'\leq 20$ which is extended to $\sigma=2$
    and to larger $k$ if $k>20$.
  - Achieves very low density. Open question how close to optimal.
  - Not 'pure': requires the memory to store the order of kmers.
- [cite/t:@polar-set-minimizers]
  - Polar set intersects each $w$-mer /at most/ once.
  - Two kmers in a polar set are at least $(w+1)/2$ apart.
  - Lemma 4: Formula for probability that a window is charged, in terms of
    number of unique kmers.
  - Progressively add 'layers' to the polar set to fill gaps.
  - Heuristic: greedily try to pick kmers that are exactly $w$ apart, by
    choosing a random offset $o\in [w]$, and adding all those kmers as long as
    they aren't too close to already chosen kmers.
    - Up to 7 rounds in practice.
  - Filter too frequent kmers.
  - Significantly improved density over other methods.
  - Requires explicitly storing an order.
*** Pure schemes
- [cite/t:@miniception]
  - Considers all closed syncmers in a window. Picks the smallest one.
  - Parameter $k_0$ (we call it $s$): the length of the hashed 'inner' slices.
  - For $k > w + O(\log_\sigma(w))$, has density below $1.67/w + o(1/w)$.
    - This requires a long proof.
  - First scheme with guaranteed density $<2/(w+1)$ when $k\approx w$ (instead
    $k\gg w$).
  - Does not require expensive heuristics for precomputation; no internal storage.
  - Charged contexts or a $(w_0, k_0)$ minimizer are the UHS of the $(w,
    k=w_0+k_0)$ minimizer, as long as $w\geq w_0$.
- [cite/t:@minimum-decycling-set]
  - MDS: a set of k-mers that hits every cycle in the DBg.
  - Mykkeltveit embedding: map each k-mer to a complex number. Take those k-mers
    with argument (angle) between $0$ and $2\pi/k$ as context-free hitting set.
  - Take a compatible minimizer.
  - Even better: prefer argument in $[0, 2\pi/k)$, and otherwise prefer argument
    $[\pi, \pi+2\pi/k)$.
  - Great density for $k$ just below $w$.
  - MDS orders outperform DOCKS and PASHA.
  - Scales to larger $k$
- [cite/t:@modmini]
  - For $k > w$, look at $t=k\bmod w$-mers instead. If the smallest $t$-mer is
    at position $x$, sample the $k$-mer at position $x\bmod w$.
  - Asymptotic optimal density as $w\to\infty$.
  - Close to optimal for large alphabet when $k\equiv 1\pmod w$.
- [cite/t:@oc-modmini-preprint]
  - Extend miniception to open syncmers, and open followed by closed syncmers.
  - Extend modmini to wrap any other sampling scheme.
  - Simple and very efficient scheme, for any $k$.
  - Greedymini has lower density, but is more complex.

*** Other variants
- [cite/t:@minmers]
  - Sample the smallest $s$ k-mers from each $s\cdot w$ consecutive k-mers.
- [cite/t:@fracminhash]
  - Sample all kmers with hash below $max\cdot f$.
- [cite:@debruijngraph-representation]
  - Frequency aware minimizers TODO
- [cite/t:@finimizers]
  - /frequency bounded minimizers/, with frequency below $t$
  - Prefers rare kmers as minimizers
  - variable length scheme.
  - /Shortest unique finimizers/
  - Uses SBWT to work around 'non-local' property.
  - Useful for SSHash-like indices.
  - Defines DSPSS: Disjoint spectrum preserving string set.
  - For each kmer, find the shortest contained substring that occurs at most $t$
    times in the DBg of the input.
  - (TODO: I'm getting a bit lost on the technicalities with the SBWT.)

**** Selection schemes
These have $k=1$
- [cite/t:@bdanchors-esa;@bdanchors]
  - In each window, sample the position that starts the lexicographically
    smallest rotation.
  - Avoid sampling the last $r\approx \log_\sigma(w)$ positions, as they cause
    'unstable' anchors.
**** Canonical minimizers
- [cite/t:@refined-minimizer]
  - Choose the strandedness via higher CG-content.
- [cite/t:@encoding-canonical-kmers]
  - TODO
- [cite/t:@knonical-reverse-complements]
  - TODO

** Open questions
- How much are local schemes better than forward schemes?
- How much are forward schemes better than minimizer schemes? Only for small $k$?
- How close to optimal is greedy minimizer?

** Checks
- select -> samples
- Marcais -> proper spelling

#+print_bibliography:
