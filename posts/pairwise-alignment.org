#+TITLE: A history of exact pairwise alignment
#+HUGO_BASE_DIR: ..
#+HUGO_CATEGORIES: posts methods
#+HUGO_TAGS: pairwise-alignment
#+HUGO_LEVEL_OFFSET: 2
# NOTE: Run citar-export-local-bib-file to generate local-bib.bib.
# +BIBLIOGRAPHY: /home/philae/git/eth/references/references.bib
#+BIBLIOGRAPHY: local-bib.bib
#+cite_export: csl
#+date: <2022-03-24 Thu>

This post lists some of the more relevant papers and implementations related to
pairwise alignment.
Unless mentioned otherwise, all these methods are *exact* and do *global
alignment*.

First, lets quickly go over the existing variants of the pairwise alignment problem.

* Variants of pairwise alignment

Pairwise alignment is the problem of finding the lowest cost way to transform a
string $A$ of length $n$ into a string $B$ of length $m$ using some set of
operations, each with their own cost.

** Cost models
See the [[https://github.com/smarco/WFA2-lib][WFA2 readme]] for more detail.
Note that some models minimize the cost, while others include a score for
matching characters and maximize.
- Indel / LCS [[[https://en.wikipedia.org/wiki/Longest_common_subsequence_problem][wikipedia]]] :: No substitutions allowed. Count minimum number of indels.
- Edit / Levenshtein distance [[[https://en.wikipedia.org/wiki/Levenshtein_distance][wikipedia]]] :: Minimum number of indels and/or substitutions needed. All
  cost $1$.
- Gap-linear :: Arbitrary costs $X$ for mismatch and $I$ per indel character, so
  cost $l\cdot I$ for a gap of length $l$.
  More generally, there can be a function $\delta(a,b) > 0$ ($a\neq b$) for the cost
  of a mismatch between characters $a$ and $b$, and a function $w(l)$ for the
  cost of a gap of length $l$.
- Gap-affine :: Gap costs are linear with an offset. Instead of cost $I$ per
  indel character, there are costs $O$ (/open/) and $E$ (/extend/), and the cost
  for a gap of length $l$ is $O + l\cdot E$.
- Dual-cost gap-affine :: Introduce two gap scores based on $(O_1, E_1)$ and
  $(O_2, E_2)$. The cost of a gap of length $l$ is $\min(O_1 + l\cdot E_1, O_2 +
  l\cdot E_2)$.

** Alignment types
- Global :: Align both sequences fully, end-to-end
- Ends-free :: Like global alignment, but indels/gaps at the end are free. This needs a
  way to ensure the aligned sequences overlap, because making them disjoint has
  cost $0$. Either introduce a score for matching characters (which not all
  aligners can handle), or set a maximum length of the free prefix (as WFA2 does).
  - Glocal, semi-global, mapping :: Map a read to a substring of a reference.
  - Extension :: Anchor both at their starts, but one is longer.
  - Overlap :: Mapping two partially overlapping reads against each other.
- Local :: Aligns a substring of $A$ to a substring of $B$.

** Parameters for the runtime
These variables are used for asymptotic complexity and memory usage.
- $n$, $m$: sequence lengths. Nowadays $n \geq m$, but historically $m\geq n$.
- $d$: edit distance.
- $s$: alignment cost (or occasionally edit distance), given some cost model.
- $r$: the number of pairs of matching characters between the two sequences.

* Semi-chronological overview

** Classic theoretical results

Once upon a time, there were cubic algorithms for various cost models.
These cost models were soon simplified to allow quadratic runtimes instead.

- [cite/text/cf:@nw] (NW) [[[https://en.wikipedia.org/wiki/Needleman%E2%80%93Wunsch_algorithm][wikipedia]]] :: Introduces the original cubic $O(m^2n)$
  ($m\geq n$) DP algorithm for
  global alignment.
- [cite/text/cf:@sankoff] :: Improves NW to have $O(mn)$ runtime. This is
  what is nowadays usually referred to by the term /Needleman-Wunsch algorithm/.

  These first two papers use match and mismatch costs, but not gap costs. Nevertheless,
  both methods are easily extended to work with gap costs.
- [cite/text/cv:@wagner74] [[[https://en.wikipedia.org/wiki/Wagner%E2%80%93Fischer_algorithm#cite_note-navarro-1][wikipedia]]] :: Also presents a quadratic algorithm. This
  algorithm has been discovered multiple times in parallel, see [cite/text:@navarro01].
- [cite/text/cf:@sellers] :: Introduces the now common scheme of mismatch and
  indel penalties: $\delta(a_i, b_j)$, $\delta(a_i, -)$, and $\delta(-, b_j)$,
  and re-states the $O(nm)$ algorithm for this case.
- [cite/text/cf:@waterman] :: Extends the cost model to allow an arbitrary cost
  function for longer insertions and deletions, giving a cubic algorithm.
- [cite/text/cf:@sw] [[[https://en.wikipedia.org/wiki/Smith%E2%80%93Waterman_algorithm#cite_note-Smith1981-1][wikipedia]]] :: Extends the NW DP to local alignment, allowing
  for arbitrary gap penalties, and runs in cubic time $O(m^2n)$ for $m\geq n$.
- [cite/text/cf:@smith81] :: Mentions /gap-affine/ penalties, where the cost of a gap of length $k$ is $w_k = uk+v$.
- [cite/text/cf:@gotoh] ::
  Shows that the cubic algorithms of [cite/text:@waterman] and [cite/text:@sw] simplify to
  quadratic $O(mn)$ algorithms for gap-affine costs. It uses three matrices $D$,
  $P$, and $Q$, where $P$ and $Q$ correspond to the minimal alignment cost when
  ending with a deletion or insertion respectively.
  While it generalizes [cite/text:@sw], it does not state the recurrence for
  local alignment.

  This seems to be the first paper to remark that linear memory is sufficient when
  only the distance is required.
- [cite/text/cf:@altschul] :: Fixes a bug in the backtracking algorithm of [cite/text:@gotoh].
- Smith-Waterman-Gotoh (SWG) ::
  This term is now occasionally used (e.g. in [cite/text:@wfa]) to refer to the gap-affine global alignment
  algorithm introduces by [cite/text:@gotoh]. This is somewhat confusing since [cite/text:@sw] is only about /local/ alignment.

  Introduces new names $C$, $D$ (end with a deletion), and $I$
  (end with an insertion) for the recursion by [cite/text:@gotoh].

At this point, the search for algorithms faster than $n^2$ started.
First there were improvements to LCS, leading to an $O(n(m-p))$ algorithm when
the LCS has length $p$.

- [cite/text/cf:@hunt77] [[[https://en.wikipedia.org/wiki/Hunt%E2%80%93Szymanski_algorithm][wikipedia]]] :: An $O((r+n) \lg n)$ algorithm for LCS, for $r$ ordered pairs
  of positions where the two sequences match, using an array of /threshold
  values/ $T_{i,k}$: the smallest $j$ such that the prefixes of length $i$ and
  $j$ have an LCS of length $k$. Faster than quadratic for large alphabets (e.g.
  lines of code).
- [cite/text/cf:@hirschberg77] :: Defines /$k$-candidates/ (already introduced in Hirschberg's
  thesis two years before) as matches where a LCS of length $k$ ends. /Minimal/
  (also called /essential/ elsewhere) $k$-candidates are those for which there
  are no other /smaller/ $k$-candidates.  This leads to /contours/: the border
  between regions of equal $L$-value. Their algorithm is $O(p (m-p) \lg n)$.
- [cite/text/cf:@nakatsu82] :: Presents the first $O(n(m-p))$ algorithm for
  LCS, by introducing a recursion on threshold values.

At this point, the search for an $O(nd)$ algorithm for pairwise alignment was
the next logical step.  This resulted in three similar but slightly different
algorithms being published almost in parallel, using what we now call the
*diagonal transition* method.

- Ukkonen [cite/text/cf:@ukkonen83 conference;@ukkonen85 paper] ::
  Introduces the diagonal transition method for edit costs, using $O(s\cdot
  \min(m,n))$ time and $O(s^2)$ space, and if only the score is needed, $O(s)$
  space.

  Concepts introduced:
  * $d_{ij}$ is non-decreasing on diagonals, and has bounded increments.
  * *Furthest reaching point*: Instead of storing $d$, we can store increments
    only: $f_{kp}$ is the largest $i$ s.t. $d_{ij}=p$ on diagonal $k$ ($j-i=k$).
    [TODO: they only generalize it from LCS elsewhere]
  * A recursion on $f_{kp}$ for unit costs, computing /wavefront/ $f_{\bullet,p}$ from
    the previous front $f_{\bullet, p-1}$, by first taking a maximum over
    insert/deletion/substitution options, and then increasing $f$ as long as
    characters on the diagonal are matching.

    Only $O(s^2)$ values of $f$ are computed, and if the alignment is not
    needed, only the last /front/ $f_{\bullet, p}$ is needed at each step.
  * *Gap heuristic*: The distance from $d_{ij}$ to the end $d_{nm}$ is at least
    $|(i-n)-(j-m)|\cdot \Delta$ when $\Delta$ is the cost of an indel.
    This allows pruning of some diagonals.

  Additionally, this paper introduces an algorithm that does exponential search
  on the band with, leading to an $O(ns)$ algorithm for general costs but using
  $O(ns)$ space.
- [cite/text/cf:@myers86], submitted '85 ::
  Independent of [cite/text:@ukkonen85], this
  introduces the concept of furthest reaching point and the
  recursion, but for LCS. Dijkstra's algorithm is used to evaluate DP states in
  order of increasing distance. For random strings, they show it runs in
  $O(n+d^2)$ expected time.

  Uses divide-and-conquer to achieve $O(n)$ space; see below.
- [cite/text/cf:@lv89], submitted '86 :: Extends [cite/text:@ukkonen85]
  to finding /all/ matches of a pattern in a text with at most $k$ errors, in
  $O(nm)$ time. They improve this to $O(nk)$ by using a suffix tree with LCA
  queries to extend matching diagonals in $O(1)$ instead of checking one
  character at a time.

Note that applying the suffix tree optimization from [cite/text:@lv89] to
[cite/text:@ukkonen85] gives a worst case $O(n+s^2)$ algorithm. [TODO: Look for
the first paper writing this down.]

At the same time, there were developments for using only linear memory to
reconstruct the alignment. The result for LCS was quite old already before it
was realized it can also be applied for pairwise alignment.

- [cite/text/cf:@hirschberg75] :: Divide-and-conquer approach to
  find the LCS (/longest common subsequence/) in quadratic time and linear space.
- [cite/text/cf:@myers88] :: Applies the divide-and-conquer approach of
  [cite/text/cf:@hirschberg75] to the quadratic gap-affine algorithm of
  [cite/text/cf:@gotoh], for $O(nm)$ time and $O(\min(n,m))$ space.

Independently, an algorithm was found that uses subquadratic time,
even in the worst case:

- [cite/text/cf:@four-russians-ed] :: Solves pairwise alignment in $O(nm / \lg
  \max(n,m))$ time for discrete scores and a finite alphabet, using the [[https://en.wikipedia.org/wiki/Method_of_Four_Russiansa][*Four Russians*]]
  technique.

It is now know that this has nearly optimal performance:

- [cite/text/cf:@no-subquadratic-ed] :: 
  Shows that edit distance can not be solved in time $O(n^{2-\delta})$
  for any $\delta > 0$, on the consition that the /Strong Exponential Time
  Hypothesis/ is true.

** Modern efficient implementations
Note: From 1990 to 2010 there is a gap without much theoretical progress on
exact alignment.
During this time, speedups were achieved by [TODO: citations]:
- more efficient implementations on available hardware;
- heuristic approaches such as banded alignment and $x$-drop.

There are many implementations of exact and inexact aligners. Here I will only
list current competitive aligners.

[TODO: This is very incomplete for now]

- Myers bit-parallel algorithm :: [TODO]
- Edlib :: A fast implementation (using Myers bit-parallel algorithm I believe)
- Block aligner :: approximate
- WFA :: exact, diagonal transition method

  States the recurrence for gap-affine costs for the diagonal transition
  algorithm, and provides a fast implementation. It is unclear to me why it took
  30+ years to merge the existing gap-affine recursion and more efficient
  diagonal-transition method.
- WFA2 :: Extends WFA to more cost models, more alignment modes, and introduces
  low-memory variants
- WFALM :: *L*ow *M*emory variant of WFA.

  Uses a square-root decomposition to do backtracking in $O(s^{3/2})$

  *Additional speedup:*
  The extension/greedy matching can be done using a precomputed suffixtree and LCA queries.
  This results in $O(n+m+s^2)$ complexity but is not faster in practice.
  [TODO: original place that does this]
- biWFA [WIP, unpublished] :: Meet-in-the-middle/divide-and-conquer variant of WFA, applying the ideas in
  [cite/text:@hirschberg75] to WFA to reconstruct the alignment in linear space.
- lh3/lv89 :: Similar to biWFA (but non-recursive) and WFALM (but with a fixed
  edit-distance between checkpoints, instead of dynamically storing every
  $2^{i}$ /th/ wavefront).

* References
#+print_bibliography:
