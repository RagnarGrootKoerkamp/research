#+title: [WIP] Computing minimizers, fast
#+filetags: @results @lablog hpc minimizers nthash wip
#+HUGO_LEVEL_OFFSET: 0
#+OPTIONS: ^:{} num:2 H:4
#+hugo_front_matter_key_replace: author>authors
#+toc: headlines 3
#+PROPERTY: header-args :eval never-export
#+hugo_paired_shortcodes: notice details
#+date: <2024-07-12 Fri>

In this post, we will develop a fast implementation of random minimizers.

Code for this post is in the =benches/= directory of
[[https://github.com/RagnarGrootKoerkamp/minimizers][github:RagnarGrootKoerkamp/minimizers]]. The unpolished experiments are in the
=playground= branch.

Contents[fn::Read this on a wide screen to see the table of contents on the
left, and footnotes on the right.]:
- [[*Random minimizers][Background]]
- [[*Algorithms][Overview of algorithms]]
- TODO: [[#Optimizing][Optimizing the implementation]]
  - TODO: [[*Setting up benchmarking][Benchmarking]]

* Random minimizers
Many tools in bioinformatics rely on /k-mers/.
Given a string/text/DNA sequence ~t: &[u8]~ of length $n$, the /k-mers/ of ~t~ are all the
length-$k$ substrings. There are $n-k+1$ of them.

In order to speed up processing, and since consecutive k-mers are overlapping
and contain redundancy, it is common to /subsample/ the k-mers. This should be
done in a deterministic way, so that similar but slightly distinct sequences
still mostly sample the same k-mers. One such way is /random minimizers/,
introduced in parallel by [cite/t:@winnowing] and [cite/t:@minimizers]:
1. First, a /window size/ $w$ is chosen, which will guarantee that at least one
   k-mer is sampled every $w$ positions of $t$, to prevent long gaps between
   consecutive k-mers.
2. Then, every k-mer is hashed using a pseudo-random hash function $h$, and in
   each /window/ of $w$ consecutive k-mers (of total length $\ell=w+k-1$), the /leftmost/[fn::Some foreshadowing here..] one with the smallest
   hash is selected.
Consecutive windows (that overlap in $w-1$ k-mers) often sample the same k-mer,
so that the total number of sampled k-mers is much smaller than $n-k+1$. In
fact, random minimizers have a /density/ of $2/(w+1)$ and thus sample
approximately two k-mers per window.

My [[../mod-minimizers/mod-minimizers.org][previous post]] on minimizers gives more background on density and other
sampling schemes. One such alternative scheme is the mod-minimizer
[cite:@modmini], that has low density when $w$ is large compared to $k$. That
paper also contains a review of methods and pseudocode for them.
In this post, I will focus on the most widely used random minimizers.

* Algorithms
** Problem statement
Before going into algorithms to compute random minimizers, let's first precisely state the
problem itself. In fact, we'll state three versions of the problem, that each
include slightly more additional information in the output.

*** Problem A: Only the set of minimizers
The most basic form of computing minimizers is to simply return the list of
minimizer kmers and their positions. This information is sufficient to /sketch/
the sequence.

#+name: problem-a
#+caption: A trait that defines Problem A and returns the set of kmers of the input.
#+begin_src rust
pub trait Minimizer {
    /// Problem A: The absolute positions of all minimizers in the text.
    fn minimizer_positions(&self, text: &[u8]) -> Vec<usize>;
}
#+end_src

*** Problem B: The minimizer of each window
When building datastructures on top on a sequence, such as SSHash
[cite:@sshash], just the set of k-mers is not sufficient. Each window should be
mapped to its minimizer, but there could be multiple minimizers in a single
window. Thus, in this version of the problem, we return the minimizer of each window.

Note that while in [[*Problem A: Only the set of minimizers][Problem A]] we return one result per minimizer, here we return
one result per window (roughly, per character). This difference can have
implications for the optimal algorithm implementation, since it's not unlikely
that the main loop of the optimal solution has /exactly/ one iteration per
returned element.

#+name: problem-b
#+caption: A trait for Problem B that returns the minimizer of each window.
#+begin_src rust
pub trait Minimizer {
    ...

    /// Problem B: For each window, the absolute position in the text of its minimizer.
    fn window_minimizers(&self, text: &[u8]) -> Vec<usize>;
}
#+end_src

*** Problem C: Super-k-mers
We can implement SSHash and other indexing datastructures using the output of [[*Problem B: The minimizer of each window][Problem B]],
but in most such applications, it is probably more natural to directly iterate over the /super-k-mers/: the longest substrings such that all contained windows have the same
minimizer. This can easily be obtained from the output of Problem B by
/grouping/ consecutive windows that share the minimal k-mer.

Note that here the result is again an iterator over /minimizers/, like in
[[*Problem A: Only the set of minimizers][Problem A]] rather than /windows/ as in [[*Problem B: The minimizer of each window][Problem B]].

#+name: problem-c
#+caption: A trait that defines Problem C and returns the set of super-k-mers.
#+begin_src rust
struct SuperKmer {
    /// The absolute position in the text where this super-k-mer starts.
    /// The end of the super-k-mer can be inferred from the start of the next super-k-mer.
    start_pos: usize,
    /// The absolute position in the text of the minimizer of this super-k-mer.
    minimizer_pos: usize,
}

pub trait Minimizer {
    ...

    /// Problem C: The super-k-mers of the text.
    fn super_kmers(&self, text: &[u8]) -> Vec<SuperKmer>;
}
#+end_src

*** Which problem to solve
In the remainder, we mostly focus on [[*Problem B: The minimizer of each window][Problem B]], since that is what I initially
started with. However, some methods more naturally lean themselves to solve the
other variants. We will benchmark each method on whichever of the problems it
runs fastest, and postpone optimizing a solution for each specific problem until
later.

Most likely though, solving [[*Problem C: Super-k-mers][Problem C]] will yield the most useful tool for end
users. I plan to revisit this at the end of this post.

Regardless, for convenience we can implement solutions to A and C in terms of
 B.[fn::Consider not skipping the code blocks. I made quite some effort to make
 them clean and readable. Most code is fairly straightforward assuming you're
 comfortable reading Rust code.]
#+caption: Implementing =minimizer_positions= and =super_kmers= in terms of =window_minimizers=.
#+include: "code/blog/minimizer.rs" src rust :lines "11-"

*** Canonical k-mers
In actual bioinformatics applications, it is common to consider /canonical
k-mers/ and corresponding /canonical minimizers/. These are defined such that
the canonical k-mer of a string and its reverse complement are the same.
This will significantly impact the complexity of our code, and hence we also
postpone this for later.


** The naive algorithm

The naive $O(|t| \cdot w)$ method simply finds the minimum of each window independently.

It looks like this:[fn:legend:*Legend*:\\
  @@html:<span style="color:orange">Orange</span>@@: processed state,\\
  @@html:<span style="border:1px black solid">Bold outline</span>@@: minimum of
  the prefix/suffix,\\
  *Bold character*: minimum of the window,\\
  @@html:<span style="color:blue">Blue</span>@@: state in memory,\\
  @@html:<span style="color:red">Red</span>@@: state removed from the queue,\\
  @@html:<span style="color:green">Green</span>@@: running prefix minimum.
  ]
#+caption: Illustration of the naive algorithm for a sequence of 8 kmers and window size /w=4/. Hashes of the kmers are shown at the top. This method iterates over all windows, and for each window, finds the element with the smallest hash. The orange colour indicates that for each window, all hashes are checked. The minimum is shown in bold.
#+attr_html: :class inset
[[file:./naive.svg]]

In code, it looks like this.

#+caption: An implementation of the naive algorithm. Note that hashes are recomputed for each window.
#+include: "code/blog/naive.rs" src rust :lines "3-29"

*** Performance characteristics
:PROPERTIES:
:CUSTOM_ID: naive-performance
:END:

I was planning to show some preliminary performance numbers here, to give some
intuition about the relative performance of this and upcoming methods.
Unfortunately though (and not completely surprising), performance depends
heavily on small details of how the code is written. I could provide a
comparison between the methods as-is, but the numbers would be hard to interpret.
Instead, let's consider some high level expectations of this algorithm for now,
and postpone the quantitive comparison until later.

Since there are no =if= statements, the code should be highly predictable. Thus,
we expect:
- few branch misses,
- high instructions per cycle,
- and generally running times linear in $w$.

** Sliding window minimum
After hashing all k-mers, we basically have a sequence of $n-k+1$ pseudo-random
integers. We would like to find the position of the leftmost minimum in each
window of $w$ of those integers.
Thus, the problem of finding random minimizers reduces to computing the sliding
window minima.
We can model this problem using the following trait:
#+caption: Trait for the sliding window minimum problem.
#+include: "code/blog/sliding_min.rs" src rust :lines "2-16"

A first implementation could simply store the last $w$ hashes in a =RingBuffer=,
and in each step recompute the minimum. The boiler plate for the ring buffer is simple:
#+caption: A simple ring buffer.
#+include: "code/blog/ringbuf.rs" src rust :lines "3-36"

And then a sliding window implementation also becomes trivial:

#+caption: The =BufferedSlidingMin= implementation of =SlidingMin= that buffers the last $w$ values and recomputes the minimum on each =push=.
#+include: "code/blog/naive.rs" src rust :lines "30-"

Using this trait, we can clean up the implementation of the naive minimizer from
before. In fact, we can now generically implement a minimizer scheme using any
implementation of the sliding window algorithm.

#+name: queue-wrapper
#+caption: A generic minimizer algorithm using any solution to the sliding window minimum problem, and the corresponding =Buffered= minimizer.
#+include: "code/blog/sliding_min.rs" src rust :lines "46-"

** The queue
Let's try to improve this somewhat inefficient $O(|t|\cdot w)$ algorithm.  A first
idea is to simply keep a rolling prefix-minimum that tracks the lowest
value seen so far. For example, when $w=3$ and the hashes are
$[10,9,8,7,6,...]$, the rolling prefix minimum is exactly also the minimum of
each window.  But alas, this won't work for
increasing sequences: when the hashes are $[1,2,3,4,5,...]$ and the window
shifts from $[1,2,3]$ to $[2,3,4]$, the prefix minimum is $1$ (at index $0$),
but it is not a minimum of the new window anymore, which instead goes up to $2$.

Nevertheless, as the window slides right, each time we see a new value that's smaller than
everything in the window, we can basically 'forget' about all existing values
and just keep the new smaller value in memory. Otherwise, we can still forget
about all values in the window larger than the value that is shifted in.

This is formalized by using a *queue* of non-decreasing values in the window.
More precisely, each queue element will be smaller than all values in the window
coming after it.
At each step, the minimum of the window is the value at the front of the queue.
Let's look at our example again.[fn:legend]

#+name: fig-queue
#+caption: The queue method: In each step, we push the new value (orange) to the right/back of the queue. States stored in the queue are blue. Any preceding values in the queue that are larger than the new element are dropped (red). The smallest element of the window is on the left/front of the queue. In the second to last window, the leading $3$ is dropped from the front queue as well because it falls out of the window.
#+attr_html: :class inset
[[file:./queue.svg]]

We see that there are two reasons elements can be removed from the queue:
1. a smaller element is pushed,
2. the window shifts so far that the leftmost/smallest element falls outside it.
To handle this second case, we don't just store values in this queue, but also
their position in the original text, so that we can pop them when needed.

In code, it looks like this:
#+caption: An implementaion of the =MonotoneQueue= and the corresponding =QueueMinimizer=.
#+include: "code/blog/queue.rs" src rust :lines "2-28"

*** Performance characteristics
:PROPERTIES:
:CUSTOM_ID: queue-performance
:END:
Since each element is pushed once and popped once, each call to
=push= takes amortized constant $O(1)$ time, so that the total runtime is
$O(|t|)$ (assuming that hashing each k-mer takes constant time).
We may expect a $w=11$ times speedup over the buffered method, but most likely
this will not be the case.
This is due to the high number of unpredictable branches,
which hurts pipelining/instruction level parallelism (ILP). Thus, while the
complexity of the =Queue= algorithm is good, its practical performance may not be optimal.

** Jumping: Away with the queue
While the queue algorithm is nice in theory, in practice it is not ideal due to
the branch-misses.
Thus, we would like to find some middle ground between keeping this queue
and the naive approach of rescanning every window. A first idea is this: Once we find the
minimizer of some window, we can jump to the first window after that position to
find the next minimizer. This
finds all distinct minimizers, although it does not compute the minimizer for
each window individually. Thus, it solves [[*Problem A: Only the set of
minimizers][Problem A]] instead of [[*Problem B: The minimizer of each
window][Problem B]].[fn:legend]

#+caption: Each time a minimizer (green) is found, jump to the window starting right after to find the next minimizer. The last iteration is special and checks if possibly one more minimizer has to be added, which is not the case here.
#+attr_html: :class inset
[[file:./jump.svg]]

Since the expected distance between random minimizers is $(w+1)/2$, we expect to
scan each position just under two times.

#+caption: Solving Problem A by jumping to the window after the previous minimizer. Note that this first computes all hashes in a separate pass.
#+include: "code/blog/jumping.rs" src rust :lines "2-"

*** Performance characteristics
:PROPERTIES:
:CUSTOM_ID: queue-performance
:END:

One benefit of this method is that it is completely branchless. Each iteration of
the loop corresponds to exactly one output. The only unpredictable step is which
element of a window is the minimizer, since that determines which is the next
window to scan. Thus, even though it is branchless, consecutive iterations
depend on each other and ILP (instruction level parallelism, aka pipelining) may only have a limited effect.

** Re-scan
I learned of an improved method via [[https://twitter.com/daniel_c0deb0t][Daniel Liu]]'s [[https://gist.github.com/Daniel-Liu-c0deb0t/7078ebca04569068f15507aa856be6e8][gist]] for winnowing,
but I believe the method is folklore[fn::Citation needed, both for it being folklore and
for the original idea.].
This is a slightly improved variant of the method above. Above, we do a new scan
for each new minimizer, but it turns out this can sometimes be avoided.
Now, we will only re-scan when a previous minimum falls outside the window, and
the minimum of the window goes up.[fn:legend]


#+caption: Keep a rolling minimum (green) of the lowest hash seen so far by comparing each new element (orange) to the minimum. Only when the minimum falls outside the window, recompute the minimum for the entire window (second to last yellow row).
#+attr_html: :class inset
[[file:./rescan.svg]]

Thus, we keep
a cyclic buffer of the last $w$ values, and scan it as needed. One point of attention
is that we need the /leftmost/ minimal value, but as the buffer is cyclic, that
is not the first minimum in the buffer. Thus, we partition the scan into two
parts, and prefer minima in the second (older) half.

#+caption: =Rescan= implementation of =SlidingMin=.
#+include: "code/blog/rescan.rs" src rust :lines "2-"

*** Performance characteristics
:PROPERTIES:
:CUSTOM_ID: rescan-performance
:END:
This method should have much fewer branch-misses than the queue, since we rescan
a full window of size $w$ at a time. Nevertheless, these re-scans occur at
somewhat random moments, so some branch-misses around this are still to be expected.

** Split windows
In an ideal world, we would use a fully predictable algorithm, i.e., an
algorithm without any data-dependent branches/if-statements.[fn::I'm not
counting the for loop that iterates up to the variable length of the string as a
branch miss, because that will only be a single branch-miss at the end, not
$O(1)$ per iteration.]

With some [[https://www.google.com/search?q=sliding%20window%20minimum][Googling]][fn::It's the third result.] I found [[https://codeforces.com/blog/entry/71687][this codeforces blog]] on
computing sliding window minima. In the post, the idea is presented using two
queues. My explanation below goes in a slightly different direction, but in the
end it comes down to the same.

*Aside: Accumulating non-invertible functions.*
So, as we saw, most methods keep some form of rolling prefix minimum that is
occasionally 'reset', since 'old' small elements should not affect the current
window.
This all has to do with the fact that the =min= operation is not /invertible/:
knowing only the minimum of a set $S$ is not sufficient to know the minimum of
$S - \{x\}$.
This is unlike for example addition: sliding window sums are easy by just adding
the new value to the sum and subtracting the value that falls out of the window.

This distinction is also exactly the difference between /segment trees/ and
/Fenwick trees/[fn::Feel free to skip this paragraph in case you are not
familiar with segment and Fenwick trees.]: a Fenwick tree computes some function
$f$ on a range $[l, r)$ by removing $f(a_0, \dots, a_{l-1})$ from $f(a_0, \dots, a_{r-1})$, by
decomposing both ranges into intervals with power-of-$2$ lengths corresponding
to the binary representation of their length. But this only works if $f$ is
invertible. If that is not the case, a segment tree must be used, which
partitions the interval $[l, r)$ itself, without going 'outside' it.

*The algorithm.*
Based on this, we would also like to partition our windows such that we can
compute their minimum as a minimum over the different parts. It turns out that
this is possible, and in a very clean way as well!

First, we conceptually chunk the input sequence into parts with length $w$, as
shown in the top row of [[fig-split]] for $w=4$. Then, each window of length $w$
either
exactly matches such a chunk, or overlaps with two consecutive chunks. In the latter
case, it covers a suffix of chunk $i$ and a prefix of chunk $i+1$. Thus, to
compute the minimum for all windows, we need to compute the prefix and suffix
minima of all chunks. We could do that as a separate pass over the data in a
pre-processing step, but it can also be done on the fly:

1. Each time a new kmer is processed, its hash is written to a size-$w$ buffer.
   (The orange values in the top-right of [[fig-split]].)
2. After processing the window corresponding to a chunk, the buffer contains
   exactly the hashes of the chunk. (Fourth row.)
3. Then, we compute the suffix-minima of the chunk by scanning the buffer
   backwards. (Fifth row.)
4. For the next $w$ windows, we intialise a new rolling prefix minimum in the
   new chunk (green outline).
5. The minimum of each window is the minimum between the rolling prefix minimum
   in chunk $i+1$ (green outline), and the suffix minimum in chunk $i$ that we computed in the
   buffer (blue outline).[fn:legend]

#+name: fig-split
#+caption: Split algorithm: for each chunk, we compute and store suffix-minima of the preceding chunk (orange row). We also track a rolling prefix minimum in the next chunk (green). Taking the minimum of that with the stored suffix-minimum gives the minimum of the window. The memory after each iteration is shown on the right, with updated values in orange.
#+attr_html: :class inset
[[file:./split.svg]]

#+caption: Code for the =Split= algorithm, that computes the suffix minima for each chunk exactly every $w$ iterations.
#+include: "code/blog/split.rs" src rust :lines "2-"

*** Performance characteristics
:PROPERTIES:
:CUSTOM_ID: split-perfomance
:END:

The big benefit of this algorithm is that its execution is completely deterministic, similar
to the [[*Jumping: Away with the queue][jump algorithm]]: every $w$ iterations we compute the suffix-minima, and
then we continue again with the rolling prefix minimum. Since this 'side loop'
is taken exactly once in every $w$ iterations, the corresponding branch is easy
to predict and does not cause branch-misses. Thus, we expect nearly $0\%$ of
branch-misses, and up to $4$ instructions per cycle.

* Analysing what we have so far
** Counting comparisons

Before we look at runtime performance, let's have a more theoretical look at the
number of comparisons each method makes.
We can do this by replacing the =u64= hash type by a wrapper type that increments a
counter each time =.cmp()= or =.partial_cmp()= is called on it. I exclude
comparisons with the =MAX= sentinel value since those could possibly be
optimized away anyway. The code is not very interesting, but you can find it [[https://github.com/RagnarGrootKoerkamp/minimizers/blob/master/benches/blog/counting.rs][here]].

#+name: counts
#+caption: The number of comparisons per character on a string of length $10^8$ for $w=10$ and $w=20$.
#+begin_src txt
           w=10    w=20     formula
Buffered:  9.000  19.000    w-1
Queue:     1.818   1.905    2-2/(w+1)
Jumping:   1.457   1.595    ?
Rescan:    1.818   1.905    2-2/(w+1)
Split:     2.700   2.850    3-3/w
#+end_src

Observations:
- =Buffered= Does $w-1$ comparisons for each window to find the max of $w$ elements.
- =Queue= and =Rescan= both do exactly $2-2/(w+1)$ comparisons per element.
- For =Queue=, each =pop_back= is preceded by a comparison, and each =push= has
  one additional comparison with the last element in the queu that is smaller
  than the new element. Only when the new element is minimal (probability
  $1/(w+1)$), the queue becomes empty and that comparison doesn't happen. Also
  when the smallest element is popped from the front (probability $1/(w+1)$), that means it won't be
  popped from the back and hence also saves a comparison.
- Unsurprisingly, the =Jumping= algorithm uses the fewest comparisons, since it also
  returns strictly less information than the other methods.
- The =Split= method, which seems very promising because of its
  data-independence, actually uses around $50\%$ more comparisons ($3-3/w$, to be
  precise), because for each window, a minimum must be taken between the suffix
  and prefix.

*** Open problem: Theoretical lower bounds

It would be cool to have a formal analysis showing that we cannot do
better than $2-2/(w+1)$ expected comparisons per element to compute the random
minimizer of all windows, and similarly it would be nice to have a lower bound
on the minimal number of comparisons needed to only compute the positions of the minimizers.

** Setting up benchmarking
*** Adding criterion
Before we start our implementation, let's set up a benchmark so we can easily
compare them. We will use [[https://crates.io/crates/criterion][criterion]], together with the [[https://crates.io/crates/cargo-criterion][cargo-criterion]] helper.
First we add =criterion= as a dev-dependency to =Cargo.toml=.

#+caption: =Cargo.toml= changes to set up =criterion=.
#+begin_src toml
[dev-dependencies]
criterion = "*"

# Do not build the library as a benchmarking target.
[lib]
bench = false

[[bench]]
name = "bench"
harness = false
#+end_src

We add =benches/bench.rs= which looks like this:
#+attr_shortcode: "Click to show code."
#+begin_details
#+caption: A basic criterion benchmark for the comparison so far.
#+include: "code/bench.rs" src rust :lines "19-53"
#+end_details

*** Making criterion fast
By default, the benchmarks are quite slow. That's for a couple of reasons:
- Before each benchmark, a 3 second warmup is done.
- Each benchmark is 5 seconds.
- After each benchmark, some plots are generated.
- At the end, some HTML reports are generated.
I'm impatient, and all this waiting *really* impacts my iteration time, so let's
make it faster:
- Reduce warmup time to 0.5s.
- Reduce benchmark time to 2s, and only take 10 samples.
- Disable plots and html generation.
The first two are done from code:
#+caption: Initializing =criterion= with reduced warmup and measurement time.
#+begin_src rust
criterion_group!(
    name = group;
    config = Criterion::default()
        // Make sure that benchmarks are fast.
        .warm_up_time(Duration::from_millis(500))
        .measurement_time(Duration::from_millis(2000))
        .sample_size(10);
    targets = initial_runtime_comparison, count_comparisons_bench
);
#+end_src
The last is best done by adding the ~--plotting-backend disabled~ flag. For
convenience, we add this rule to the =justfile= so we can /just/ do =just
bench=. I'm also adding =quiet= to hide the comparison between runs to simplify presentation.
#+caption: Adding the =bench= rule to the =justfile=.
#+begin_src make
bench:
    cargo criterion --plotting-backend disabled --output-format quiet
#+end_src

When we now do =just bench=, we see this (TODO update):

#+begin_export html
<script src="https://asciinema.org/a/EQtJkYBEXYzHsEBnhrMLOp29l.js" id="asciicast-EQtJkYBEXYzHsEBnhrMLOp29l" async="true"></script>
#+end_export

*** A note on CPU frequency

Most consumer CPUs support turboboost to increase the clock frequency for short
periods of time. That's nice, but not good for stable measurements. Thus, I
always pin the frequency of my =i7-10750H= to the default ~2.6GHz~:
#+caption: Pinning CPU frequency to =2.6GHz=.
#+begin_src sh
sudo cpupower frequency-set --governor powersave -d 2.6GHz -u 2.6GHz
#+end_src
This usually results in very stable measurements.

Similarly, I have hyper threading disabled, so that each program has a full core
to itself, without interference with other programs.


** Runtime comparison with other implementations
Let's compare the runtime of our method so far with
two other Rust implementations:
- [[https://crates.io/crates/minimizer-iter][minimizer-iter]] is an implementation written by [[https://twitter.com/IgorMartayan][Igor Martayan]]. It returns an iterator over all
  /distinct/ minimizers and uses the queue algorithm.
- Daniel Liu's [[https://gist.github.com/Daniel-Liu-c0deb0t/7078ebca04569068f15507aa856be6e8][implementation]] of the rescan algorithm.

#+caption: Runtime comparison between our and other's implementations.
#+begin_src txt
g/naive                 time:   [64.399 ms 64.401 ms 64.404 ms]
g/buffered              time:   [55.154 ms 55.160 ms 55.173 ms]
g/queue                 time:   [22.305 ms 22.322 ms 22.335 ms]
g/jumping               time:   [7.0913 ms 7.0969 ms 7.1042 ms]
g/rescan                time:   [14.067 ms 14.071 ms 14.078 ms]
g/split                 time:   [14.021 ms 14.023 ms 14.027 ms]
g/queue_igor            time:   [26.680 ms 26.689 ms 26.700 ms]
g/rescan_daniel         time:   [9.0335 ms 9.0340 ms 9.0349 ms]
#+end_src

Some initial observations:
- Measurements between samples are very stable, varying less than $1\%$.
- Each method takes 7-55ms to process 1 million characters. That's 7-55ns per
  character, or 7s to 55s for 1Gbp.
- Our queue implementation performs similar to the one in =minimizer-iter=.
- Daniel's implementation of =Rescan= is around twice as fast as ours! Time to
  start looking deeper into our code.

Looking back at our expectations:
- As expected, =Buffered= is the slowest method by far, but even for $w=11$, it
  is only $2\times$ slower than the next-slowest method.
- =Split= is about as fast as =Rescan=, and even a bit slower. We were expecting
  a big speedup here, so something is weird.
- =RescanDaniel= is even much faster than our =Split=, so clearly there must be
  a lot of headroom.

** Deeper inspection using =perf stat=
Let's look some more at performance statistics using =perf stat=. We can reuse
the benchmarks for this, and call =criterion= with the =--profile-time 2= flag,
which simple runs a benchmark for $2$ seconds.

#+caption: =just stat= will run a benchmark for 2 seconds under =perf stat=.
#+begin_src just
stat test='':
    cargo build --profile bench --benches
    perf stat -d cargo criterion -- --profile-time 2 {{test}}
#+end_src

The metrics that are relevant to us now are summarized in the following table.

#+begin_src sh :exports none
for alg in naive buffered\$ queue\$ queue_igor jumping rescan\$ rescan_daniel split\$ ; just stat-selection $alg ; end
#+end_src

#+caption: =perf stat= results for all methods so far.
#+attr_html: :class small
| metric         | Runtime (ns/char) | instructions/cycle (GHz) | branches/sec (M/sec) | branch misses % |
| =Naive=        |                64 |                     4.13 |                 1743 |           0.11% |
| =Buffered=     |                55 |                     2.17 |                  987 |           4.66% |
| =Queue=        |                22 |                     2.29 |                  810 |           4.29% |
| =QueueIgor=    |                27 |                     2.33 |                  662 |           4.29% |
| =Jumping=      |               7.1 |                     3.89 |                 1459 |           0.13% |
| =Rescan=       |                14 |                     3.02 |                 1116 |           2.26% |
| =RescanDaniel= |               9.0 |                     3.21 |                 1410 |           0.85% |
| =Split=        |                14 |                     3.37 |                 1116 |           1.55% |

Observe:
- =Naive= has an instructions per cycle (IPC) of over 4! I have never actually seen IPC that high.
  It's not measurement noise, so most likely it comes from the fact that some
  instructions such as compare followed by conditional jump are fused.
- =Naive= also has pretty much no branch misses.
- =Buffered= only has half the IPC of =Naive= but is still slightly faster since
  it only hashes each kmer once. For some reason, there are a lot of branch
  misses, which we'll have to investigate further.
- The two =Queue= variants also have
  relatively few instructions per cycle, and
  also relatively a lot of branch misses. Here, the branch-misses are expected,
  since the queue pushing and popping are inherently unpredictable.
- In fact, I argued that the last element of the =Queue= is popped with
  probability $50\%$. But even then, the branch predictor is able to get down to
  $4\%$ of branch misses. Most likely there are correlations that make these
  events somewhat predictable, and of course, there are also other easier
  branches that count towards the total.
- =Jumping= has an very good IPC of nearly $4$, as we somewhat hoped for, and
  correspondingly has almost no branch misses.
- The remaining methods all have a decent IPC just above $3$.
- =RescanDaniel= has much fewer branch misses than =Rescan=. We should investigate this.
- We argued that =Split= is completely predictable, but we see a good number of
  branch misses anyway. Another thing to investigate.

** A first optimization pass
It's finally time to do some actual optimizations!
We'll go over each of the algorithms we saw and optimize them.
This step is 'just' so we can make a fair benchmark between them, and in the end
we will only optimize one of them further, so I will try to keep it short.
The final optimized version of each algorithm can be found in the repository,
but intermediate diffs are only shown here.
*** Optimizing =Buffered=: reducing branch misses using forward minima
Let's have a look at =perf record=:
#+caption: =just perf buffered= shows very inefficient and branchy code. Lines marked with a =*= have over /5%/ of time spent in them. This pattern occurs multiple times due to loop unrolling.
#+begin_src asm
  1.16 │       cmp    %r8,%rbp          ; compare two values
  0.92 │       setne  %r11b
  0.70 │    ┌──jb     976               ; conditionally jump down
 *6.15*│    │  mov    %r11b,%r13b       ; overwrite the minimum (I think)
  0.02 │    │  mov    %r13d,%r12d       ; overwrite the position (I think)
 *5.44*│976:└─→mov    0x8(%rdx),%r10    ; continue
#+end_src
Most of these branches come from the fact that computing the minimum of two
=Elem { val: usize, pos: usize }= is very inefficient, since two comparisons are
needed, and then the result is conditionally overwritten.

The only reason we are storing an =Elem= that includes the position is that we
must return the leftmost minimum. But since e.g. =position_min()= returns the
leftmost minimum anyway, we can try to avoid storing the position and always
iterate over values from left to right. We'll add a method to our =RingBuf= for this:
#+caption: Split the =RingBuf= into two slices, such that the element of the first returned slice come before the second part, and compute their minimum.
#+include: "code/blog/ringbuf.rs" src rust :lines "44-"

#+caption: =BufferedOpt= that only stores values and finds the minimum from left to right. Runs in =26ms=, down from =55ms=.
#+include: "code/blog/naive.rs" src rust :lines "49-"

The hot loop of the code looks much, much better now, using =cmov= instead of
branches to overwrite the minimum.

#+attr_shortcode: "New assembly"
#+begin_details
#+caption: New hot-loop, using =cmov=, and very few branches.
#+begin_src asm
  1.59 │898:┌─→lea        0x8(%r14),%r13
  4.85 │    │  mov        %rbx,0xe0(%rsp)
  2.72 │    │  mov        %r9,0xf8(%rsp)
  5.29 │    │  mov        %r13,0xf0(%rsp)
  1.85 │    │  lea        0x1(%rax),%rbp
  4.78 │    │  mov        %rbp,0x110(%rsp)
 11.20 │    │  mov        (%r14),%r14
  7.24 │    │  cmp        %rcx,%r14       ; compare
  3.41 │    │  cmovb      %rax,%r11       ; conditionally overwrite pos
  4.04 │    │  cmovb      %r14,%rcx       ; conditionally overwrite min
  2.50 │    │  mov        %rbp,%rax
  5.19 │    │  mov        %r13,%r14
  3.01 │8d4:│  test       %r14,%r14
  0.92 │    │↓ je         8de
  0.44 │    ├──cmp        %r9,%r14
  3.90 │    └──jne        898          
#+end_src
#+end_details

Also the =perf stat= is great now: =4.23 IPC= and =0.13%= branch misses, so
we'll leave it at this.

#+attr_shortcode: takeaway
#+begin_notice
- Compute minima from left to right to avoid comparing positions.
- Prefer =cmov= over branches to compute minima.
#+end_notice

*** =Queue= is hopelessly branchy
We can confirm that indeed a lot of time is spend on branch-misses around
the popping of the queue.
#+attr_shortcode: "Assembly code showing a badly predicted branch."
#+begin_details
#+caption: Each of the two directions after the branch takes =10%= of time.
#+begin_src asm
 1.20 │3a0:   lea        (%rdx,%rax,1),%rdi
 0.63 │ ↑     cmp        %rcx,%rdi
 1.34 │ │     mov        $0x0,%edi
 0.97 │ │     cmovae     %rcx,%rdi
 0.46 │ │     shl        $0x4,%rdi
 0.49 │ │     mov        %rsi,%r8
 1.58 │ │     sub        %rdi,%r8
 1.52 │ │  ┌──cmp        %rbp,(%r8)    ; compare the new element with the back
 1.97 │ │  ├──jb         3d2           ; if back is smaller, break loop
*9.85*│ │  │  dec        %rax          ; pop: decrease capacity
 3.70 │ │  │  mov        %rax,0x28(%rsp)
 0.41 │ │  │  add        $0xfffffffffffffff0,%rsi
 0.08 │ │  │  test       %rax,%rax     ; queue not yet empty
 0.37 │ └──│─ jne        3a0           ; try popping another element
 0.38 │    │  xor        %eax,%eax
*9.64*│3d2:└─→cmp        %rcx,%rax
#+end_src
#+end_details

Sadly, I don't really see much room for improvement here.

*** =Jumping= is already very efficient
If we do a =perf report= here, we see that only =25%= of the time is spent in
the =minimizer_positions= function:
#+caption: =just perf jumping= shows that most time is spent computing hashes.
#+begin_src txt
  63.69%  <map::Map<I,F> as Iterator>::fold
  24.25%  <JumpingMinimizer<H> as Minimizer>::minimizer_positions
#+end_src

The hot loop for finding the minimum of a window is very compact again.
#+attr_shortcode: "Assembly code"
#+begin_details
#+caption: Hot loop of =Jumping= that finds the minimum of a window, with plenty =cmov= instructions and no branches.
#+begin_src asm
  2.84 │170:┌─→lea     0x1(%rcx),%rsi
  1.58 │    │  mov     -0x18(%rdx,%rcx,8),%rdi
  4.17 │    │  mov     -0x10(%rdx,%rcx,8),%r8
  0.09 │    │  cmp     %rdi,%rax
  3.60 │    │  cmovb   %rax,%rdi
  9.34 │    │  cmovbe  %rbp,%rsi
       │    │  lea     0x2(%rcx),%rax
  0.94 │    │  cmp     %r8,%rdi
  2.59 │    │  cmovae  %r8,%rdi
  9.36 │    │  cmovbe  %rsi,%rax
  1.35 │    │  mov     -0x8(%rdx,%rcx,8),%rsi
  0.27 │    │  lea     0x3(%rcx),%rbp
  0.86 │    │  cmp     %rsi,%rdi
  5.69 │    │  cmovae  %rsi,%rdi
  5.94 │    │  cmovbe  %rax,%rbp
  1.48 │    │  mov     (%rdx,%rcx,8),%rsi
  0.72 │    │  add     $0x4,%rcx
  0.32 │    │  cmp     %rsi,%rdi
  4.98 │    │  mov     %rdi,%rax
  1.31 │    │  cmovae  %rsi,%rax
  5.76 │    │  cmova   %rcx,%rbp
       │    ├──cmp     %rbx,%rcx
  0.13 │    └──jne     170                                                 
#+end_src
#+end_details

More problematic is the =63%= of time spent on hashing kmers, but we will
postpone introducing a faster hash function a bit longer.

*** Optimizing =Rescan=
The original =Rescan= implementation suffers from the same issue as the
=Buffered= and compares positions, so let's reuse the =RingBuf::forward_min()=
function, and compare elements only by value.
#+attr_shortcode: "Code for RescanOpt."
#+begin_details
#+caption: Optimized =RescanOpt= that runs in =10.2ms= instead of =14.1ns=.
#+include: "code/blog/rescan.rs" src rust :lines "25-"
#+end_details

This has =3.55 IPC= up from =3.02=, and =0.91%= branch misses, down from
=2.26%=, which results in the runtime going down from =14.1ms= to =10.2ms=.

The hottest part of the remaining code is now the branch-miss when ~if pos -
min.pos == w~ is true, taking ~6%~ of time, since this is an unpredictable
branch. This seems rather unavoidable. Otherwise most time is spent hashing kmers.

*** Optimizing =Split=
Also here we start by avoiding position comparisons.
This time it is slightly more tricky though: the =RingBuf= /does/ have to store
positions, since after taking suffix-minima, the value at each position does not
have to correspond anymore to its original position.

The tricky part here is to ensure that the compiler generates a branchless
minimum, since usually it creates a branch to avoid overwriting two values with
their own value:

#+caption: Making sure the suffix minima use =cmov= instructions.
#+begin_src diff
-if ring_buf[i + 1].val <= ring_buf[i].val {
-    ring_buf[i] = ring_buf[i + 1];
-};
+ring_buf[i] = if ring_buf[i + 1].val <= ring_buf[i].val {
+    ring_buf[i + 1]
+} else {
+    ring_buf[i]
+};
#+end_src

#+attr_shortcode: "Code for SplitOpt."
#+begin_details
#+caption: Optimized =SplitOpt= that runs in =11.2ms= instead of =14.0ns=.
#+include: "code/blog/split.rs" src rust :lines "30-"
#+end_details

Now, it runs in =11.2ms= instead of =14.0ms=, and has =3.46 IPC= (up from =3.37
IPC=) and =0.63%= branch misses, down from =1.55%=.
The remaining branch misses seem to be related to the kmer hashing, although
this is somewhat surprising as =Jumping= doesn't have them.

** A new performance comparison
Now that we have our optimized versions, let's summarize the new results.

#+begin_src sh :exports none
for alg in naive buffered\$ buffered_opt queue\$ queue_igor jumping rescan\$ rescan_opt rescan_daniel split\$ split_opt; just stat-selection $alg ; end
#+end_src

#+caption: =perf stat= results for all methods so far, for /w=11/.
#+attr_html: :class small
| metric         | Runtime (ns/char) | instructions/cycle (GHz) | branches/sec (M/sec) | branch misses % |
| =Naive=        |                64 |                     4.13 |                 1743 |           0.11% |
| =Buffered=     |                55 |                     2.17 |                  987 |           4.66% |
| =BufferedOpt=  |              *26* |                   *4.19* |               *1499* |         *0.13%* |
| =Queue=        |                22 |                     2.29 |                  810 |           4.29% |
| =QueueIgor=    |                27 |                     2.33 |                  662 |           4.29% |
| =Jumping=      |               7.1 |                     3.89 |                 1459 |           0.13% |
| =Rescan=       |                14 |                     3.02 |                 1116 |           2.26% |
| *=RescanOpt=*  |              *10* |                   *3.58* |               *1397* |         *0.90%* |
| =RescanDaniel= |               9.0 |                     3.21 |                 1410 |           0.85% |
| =Split=        |                14 |                     3.37 |                 1116 |           1.55% |
| =SplitOpt=     |              *11* |                   *3.47* |               *1410* |         *0.63%* |

Note how =BufferedOpt= is now almost as fast as =Queue=, even though it's
$O(nw)$ instead of $O(n)$. Also, =RescanOpt= is now nearly as fast as
=RescanDaniel=, although =Jumping= is still quite a bit faster.
For reasons I do not understand =SplitOpt= is still not faster than =RescanOpt=,
but we'll have to leave it at this.

* TODO Let the optimization begin
:PROPERTIES:
:CUSTOM_ID: Optimizing
:END:
Topics to cover, in some order:
- Branch misses around =min= of =(val, pos)= pairs
- unchecked indexing
- wyhash
- NtHash, and how to make an efficient implementation.
- SIMD: processing 4/8/16(?) sequences in parallel.
- Procomputing hashes, separate from ISMD

** TODO NtHash: a rolling kmer hash
One problem with =fxhash= and =wyhash= is that they hash strings of arbitrary
length, and hence generate a lot of code to handle all length modulo $8$ efficiently.
In practice, we've only been using $k=21$ so far, but this still requires
iterating over three $8$ byte words. Instead, a rolling hash only has to handle
the first and last character of each string, regardless of the length of the
string. We will use ntHash [cite:@nthash] (see also [[../nthash.org][this post]]). This assigns a random value $h(c)$ to
each DNA character $c$, and computes the hash of a string as $x$ as
\begin{equation}
h(x) = \bigoplus_{i=0}^{k-1} rot^i(h(x_i)),
\end{equation}
where $rot^i$ does a $64$-bit rotate, and $\oplus$ is the xor operation.
This can be efficiently computed incrementally by rotating the hash $1$, then
xor'ing in $h(x_k)$, and then xor'ing out $rot^{k}(h(x_0))$.

Using the =nthash= [[https://crates.io/crates/nthash][crate]], the implementation is simple:
#+begin_src rust
pub struct V5RescanNtHash {
    pub w: usize,
    pub k: usize,
}

impl Minimizer for V5RescanNtHash {
    fn minimizers(&self, text: &[u8]) -> Vec<usize> {
        let mut q = Rescan::new(self.w);
        let mut kmer_hashes = nthash::NtHashForwardIterator::new(text, self.k).unwrap();
        for h in kmer_hashes.by_ref().take(self.w - 1) {
            q.push(h);
        }
        kmer_hashes.map(|h| q.push(h).pos).collect()
    }
}
#+end_src

Unfortunately, it's slower than before:
#+begin_src txt
g/ext_daniel            time:   [8.9199 ms 8.9300 ms 8.9405 ms]
g/4_rescan              time:   [10.557 ms 10.562 ms 10.574 ms]
g/5_rescan_nthash       time:   [12.964 ms 12.980 ms 12.992 ms]
#+end_src


** Making ntHash fast
One reason is that the =next= function on the iterator is [[https://github.com/luizirber/nthash/pull/13][not inlined]].
Updating to the git version brings it down from 12.9ms to 12.7ms, a whole 2%
faster:
#+begin_src diff
-nthash = "0.5.1"
+nthash = { git = "https://github.com/luizirber/nthash" }
#+end_src
But actually the function still isn't inlined anyway. Enabling link-time optimization:
#+begin_src diff
 [profile.release]
 debug = true
+lto = true
#+end_src
brings the runtime down to 11.3ms, but now the build is very slow, and actually
still not all functions are inlined. I'm not quite sure why this is. I tried
inlining the closure:
#+begin_src diff
 kmer_hashes.map(
+    #[inline(always)]
     |h| q.push(h).pos).collect()
 )
#+end_src
but that also didn't help.

Instead, we'll just optimize the nthash library itself. For convenience we first
copy the source to our repo.
First, let's remove the check that all characters are =ACGT=:
#+begin_src diff
 fn h(c: u8) -> u64 {
     let val = H_LOOKUP[c as usize];
-    if val == 1 {
-        panic!("Non-ACGTN nucleotide encountered! {}", c as char)
-    }
     val
 }
#+end_src
Runtime goes to 11.1s.

Next, we sprinkle in some =get_unchecked=:
#+begin_src diff
-let val = H_LOOKUP[c as usize];
 ...
-let seqi = self.seq[i];
+let seqi = unsafe { *self.seq.get_unchecked(i) };
-let seqk = self.seq[i + self.k];
+let seqk = unsafe { *self.seq.get_unchecked(i + self.k) };
#+end_src
and runtime goes down to 10.7s.

It's not exactly clear to me why this is not as fast as Daniel's original
version. Probably it's still because things aren't inlined, and I don't understand why
that's not happening.


* TODO Super-k-mers and canonical kmers

#+print_bibliography:
