#+title: [WIP] Computing minimizers, fast
#+HUGO_SECTION: posts
#+filetags: hpc minimizer wip
#+HUGO_LEVEL_OFFSET: 1
#+OPTIONS: ^:{} num:
#+hugo_front_matter_key_replace: author>authors
#+toc: headlines 3
#+PROPERTY: header-args :eval never-export
#+date: <2024-07-12 Fri>
#+author: Ragnar Groot Koerkamp

In this post, we will develop a fast implementation of random minimizers.

Code for this post is in the =benches/= directory of
[[https://github.com/RagnarGrootKoerkamp/minimizers][github:RagnarGrootKoerkamp/minimizers]]. The unpolished experiments are in the
=playground= branch.

* Random minimizers

Many tools in bioinformatics rely on /k-mers/.
Given a string/text/DNA sequence ~t: &[u8]~ of length $n$, the /k-mers/ of ~t~ are all the
length-$k$ substrings. There are $n-k+1$ of them$.

In order to speed up processing, and since consecutive k-mers are overlapping
and contain redundancy, it is common to /subsample/ the k-mers. This should be
done in a deterministic way, so that similar but slightly distinct sequences
still sample mostly the same k-mers. One such way is /random minimizers/:
1. First, a /window size/ $w$ is chosen, which will guarantee that at least one
   k-mer is sampled every $w$ positions of $t$, to prevent long gaps between
   consecutive k-mers.
2. Then, every k-mer is hashed using a pseudo-random hash function $h$, and in
   each /window/ of $w$ consecutive k-mers (of length $\ell=w+k-1$), the *leftmost* one with the smallest
   hash is selected.[fn::Some foreshadowing here..]
Consecutive windows (that overlap in $w-1$ k-mers) often sample the same k-mer,
so that the total number of sampled k-mers is much smaller than $n-k+1$. In
fact, random minimizers have a /density/ of $2/(w+1)$ and thus sample
approximately two k-mers per window.

My [[../minimizers/minimizers.org][previous post]] on minimizers gives more background on density and other
sampling schemes. One such alternative scheme is the mod-minimizer
[cite:@modmini], that has low density when $w$ is large compared to $k$. That
paper also contains a review of methods and pseudocode for them.

* Setting up benchmarking
** Adding criterion
Before we start our implementation, let's set up a benchmark so we can easily
compare them. We will use [[https://crates.io/crates/criterion][criterion]], together with the [[https://crates.io/crates/cargo-criterion][cargo-criterion]] helper.
First we add =criterion= as a dev-dependency to =Cargo.toml=, and set up a stub
benchmark.

The code can be found at [[https://github.com/RagnarGrootKoerkamp/minimizers][github:RagnarGrootKoerkamp/minimizers]], mostly in the
=benches/= folder. The change below is [[https://github.com/RagnarGrootKoerkamp/minimizers/commit/e758f20e94e7a65c4acd93a5c39a3a9362994fe9][this commit]].

#+begin_src toml
[dev-dependencies]
criterion = "*"

# Do not build the library as a benchmarking target.
[lib]
bench = false

[[bench]]
name = "bench"
harness = false
#+end_src

We add =benches/bench.rs= which looks like this:
#+begin_src rust
use criterion::{criterion_group, criterion_main, Criterion};

/// Generate a random string of length `n` over an alphabet of size `sigma`.
pub fn generate_random_string(n: usize, sigma: usize) -> Vec<u8> {
    (0..n)
        .map(|_| (rand::random::<usize>() % sigma) as u8)
        .collect()
}

/// Benchmark some functions.
fn bench(c: &mut Criterion) {
    let string = generate_random_string(1000000, 256);
    c.bench_function("sum of chars", |b| {
        b.iter(|| string.iter().map(|&c| c as usize).sum::<usize>() as usize);
    });
    c.bench_function("sum of squares", |b| {
        b.iter(|| string.iter().map(|&c| (c as usize).pow(2)).sum::<usize>() as usize);
    });
}

// Criterion setup.
criterion_group!(name = group; config = Criterion::default(); targets = bench);
criterion_main!(group);
#+end_src

If we run this using =cargo criterion=, we get:
#+begin_export html
<script src="https://asciinema.org/a/qXoOOXgGstEoNXyiT3HtzHgBL.js" id="asciicast-qXoOOXgGstEoNXyiT3HtzHgBL" async="true"></script>
#+end_export
Running it again, we get a nice diff with the difference in how long the run
took compared to before.
#+begin_export html
<script src="https://asciinema.org/a/ZuPOAKYv3grH65vJxB8sivgyh.js" id="asciicast-ZuPOAKYv3grH65vJxB8sivgyh" async="true"></script>
#+end_export


** Making it faster
As you can see, this is quite slow. That's for a couple of reasons:
- Before each benchmark, a 3 second warmup is done.
- Each benchmark is 5 seconds.
- After each benchmark, some plots are generated.
- At the end, some HTML reports are generated.
I'm impatient, and all this waiting *really* impacts my iteration time, so let's
make it faster (commit TODO):
- Reduce warmup time to 0.5s.
- Reduce benchmark time to 2s, and only take 10 samples (otherwise slow cases
  are slow).
- Disable plots and html generation.
The first two are done from code (TODO commit):
#+begin_src diff
-    config = Criterion::default();
+    config = Criterion::default()
+        .warm_up_time(Duration::from_millis(500))
+        .measurement_time(Duration::from_millis(2000))
+        .sample_size(10);
#+end_src
The last is best done by adding the ~--plotting-backend disabled~ flag. For
convenience, we add this rule to the =justfile= so we can /just/ do =just
bench=. I'm also adding =quiet= to hide the comparison between runs to simplify presentation.
#+begin_src make
bench:
    cargo criterion --plotting-backend disabled --output-format quiet
#+end_src
#+begin_export html
<script src="https://asciinema.org/a/EQtJkYBEXYzHsEBnhrMLOp29l.js" id="asciicast-EQtJkYBEXYzHsEBnhrMLOp29l" async="true"></script>
#+end_export
Much better.

** A note on CPU frequency

Most consumer CPUs support turboboost to increase the clock frequency for short
periods of time. That's nice, but not good for stable measurements. Thus, I
always pin the frequency of my =i7-10750H= to the default ~2.6GHz~:
#+begin_src sh
sudo cpupower frequency-set --governor powersave -d 2.6GHz -u 2.6GHz
#+end_src
This usually results in quite stable measurements.

Similarly, I have hyper threading disabled.

* Baselines
With that out of the way, let's write some code.
But actually, we should first decide what exactly we are benchmarking.
For now, let's keep things simple: we would like to obtain a vector that
contains for each of the $n-w+1$ windows the absolute position of the minimal k-mer in
that window:
#+begin_src rust
pub trait Minimizer {
    fn minimizers(&self, text: &[u8]) -> Vec<usize>;
}
#+end_src

** Naive brute force

A naive $O(|t| \cdot w)$ algorithm iterates over the
windows, hashes each k-mer, and finds the position of the minimum. (While
hashing each k-mer is technically not $O(1)$, we will assume that $k$ is small
enough compared to the word size that this holds in practice.)

It looks like this:
#+caption: Illustration of the naive algorithm for a sequence of 8 kmers and $w=4$. Hashes of the kmers are shown at the top. This method iterates over all windows, and for each window of length $w$, finds the element with the smallest hash. The orange colour indicates that for each window, we iterate over all hashes. The minimum has a bold outline.
#+attr_html: :class inset
[[file:./naive.svg]]

#+caption: V0: a naive implementation of lexicographic minimizers. (TODO commit)
#+begin_src rust
pub struct V0NaiveLex {
    pub w: usize,
    pub k: usize,
}

impl Minimizer for V0NaiveLex {
    fn minimizers(&self, text: &[u8]) -> Vec<usize> {
        // Iterate over the windows of size l=w+k-1.
        text.windows(self.w + self.k - 1)
            .enumerate()
            // For each window, starting at pos j, find the lexicographically smallest k-mer.
            .map(|(j, window)| {
                j + window
                    .windows(self.k)
                    .enumerate()
                    // min_by_key returns the leftmost minimum.
                    .min_by_key(|(_idx, kmer)| *kmer)
                    .unwrap()
                    .0
            })
            .collect()
    }
}
#+end_src

Let's also already add in two versions that use =fxhash= and =wyhash= already,
two very simple and fast hash functions.
#+caption: V1 and V2. (TODO commit)
#+begin_src diff
V1NaiveFx:
- .min_by_key(|(_idx, kmer)| *kmer)
+ .min_by_key(|(_idx, kmer)| fxhash::hash64(kmer))
V2NaiveWy:
- .min_by_key(|(_idx, kmer)| *kmer)
+ .min_by_key(|(_idx, kmer)| wyhash::wyhash(kmer, 0))
#+end_src

The benchmark now looks like this. I changed to a /benchmark group/ since this
gives slightly more compact output, and tells criterion that the functions belong
together and benchmark the same thing.
#+begin_src rust
fn bench(c: &mut Criterion) {
    let mut g = c.benchmark_group("randmini");
    let text = &generate_random_string(1000000, 256);
    let w = 20;
    let k = 20;

    g.bench_function("0_naive_lex", |b| {
        let m = V0NaiveLex { w, k };
        b.iter(|| m.minimizers(text));
    });
    g.bench_function("1_naive_fx", |b| {
        let m = V1NaiveFx { w, k };
        b.iter(|| m.minimizers(text));
    });
    g.bench_function("2_naive_wy", |b| {
        let m = V2NaiveWy { w, k };
        b.iter(|| m.minimizers(text));
    });
}
#+end_src
First results:
#+begin_src txt
                                 -stddev    mean     +stddev
rnd/0_naive_lex         time:   [87.264 ms 87.285 ms 87.308 ms]
rnd/1_naive_fx          time:   [69.025 ms 69.032 ms 69.039 ms]
rnd/2_naive_wy          time:   [99.193 ms 99.203 ms 99.215 ms]
#+end_src
Observe:
- Each method takes 50-100ms to process 1 million characters. That would be
  50-100s for 1Gbp.
- Measurements between runs are very stable.
- FxHash is fastest. It's just one multiply-add per 8 bytes of kmer.
- WyHash is actually slower than lexicographic comparison in this case!

** Other crates
Let's also compare with some external implementations.
- [[https://crates.io/crates/minimizer-iter][minimizer-iter]] is one baseline implementation. It returns an iterator over all
  distinct minimizers.
  #+begin_src rust
    g.bench_function("ext_minimizer_iter", |b| {
        b.iter(|| {
            minimizer_iter::MinimizerBuilder::<u64>::new()
                .minimizer_size(k)
                .width(w as u16)
                .iter_pos(text)
                .collect_vec()
        });
    });
  #+end_src
- Daniel Liu's [[https://gist.github.com/Daniel-Liu-c0deb0t/7078ebca04569068f15507aa856be6e8][gist]], to which we'll come back in more detail later.

#+begin_src txt
                                 -stddev    mean     +stddev
rnd/0_naive_lex         time:   [87.264 ms 87.285 ms 87.308 ms]
rnd/1_naive_fx          time:   [69.025 ms 69.032 ms 69.039 ms]
rnd/2_naive_wy          time:   [99.193 ms 99.203 ms 99.215 ms]
rnd/ext_minimizer_iter  time:   [19.958 ms 19.960 ms 19.961 ms]
rnd/ext_daniel          time:   [9.2473 ms 9.2487 ms 9.2507 ms]
#+end_src
We see that =minimizer-iter= is quite a bit faster than our methods, and
Daniel's code is another two times faster. So let's get to work :)

* Sliding window minimum
After hashing all k-mers, we basically have a sequence of $n-k+1$ pseudo-random
integers, and we would like to find the position of the leftmost minimum in each
window of $w$ of those integers. Thus, we can model the problem using the
following trait:
#+caption: Trait for the sliding window minimum problem.
#+begin_src rust
pub trait SlidingMin<V> {
    /// Initialize a new datastructure with window size `w`.
    fn new(w: usize) -> Self;
    /// Push a new value, starting at position 0.
    /// Return the pos and value of the minimum of the last w elements.
    fn push(&mut self, val: V) -> Elem<V>;
}

#[derive(Clone, Copy)]
pub struct Elem<Val> {
    pub pos: usize,
    pub val: Val,
}
#+end_src

** The queue

Your first idea may be to simply keep a rolling prefix-minimum that tracks the
lowest hash/value seen so far. When $w=3$ and the hashes are $[10,9,8,7,6,...]$,
the rolling prefix minimum is exactly also the minimum of each window of size
$3$ (like $[8,7,6]$).
But alas, this won't work for increasing sequences:
when the hashes are $[1,2,3,4,5,...]$ and the window shifts to $[2,3,4]$, the $1$ at index $0$ is not a minimum of that
window anymore, and the minimum goes up to $2$.

As the window slides right, each time we see a new value that's smaller than
everything in the window, we can basically 'forget' about all existing values
and just keep that one in memory. Otherwise, we can still forget about all values
in the window larger than the value that is shifted in.

This is formalized by using a *queue* of increasing values in the window. At
each step, the minimum of the window is the value at the front of the queue. It's
probably best explained using an example.

#+caption: The queue method: In each step, we add the new hash value to the right/back of the queue that is shown in blue. Any preceding values in the queue that are larger are dropped (red). The smallest element of the window is on the left/front of the queue. In the second to last window, the leading $3$ is dropped from the queue as well because it falls out of the window.
#+attr_html: :class inset
[[file:./queue.svg]]

In order to know when to /pop/ elements from the front, we don't just store
values in this queue, but also the positions of all elements in the original text.

In code, it looks like this:
#+caption: A simple 'monotone queue' implementation. (TODO commit)
#+begin_src rust
pub struct MonotoneQueue<Val: Ord> {
    w: usize,
    pos: usize,
    /// A queue of (pos, val) objects.
    /// Both pos and val values are always increasing, so that the smallest
    /// value is always at the front.
    q: VecDeque<Elem<Val>>,
}

impl<Val: Ord + Copy> SlidingMin<Val> for MonotoneQueue<Val> {
    fn new(w: usize) -> Self {
        assert!(w > 0);
        Self {
            w,
            pos: 0,
            q: VecDeque::new(),
        }
    }

    fn push(&mut self, val: Val) -> Elem<Val> {
        // Strictly larger preceding `k` are removed, so that the queue remains
        // non-decreasing.
        while let Some(back) = self.q.back() {
            if back.val > val {
                self.q.pop_back();
            } else {
                break;
            }
        }
        self.q.push_back(Elem { pos: self.pos, val });
        let front = self.q.front().unwrap(); // Safe, because we just pushed.
        if self.pos - front.pos >= self.w {
            self.q.pop_front();
        }
        self.pos += 1;
        *self.q.front().unwrap() // Safe, because w > 0.
    }
}
#+end_src

*Analysis:* Since each element is pushed once and popped once, each call to
=push= takes amortized constant $O(1)$ time!

** A minimizer using the queue
It's now trivial to implement an $O(n)$ minimizer scheme using this queue:
#+caption: v3: using a queue for sliding window minimum.
#+begin_src rust
pub struct V3Queue {
    pub w: usize,
    pub k: usize,
}

impl Minimizer for V3Queue {
    fn minimizers(&self, text: &[u8]) -> Vec<usize> {
        let mut q = MonotoneQueue::new(self.w);
        let mut kmers = text.windows(self.k);
        // Inset the first w-1 k-mers, that do not yet form a full window.
        for kmer in kmers.by_ref().take(self.w - 1) {
            q.push(fxhash::hash(kmer));
        }
        kmers.map(|kmer| q.push(fxhash::hash(kmer)).pos).collect()
    }
}
#+end_src

How does it do?
#+begin_src txt
                                 -stddev    mean     +stddev
rnd/0_naive_lex         time:   [87.309 ms 87.315 ms 87.321 ms]
rnd/1_naive_fx          time:   [69.089 ms 69.121 ms 69.147 ms]
rnd/2_naive_wy          time:   [96.830 ms 96.842 ms 96.854 ms]
rnd/ext_minimizer_iter  time:   [20.001 ms 20.007 ms 20.012 ms]
rnd/ext_daniel          time:   [9.2662 ms 9.2696 ms 9.2735 ms]
rnd/3_queue             time:   [23.952 ms 24.512 ms 25.095 ms]
#+end_src

Great! Already very close to the =minimizer-iter= crate, and we didn't even
write much code yet.
From now on, I'll leave out the naive $O(wn)$ implementations.

* Re-scanning: Away with the queue
We would like to find some middle ground between keeping the queue of possible
minima and rescanning every window. A first idea is this: Once we find the
minimizer of some window, we can jump to the first window after that position to
find the next minimizer. This
finds all distinct minimizers, although it does not compute the minimizer for each window individually.

#+caption: Each time a minimizer (green) is found, jump to the window starting right after to find the next minimizer. (The leading $4$ and final $8$ are minimizers of a prefix/suffix of the sequence.)
#+attr_html: :class inset
[[file:./jump.svg]]

Since the expected distance between random minimizers is $(w+1)/2$, we expect to
scan each position just under two times.

I learned of an improved method via Daniel Liu's [[https://gist.github.com/Daniel-Liu-c0deb0t/7078ebca04569068f15507aa856be6e8][gist]] for robust winnowing,
but I believe it is folklore[fn::Citation needed, both for it being folklore and
for the original idea.].
This is a slightly improved variant of the method above, where we don't consider
all windows independently, but still keep track of a rolling minimum.
Only when 'the minimum falls out of the window' a re-scan is done.

#+caption: Keep a rolling minimum (green) of the lowest hash seen so far by comparing each new element (orange) to the minimum. Only when the minimum falls outside the window, recompute the minimum for the entire window (yellow row).
#+attr_html: :class inset
[[file:./rescan.svg]]

TODO: I believe this only does $\approx 1.5$ comparisons per element instead of $2$.

Thus, we keep
a cyclic buffer of the last $w$ values, and scan it as needed. One point of attention
is that we need the /leftmost/ minimal value, but as the buffer is cyclic, that
is not the first minimum in the buffer. Thus, we partition the scan into two
parts, and prefer minima in the second (older) half.

#+caption: =Rescan= implementation of =SlidingMin=.
#+begin_src rust
pub struct Rescan<Val: Ord> {
    w: usize,
    /// Position of next element.
    pos: usize,
    /// Index in `vals` of next element.
    idx: usize,
    /// Position of the smallest element in the last window.
    min_pos: usize,
    /// Value of the smallest element in the last window.
    min_val: Val,
    vals: Vec<Val>,
}

impl<Val: Ord + Copy + Max> SlidingMin<Val> for Rescan<Val> {
    #[inline(always)]
    fn new(w: usize) -> Self {
        assert!(w > 0);
        Self {
            w,
            pos: 0,
            idx: 0,
            min_pos: 0,
            min_val: Val::MAX,
            vals: vec![Val::MAX; w],
        }
    }

    #[inline(always)]
    fn push(&mut self, val: Val) -> Elem<Val> {
        self.vals[self.idx] = val;
        if val < self.min_val {
            self.min_val = val;
            self.min_pos = self.pos;
        }
        if self.pos - self.min_pos == self.w {
            // Find the position of the minimum, preferring older elements that
            // come *after* self.idx.
            let p1 = self.vals[self.idx + 1..].iter().position_min();
            let p2 = self.vals[..=self.idx].iter().position_min().unwrap();
            (self.min_val, self.min_pos) = if let Some(p1) = p1 {
                let p1 = self.idx + 1 + p1;
                if self.vals[p1] <= self.vals[p2] {
                    (self.vals[p1], self.pos - self.idx + p1 - self.w)
                } else {
                    (self.vals[p2], self.pos - self.idx + p2)
                }
            } else {
                (self.vals[p2], self.pos - self.idx + p2)
            };
        }
        self.pos += 1;
        self.idx += 1;
        if self.idx == self.w {
            self.idx = 0;
        }

        return Elem {
            pos: self.min_pos,
            val: self.min_val,
        };
    }
}
#+end_src

As before, the corresponding minimizer scheme is trivial to implement:
#+caption: v4: rescan
#+begin_src diff
-pub struct V3Queue  {..}
+pub struct V4Rescan {..}
 ...
-    let mut q = MonotoneQueue::new(self.w);
+    let mut q = Rescan::new(self.w);
#+end_src
The result is *fast*: almost twice as fast as the previous best! Also close to
Daniel's version, but not quite there yet.
#+begin_src txt
rnd/ext_minimizer_iter  time:   [26.950 ms 27.139 ms 27.399 ms]
rnd/ext_daniel          time:   [9.2476 ms 9.2497 ms 9.2532 ms]
rnd/3a_queue            time:   [23.686 ms 23.692 ms 23.699 ms]
rnd/3b_inlined_queue    time:   [22.620 ms 22.631 ms 22.641 ms]
rnd/4_rescan            time:   [10.876 ms 10.882 ms 10.894 ms]
#+end_src


* TODO Sliding window: away with branch-misses

#+caption: Split algorithm: after each $w$ rows, we compute and store reverse suffix-minima (blue, bottom left) of the preceding slice of $w$ values and use those together with the forward rolling minimum to find the minimum of each window. The memory after each iteration is shown on the right, with updated values in orange.
#+attr_html: :class inset
[[file:./split.svg]]
* TODO NtHash: a rolling kmer hash
One problem with =fxhash= and =wyhash= is that they hash strings of arbitrary
length, and hence generate a lot of code to handle all length modulo $8$ efficiently.
In practice, we've only been using $k=21$ so far, but this still requires
iterating over three $8$ byte words. Instead, a rolling hash only has to handle
the first and last character of each string, regardless of the length of the
string. We will use ntHash [cite:@nthash] (see also [[../nthash.org][this post]]). This assigns a random value $h(c)$ to
each DNA character $c$, and computes the hash of a string as $x$ as
\begin{equation}
h(x) = \bigoplus_{i=0}^{k-1} rot^i(h(x_i)),
\end{equation}
where $rot^i$ does a $64$-bit rotate, and $\oplus$ is the xor operation.
This can be efficiently computed incrementally by rotating the hash $1$, then
xor'ing in $h(x_k)$, and then xor'ing out $rot^{k}(h(x_0))$.

Using the =nthash= [[https://crates.io/crates/nthash][crate]], the implementation is simple:
#+begin_src rust
pub struct V5RescanNtHash {
    pub w: usize,
    pub k: usize,
}

impl Minimizer for V5RescanNtHash {
    fn minimizers(&self, text: &[u8]) -> Vec<usize> {
        let mut q = Rescan::new(self.w);
        let mut kmer_hashes = nthash::NtHashForwardIterator::new(text, self.k).unwrap();
        for h in kmer_hashes.by_ref().take(self.w - 1) {
            q.push(h);
        }
        kmer_hashes.map(|h| q.push(h).pos).collect()
    }
}
#+end_src

Unfortunately, it's slower than before:
#+begin_src txt
g/ext_daniel            time:   [8.9199 ms 8.9300 ms 8.9405 ms]
g/4_rescan              time:   [10.557 ms 10.562 ms 10.574 ms]
g/5_rescan_nthash       time:   [12.964 ms 12.980 ms 12.992 ms]
#+end_src

* TODO Optimizing what we have

** Optimizing the queue
We're already close to the reference implementation, but not quite there yet.
Let's do some profiling. For this, we can pass ~--profile-time 5~ to
~criterion~, so that instead of the usual benchmarking, it just runs the
selected benchmarks for 5 seconds. We start with a flamegraph of the v3 method above.
#+begin_src just
flame test='':
    cargo flamegraph --bench bench --open -- --bench --profile-time 2 {{test}}
#+end_src

#+caption: A flamegraph made using =just flame 3_queue= showing that some time is spent in the warm-up, some in main loop, and that most time is spent in the =push= function.
#+attr_html: :class inset
[[./3_flame.svg][file:3_flame.svg]]

This is not yet super insightful though. It's pretty much expected that most
time is in the =push= function anyway. Let's get some more statistics using
=perf stat=:
#+begin_src just
stat test='':
    cargo build --profile bench --benches
    perf stat -d cargo criterion -- --profile-time 2 {{test}}
#+end_src
#+begin_src txt
          2,380.66 msec task-clock:u                     #    1.005 CPUs utilized
                 0      context-switches:u               #    0.000 /sec
                 0      cpu-migrations:u                 #    0.000 /sec
            22,675      page-faults:u                    #    9.525 K/sec
     5,873,141,947      cycles:u                         #    2.467 GHz
    13,624,513,378      instructions:u                   #    2.32  insn per cycle
     1,893,102,104      branches:u                       #  795.201 M/sec
        77,266,703      branch-misses:u                  #    4.08% of all branches
     2,960,654,139      L1-dcache-loads:u                #    1.244 G/sec
        19,781,179      L1-dcache-load-misses:u          #    0.67% of all L1-dcache accesses
         1,659,216      LLC-loads:u                      #  696.957 K/sec
           269,546      LLC-load-misses:u                #   16.25% of all LL-cache accesses
#+end_src

There is still nothing that stands out as very bad. 2.3 instructions per cycle
is not great, but still reasonable. (It can go up to 4 for my processor, and
above 3 is good usually.) Maybe $4\%$ of branch misses is a problem though.
Let's dive deeper and look at =perf record=:
#+begin_src just
perf test='':
    cargo build --profile bench --benches
    perf record cargo criterion -- --profile-time 2 {{test}}
    perf report
#+end_src
#+caption: =just perf 3_queue=
#+begin_src txt
  65.79%  <bench::randmini::sliding_min::MonotoneQueue<Val> as bench::randmini::sliding_min::SlidingMin<Val>>::push
  22.15%  <core::iter::adapters::map::Map<I,F> as core::iter::traits::iterator::Iterator>::fold
  ...
#+end_src

Now the problem is clear! The =push= function is not inlined. Let's fix that.
(TODO commit)
#+begin_src diff
+#[inline(always)]
 fn new(w: usize) -> Self {

+#[inline(always)]
 fn push(&mut self, val: Val) -> Elem<Val> {
#+end_src
#+begin_src txt
rnd/ext_minimizer_iter  time:   [20.023 ms 20.092 ms 20.206 ms]
rnd/ext_daniel          time:   [9.2619 ms 9.4479 ms 9.6512 ms]
rnd/3a_queue            time:   [23.936 ms 23.948 ms 23.964 ms]
rnd/3b_inlined_queue    time:   [22.786 ms 22.874 ms 22.988 ms]
#+end_src

A well, forgive me my optimism. Either way, this is decently close to the baseline
version. Let's look in slightly more detail at the =perf report=:

#+begin_src asm
  1.02 │2f0:   lea    (%rdx,%rax,1),%rdi  ; start of the while pop-back loop.
  0.58 │       cmp    %rcx,%rdi
  1.41 │       mov    $0x0,%edi
  0.67 │       cmovae %rcx,%rdi
  0.35 │       shl    $0x4,%rdi
  0.59 │       mov    %rsi,%r8
  1.28 │       sub    %rdi,%r8
  1.74 │    ┌──cmp    %r12,(%r8)              ; Is back > val?
  1.91 │    ├──jbe    321                     ; -> NO, pop it.
 *9.08*│    │  dec    %rax
  3.41 │    │  mov    %rax,0x18(%rbx)
  0.35 │    │  add    $0xfffffffffffffff0,%rsi
  0.10 │    │  test   %rax,%rax
  0.33 │    │↑ jne    2f0                 ; jumps back to the top
  0.28 │    │  xor    %eax,%eax
       │    │self.q.push_back(Elem { pos: self.pos, val });
*12.02*│321:└─→mov    0x28(%rbx),%r13         ; -> YES, stop.
#+end_src
We can see that a lot of time, 21% of the total, is spent on the two
instructions right after the branch. Indeed, this branch checks whether the
previous element is larger than the current one, and that is basically 50-50
random, as bad as it can be.

Thus, we would like a less branchy and more predictable method for sliding windows.

** Optimizing the rescan

If we look at the generated assembly, we can see it is quite branchy. In an
attempt to fix this, we can replace all array indexing by =unsafe {v.get_unchecked(idx) }=.
But it turns out doing so gives only negligible performance gains.

One other thing that stands out in the =perf= is this:
#+begin_src asm
       │    ┌──cmp    0x38(%r15),%rdx
       │    │if val < self.min_val {
       │    ├──jae    300
       │    │self.min_val = val;
  5.61 │    │  mov    %rdx,0x38(%r15)
       │    │self.min_pos = self.pos;
       │    │  mov    0x20(%r15),%rdx
  0.38 │    │  mov    %rdx,0x30(%r15)
#+end_src
5% of the time is spend on the case where the =self.min_val= is updated,
because of branch mispredictions. It would be better to avoid the branch by
using =cmov= instead:
#+begin_src diff
-            if val < self.min_val {
-                self.min_val = val;
-                self.min_pos = self.pos;
-            }
+            (self.min_val, self.min_pos) = if val < self.min_val {
+                (val, self.pos)
+            } else {
+                (self.min_val, self.min_pos)
+            };
#+end_src
This indeed lowers the runtime by 3% to =10.5s=.

Now, 5% of time is spent on the branch miss when the minimum falls out of the
window, since this happens at random times. But first, we improve the hash function.

** Making ntHash fast
One reason is that the =next= function on the iterator is [[https://github.com/luizirber/nthash/pull/13][not inlined]].
Updating to the git version brings it down from 12.9ms to 12.7ms, a whole 2%
faster:
#+begin_src diff
-nthash = "0.5.1"
+nthash = { git = "https://github.com/luizirber/nthash" }
#+end_src
But actually the function still isn't inlined anyway. Enabling link-time optimization:
#+begin_src diff
 [profile.release]
 debug = true
+lto = true
#+end_src
brings the runtime down to 11.3ms, but now the build is very slow, and actually
still not all functions are inlined. I'm not quite sure why this is. I tried
inlining the closure:
#+begin_src diff
 kmer_hashes.map(
+    #[inline(always)]
     |h| q.push(h).pos).collect()
 )
#+end_src
but that also didn't help.

Instead, we'll just optimize the nthash library itself. For convenience we first
copy the source to our repo.
First, let's remove the check that all characters are =ACGT=:
#+begin_src diff
 fn h(c: u8) -> u64 {
     let val = H_LOOKUP[c as usize];
-    if val == 1 {
-        panic!("Non-ACGTN nucleotide encountered! {}", c as char)
-    }
     val
 }
#+end_src
Runtime goes to 11.1s.

Next, we sprinkle in some =get_unchecked=:
#+begin_src diff
-let val = H_LOOKUP[c as usize];
 ...
-let seqi = self.seq[i];
+let seqi = unsafe { *self.seq.get_unchecked(i) };
-let seqk = self.seq[i + self.k];
+let seqk = unsafe { *self.seq.get_unchecked(i + self.k) };
#+end_src
and runtime goes down to 10.7s.

It's not exactly clear to me why this is not as fast as Daniel's original
version. Probably it's still because things aren't inlined, and I don't understand why
that's not happening.
* TODO SIMD, SIMD everywhere


#+print_bibliography:
