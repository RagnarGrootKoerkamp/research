#+title: [WIP] Computing random minimizers, fast
#+filetags: @results @walkthrough hpc minimizers nthash wip
#+HUGO_LEVEL_OFFSET: 0
#+OPTIONS: ^:{} num:2 H:4
#+hugo_front_matter_key_replace: author>authors
#+toc: headlines 3
#+PROPERTY: header-args :eval never-export
#+hugo_paired_shortcodes: notice details
#+date: <2024-07-12 Fri>

In this post, we will develop a fast implementation of random minimizers.

The goals here are:
- a library for fast random minimizers,
- explaining the final code, and the design decisions leading to it,
- walking you through the development process, hopefully teaching how to do this yourself.

Code for this post is in the =benches/= directory of
[[https://github.com/RagnarGrootKoerkamp/minimizers][github:RagnarGrootKoerkamp/minimizers]]. The unpolished experiments are in the
=playground= branch.
Most code snippets are directly included from the repository. I have tried to
write clean and readable code that follows the order of this post. Still,
there will be small parts that only become useful later on.

Contents[fn::Read this on a wide screen to see the table of contents on the
left, and footnotes on the right.]:
- [[*Random minimizers][Background]]
- [[*Algorithms][Overview of algorithms]]
- TODO

* Random minimizers
Many tools in bioinformatics rely on /k-mers/.
Given a string/text/DNA sequence ~t: &[u8]~ of length $|t|=n$, the /k-mers/ of ~t~ are all the
length-$k$ substrings. There are $n-k+1$ of them.

In order to speed up processing, and since consecutive k-mers are overlapping
and contain redundancy, it is common to /subsample/ the k-mers. This should be
done in a deterministic way, so that similar but slightly distinct text
still mostly sample the same k-mers. One such way is /random minimizers/,
introduced in parallel by [cite/t:@winnowing] and [cite/t:@minimizers]:
1. First, a /window size/ $w$ is chosen, which will guarantee that at least one
   k-mer is sampled every $w$ positions of $t$, to prevent long gaps between
   consecutive k-mers.
2. Then, every k-mer is hashed using a pseudo-random hash function $h$, and in
   each /window/ of $w$ consecutive k-mers (of total length $\ell=w+k-1$), the /leftmost/[fn::Some foreshadowing here..] one with the smallest
   hash is selected.
Consecutive windows (that overlap in $w-1$ k-mers) often sample the same k-mer,
so that the total number of sampled k-mers is much smaller than $n-k+1$. In
fact, random minimizers have a /density/ of $2/(w+1)$ and thus sample
approximately two k-mers per window.

My [[../mod-minimizers/mod-minimizers.org][previous post]] on minimizers gives more background on density and other
sampling schemes. One such alternative scheme is the mod-minimizer
[cite:@modmini], that has low density when $k$ is large compared to $w$. That
paper also contains a review of methods and pseudocode for them.
In this post, I will focus on the most widely used random minimizers.

* Algorithms
** Problem statement
Before going into algorithms to compute random minimizers, let's first precisely state the
problem itself. In fact, we'll state three versions of the problem, that each
include slightly more additional information in the output.

*** Problem A: Only the set of minimizers
The most basic form of computing minimizers is to simply return the list of
minimizer kmers and their positions. This information is sufficient to /sketch/
the text.

#+name: problem-a
#+caption: A trait that defines Problem A and returns the set of kmers of the input.
#+begin_src rust
pub trait Minimizer {
    /// Problem A: The absolute positions of all minimizers in the text.
    fn minimizer_positions(&self, text: &[u8]) -> Vec<usize>;
}
#+end_src

*** Problem B: The minimizer of each window
When building datastructures on top of a sequence, such as SSHash
[cite:@sshash], just the set of k-mers is not sufficient. Each window should be
mapped to its minimizer, but there could be multiple minimizers in a single
window. Thus, in this version of the problem, we return the minimizer of each window.

Note that while in [[*Problem A: Only the set of minimizers][Problem A]] we return one result per minimizer, here we return
one result per window (roughly, per character). This difference can have
implications for the optimal algorithm implementation, since it's not unlikely
that the main loop of the optimal solution has /exactly/ one iteration per
returned element.

#+name: problem-b
#+caption: A trait for Problem B that returns the minimizer of each window.
#+begin_src rust
pub trait Minimizer {
    ...

    /// Problem B: For each window, the absolute position in the text of its minimizer.
    fn window_minimizers(&self, text: &[u8]) -> Vec<usize>;
}
#+end_src

*** Problem C: Super-k-mers
We can implement SSHash and other indexing datastructures using the output of [[*Problem B: The minimizer of each window][Problem B]],
but in most such applications, it is probably more natural to directly iterate over the /super-k-mers/: the longest substrings such that all contained windows have the same
minimizer. This can easily be obtained from the output of Problem B by
/grouping/ consecutive windows that share the minimal k-mer.

Note that here the result is again an iterator over /minimizers/, like in
[[*Problem A: Only the set of minimizers][Problem A]] rather than /windows/ as in [[*Problem B: The minimizer of each window][Problem B]].

#+name: problem-c
#+caption: A trait that defines Problem C and returns the set of super-k-mers.
#+begin_src rust
struct SuperKmer {
    /// The absolute position in the text where this super-k-mer starts.
    /// The end of the super-k-mer can be inferred from the start of the next super-k-mer.
    start_pos: usize,
    /// The absolute position in the text of the minimizer of this super-k-mer.
    minimizer_pos: usize,
}

pub trait Minimizer {
    ...

    /// Problem C: The super-k-mers of the text.
    fn super_kmers(&self, text: &[u8]) -> Vec<SuperKmer>;
}
#+end_src

*** Which problem to solve
In the remainder, we mostly focus on [[*Problem B: The minimizer of each window][Problem B]], since that is what I initially
started with. However, some methods more naturally lean themselves to solve the
other variants. We will benchmark each method on whichever of the problems it
runs fastest, and postpone optimizing a solution for each specific problem until
later.

Most likely though, solving [[*Problem C: Super-k-mers][Problem C]] will yield the most useful tool for end
users. I plan to revisit this at the end of this post.

Regardless, for convenience we can implement solutions to A and C in terms of
 B.[fn::Consider not skipping the code blocks. I made quite some effort to make
 them clean and readable. Most code is fairly straightforward assuming you're
 comfortable reading Rust code.]
#+caption: Implementing =minimizer_positions= and =super_kmers= in terms of =window_minimizers=.
#+include: "code/blog/minimizer.rs" src rust :lines "11-"

*** Canonical k-mers
In actual bioinformatics applications, it is common to consider /canonical
k-mers/ and corresponding /canonical minimizers/. These are defined such that
the canonical k-mer of a string and its reverse complement are the same.
This will significantly impact the complexity of our code, and hence we also
postpone this for later.


** The naive algorithm

The naive $O(n \cdot w)$ method simply finds the minimum of each window independently.

It looks like this:[fn:legend:*Legend*:\\
  @@html:<span style="color:orange">Orange</span>@@: processed state,\\
  @@html:<span style="border:1px black solid">Bold outline</span>@@: minimum of
  the prefix/suffix,\\
  *Bold character*: minimum of the window,\\
  @@html:<span style="color:blue">Blue</span>@@: state in memory,\\
  @@html:<span style="color:red">Red</span>@@: state removed from the queue,\\
  @@html:<span style="color:green">Green</span>@@: running prefix minimum.
  ]
#+caption: Illustration of the naive algorithm for a text of 8 kmers and window size /w=4/. Hashes of the kmers are shown at the top. This method iterates over all windows, and for each window, finds the element with the smallest hash. The orange colour indicates that for each window, all hashes are checked. The minimum is shown in bold.
#+attr_html: :class inset
[[file:./naive.svg]]

In code, it looks like this.

#+caption: An implementation of the naive algorithm. Note that hashes are recomputed for each window.
#+include: "code/blog/naive.rs" src rust :lines "3-29"

Note that we assume that hashing each k-mer takes constant time. This is
reasonable in practice, since most hash functions hash many characters at once.

*** Performance characteristics
:PROPERTIES:
:CUSTOM_ID: naive-performance
:END:

I was planning to show some preliminary performance numbers here, to give some
intuition about the relative performance of this and upcoming methods.
Unfortunately though (and not completely surprising), performance depends
heavily on small details of how the code is written. I could provide a
comparison between the methods as-is, but the numbers would be hard to interpret.
Instead, let's consider some high level expectations of this algorithm for now,
and postpone the quantitive comparison until later.

Since there are no =if= statements, the code should be highly predictable. Thus,
we expect:
- few branch misses,
- high instructions per cycle,
- and generally running times linear in $w$.

** Rephrasing as sliding window minimum
After hashing all k-mers, we basically have a sequence of $n-k+1$ pseudo-random
integers. We would like to find the position of the leftmost minimum in each
window of $w$ of those integers.

Thus, the problem of finding random minimizers can be reformulated in terms of the sliding
window minima.
We can model this problem using the following trait:
#+caption: Trait for the sliding window minimum problem.
#+include: "code/blog/sliding_min.rs" src rust :lines "2-16"

A first implementation that mirrors the =Naive= implementation above could simply store the last $w$ hashes in a =RingBuffer=,
and in each step recompute the minimum. The boiler plate for the ring buffer is simple:
#+caption: A simple ring buffer.
#+include: "code/blog/ringbuf.rs" src rust :lines "3-36"

And then a sliding window implementation also becomes trivial:

#+caption: The =Buffered= implementation of =SlidingMin= that buffers the last $w$ values and recomputes the minimum on each =push=.
#+include: "code/blog/naive.rs" src rust :lines "30-"

Using this trait, we can clean up[fn::The =hash_kmers= function here is part of
the =Hasher= trait that can be found in [[hasher]].] the implementation of the =Naive= minimizer from
before[fn::In fact, the =Buffered= implementation is slightly more efficient
since it stores hashes instead of recomputing them $w$ times.]. In fact, we can now generically implement a minimizer scheme using any
implementation of the sliding window algorithm.

#+name: queue-wrapper
#+caption: A generic minimizer algorithm using any solution to the sliding window minimum problem, and the corresponding =Buffered= minimizer.
#+include: "code/blog/sliding_min.rs" src rust :lines "46-"

** The queue
Let's try to improve this somewhat inefficient $O(n\cdot w)$ algorithm.  A first
idea is to simply keep a rolling prefix-minimum that tracks the lowest
value seen so far. For example, when $w=3$ and the hashes are
$[10,9,8,7,6,...]$, the rolling prefix minimum is exactly also the minimum of
each window.  But alas, this won't work for
increasing sequences: when the hashes are $[1,2,3,4,5,...]$ and the window
shifts from $[1,2,3]$ to $[2,3,4]$, the prefix minimum is $1$ (at index $0$),
but it is not a minimum of the new window anymore, which instead goes up to $2$.

Nevertheless, as the window slides right, each time we see a new value that's smaller than
everything in the window, we can basically 'forget' about all existing values
and just keep the new smaller value in memory. Otherwise, we can still forget
about all values in the window larger than the value that is shifted in.

This is formalized by using a *queue* of non-decreasing values in the window.
More precisely, each queue element will be smaller than all values in the window
coming after it.
At each step, the minimum of the window is the value at the front of the queue.
Let's look at our example again.[fn:legend]

#+name: fig-queue
#+caption: The queue method: In each step, we push the new value (orange) to the right/back of the queue. States stored in the queue are blue. Any preceding values in the queue that are larger than the new element are dropped (red). The smallest element of the window is on the left/front of the queue. In the second to last window, the leading $3$ is dropped from the front queue as well because it falls out of the window.
#+attr_html: :class inset
[[file:./queue.svg]]

We see that there are two reasons elements can be removed from the queue:
1. a smaller element is pushed,
2. the window shifts so far that the leftmost/smallest element falls outside it.
To handle this second case, we don't just store values in this queue, but also
their position in the original text, so that we can pop them when needed.

In code, it looks like this:
#+caption: An implementaion of the =MonotoneQueue= and the corresponding =QueueMinimizer=.
#+include: "code/blog/queue.rs" src rust :lines "2-28"

*** Performance characteristics
:PROPERTIES:
:CUSTOM_ID: queue-performance
:END:
Since each element is pushed once and popped once, each call to
=push= takes amortized constant $O(1)$ time, so that the total runtime is
$O(n)$.
We may expect a $w=11$ times speedup over the buffered method, but most likely
this will not be the case.
This is due to the high number of unpredictable branches,
which hurts pipelining/instruction level parallelism (ILP). Thus, while the
complexity of the =Queue= algorithm is good, its practical performance may not be optimal.

** Jumping: Away with the queue
While the queue algorithm is nice in theory, in practice it is not ideal due to
the branch-misses.
Thus, we would like to find some middle ground between keeping this queue
and the naive approach of rescanning every window. A first idea is this: Once we find the
minimizer of some window, we can jump to the first window after that position to
find the next minimizer. This
finds all distinct minimizers, although it does not compute the minimizer for
each window individually. Thus, it solves [[*Problem A: Only the set of
minimizers][Problem A]] instead of [[*Problem B: The minimizer of each
window][Problem B]].[fn:legend]

#+caption: Each time a minimizer (bold) is found, jump to the window starting right after to find the next minimizer. The last iteration is special and checks if possibly one more minimizer has to be added, which is not the case here.
#+attr_html: :class inset
[[file:./jump.svg]]

Since the expected distance between random minimizers is $(w+1)/2$, we expect to
scan each position just under two times.

#+name: jumping-code
#+caption: Solving Problem A by jumping to the window after the previous minimizer. Note that this first computes all hashes in a separate pass.
#+include: "code/blog/jumping.rs" src rust :lines "2-"

*** Performance characteristics
:PROPERTIES:
:CUSTOM_ID: queue-performance
:END:

One benefit of this method is that it is completely branchless. Each iteration of
the loop corresponds to exactly one output. The only unpredictable step is which
element of a window is the minimizer, since that determines which is the next
window to scan. Thus, even though it is branchless, consecutive iterations
depend on each other and ILP (instruction level parallelism, aka pipelining) may only have a limited effect.

** Re-scan
I learned of an improved method via [[https://twitter.com/daniel_c0deb0t][Daniel Liu]]'s [[https://gist.github.com/Daniel-Liu-c0deb0t/7078ebca04569068f15507aa856be6e8][gist]] for winnowing,
but I believe the method is folklore[fn::Citation needed, both for it being folklore and
for the original idea.].
This is a slightly improved variant of the method above. Above, we do a new scan
for each new minimizer, but it turns out this can sometimes be avoided.
Now, we will only re-scan when a previous minimum falls outside the window, and
the minimum of the window goes up.[fn:legend]


#+caption: Keep a rolling minimum (green) of the lowest hash seen so far by comparing each new element (orange) to the minimum. Only when the minimum falls outside the window, recompute the minimum for the entire window (second to last yellow row).
#+attr_html: :class inset
[[file:./rescan.svg]]

Thus, we keep
a ring buffer of the last $w$ values, and scan it as needed. One point of attention
is that we need the /leftmost/ minimal value, but since the buffer is cyclic,
the leftmost element in the buffer is not necessarily the leftmost one in the
original text. Thus, we don't only store elements but also their positions,
and break ties by position.

#+caption: =Rescan= implementation of =SlidingMin=.
#+include: "code/blog/rescan.rs" src rust :lines "2-"

*** Performance characteristics
:PROPERTIES:
:CUSTOM_ID: rescan-performance
:END:
This method should have much fewer branch-misses than the queue, since we rescan
a full window of size $w$ at a time. Nevertheless, these re-scans occur at
somewhat random moments, so some branch-misses around this are still to be expected.

** Split windows
In an ideal world, we would use a fully predictable algorithm, i.e., an
algorithm without any data-dependent branches/if-statements.[fn::I'm not
counting the for loop that iterates up to the variable length of the string as a
branch miss, because that will only be a single branch-miss at the end, not
$O(1)$ per iteration.]

With some [[https://www.google.com/search?q=sliding%20window%20minimum][Googling]][fn::It's the third result.] I found [[https://codeforces.com/blog/entry/71687][this codeforces blog]] on
computing sliding window minima. In the post, the idea is presented using two
queues. My explanation below goes in a slightly different direction, but in the
end it comes down to the same.

*Aside: Accumulating non-invertible functions.*
So, as we saw, most methods keep some form of rolling prefix minimum that is
occasionally 'reset', since 'old' small elements should not affect the current
window.
This all has to do with the fact that the =min= operation is not /invertible/:
knowing only the minimum of a set $S$ is not sufficient to know the minimum of
$S - \{x\}$.
This is unlike for example addition: sliding window sums are easy by just adding
the new value to the sum and subtracting the value that falls out of the window.

This distinction is also exactly the difference between /segment trees/ and
/Fenwick trees/[fn::Feel free to skip this paragraph in case you are not
familiar with segment and Fenwick trees.]: a Fenwick tree computes some function
$f$ on a range $[l, r)$ by removing $f(a_0, \dots, a_{l-1})$ from $f(a_0, \dots, a_{r-1})$, by
decomposing both ranges into intervals with power-of-$2$ lengths corresponding
to the binary representation of their length. But this only works if $f$ is
invertible. If that is not the case, a segment tree must be used, which
partitions the interval $[l, r)$ itself, without going 'outside' it.

*The algorithm.*
Based on this, we would also like to partition our windows such that we can
compute their minimum as a minimum over the different parts. It turns out that
this is possible, and in a very clean way as well!

First, we conceptually chunk the input text into parts with length $w$, as
shown in the top row of [[fig-split]] for $w=4$. Then, each window of length $w$
either
exactly matches such a chunk, or overlaps with two consecutive chunks. In the latter
case, it covers a suffix of chunk $i$ and a prefix of chunk $i+1$. Thus, to
compute the minimum for all windows, we need to compute the prefix and suffix
minima of all chunks. We could do that as a separate pass over the data in a
pre-processing step, but it can also be done on the fly:

1. Each time a new kmer is processed, its hash is written to a size-$w$ buffer.
   (The orange values in the top-right of [[fig-split]].)
2. After processing the window corresponding to a chunk, the buffer contains
   exactly the hashes of the chunk. (Fourth row.)
3. Then, we compute the suffix-minima of the chunk by scanning the buffer
   backwards. (Fifth row.)
4. For the next $w$ windows, we intialise a new rolling prefix minimum in the
   new chunk (green outline).
5. The minimum of each window is the minimum between the rolling prefix minimum
   in chunk $i+1$ (green outline), and the suffix minimum in chunk $i$ that we computed in the
   buffer (blue outline).[fn:legend]

#+name: fig-split
#+caption: Split algorithm: for each chunk, we compute and store suffix-minima of the preceding chunk (orange row). We also track a rolling prefix minimum in the next chunk (green). Taking the minimum of that with the stored suffix-minimum gives the minimum of the window. The memory after each iteration is shown on the right, with updated values in orange.
#+attr_html: :class inset
[[file:./split.svg]]

#+caption: Code for the =Split= algorithm, that computes the suffix minima for each chunk exactly every $w$ iterations.
#+include: "code/blog/split.rs" src rust :lines "2-"

*** Performance characteristics
:PROPERTIES:
:CUSTOM_ID: split-perfomance
:END:

The big benefit of this algorithm is that its execution is completely deterministic, similar
to the [[*Jumping: Away with the queue][jump algorithm]]: every $w$ iterations we compute the suffix-minima, and
then we continue again with the rolling prefix minimum. Since this 'side loop'
is taken exactly once in every $w$ iterations, the corresponding branch is easy
to predict and does not cause branch-misses. Thus, we expect nearly $0\%$ of
branch-misses, and up to $4$ instructions per cycle.

* Analysing what we have so far
** Counting comparisons

Before we look at runtime performance, let's have a more theoretical look at the
number of comparisons each method makes.
We can do this by replacing the =u64= hash type by a wrapper type that increments a
counter each time =.cmp()= or =.partial_cmp()= is called on it. I exclude
comparisons with the =MAX= sentinel value since those could possibly be
optimized away anyway. The code is not very interesting, but you can find it [[https://github.com/RagnarGrootKoerkamp/minimizers/blob/master/benches/blog/counting.rs][here]].

#+name: counts
#+caption: The number of comparisons per character on a string of length $10^8$ for $w=10$ and $w=20$.
#+begin_src txt
           w=10    w=20     formula
Buffered:  9.000  19.000    w-1
Queue:     1.818   1.905    2-2/(w+1)
Jumping:   1.457   1.595    ?
Rescan:    1.818   1.905    2-2/(w+1)
Split:     2.700   2.850    3-3/w
#+end_src

Observations:
- =Buffered= Does $w-1$ comparisons for each window to find the max of $w$ elements.
- =Queue= and =Rescan= both do exactly $2-2/(w+1)$ comparisons per element.
- For =Queue=, each =pop_back= is preceded by a comparison, and each =push= has
  one additional comparison with the last element in the queu that is smaller
  than the new element. Only when the new element is minimal (probability
  $1/(w+1)$), the queue becomes empty and that comparison doesn't happen. Also
  when the smallest element is popped from the front (probability $1/(w+1)$), that means it won't be
  popped from the back and hence also saves a comparison.
- Unsurprisingly, the =Jumping= algorithm uses the fewest comparisons, since it also
  returns strictly less information than the other methods.
- The =Split= method, which seems very promising because of its
  data-independence, actually uses around $50\%$ more comparisons ($3-3/w$, to be
  precise), because for each window, a minimum must be taken between the suffix
  and prefix.

*** Open problem: Theoretical lower bounds

It would be cool to have a formal analysis showing that we cannot do
better than $2-2/(w+1)$ expected comparisons per element to compute the random
minimizer of all windows, and similarly it would be nice to have a lower bound
on the minimal number of comparisons needed to only compute the positions of the minimizers.

** Setting up benchmarking
*** Adding criterion
Before we start our implementation, let's set up a benchmark so we can easily
compare them. We will use [[https://crates.io/crates/criterion][criterion]], together with the [[https://crates.io/crates/cargo-criterion][cargo-criterion]] helper.
First we add =criterion= as a dev-dependency to =Cargo.toml=.

#+caption: =Cargo.toml= changes to set up =criterion=.
#+begin_src toml
[dev-dependencies]
criterion = "*"

# Do not build the library as a benchmarking target.
[lib]
bench = false

[[bench]]
name = "bench"
harness = false
#+end_src

We add =benches/bench.rs= which looks like this:
#+attr_shortcode: "Click to show code."
#+begin_details
#+caption: A basic criterion benchmark for the comparison so far.
#+include: "code/bench.rs" src rust :lines "19-53"
#+end_details

*** Making criterion fast
By default, the benchmarks are quite slow. That's for a couple of reasons:
- Before each benchmark, a 3 second warmup is done.
- Each benchmark is 5 seconds.
- After each benchmark, some plots are generated.
- At the end, some HTML reports are generated.
I'm impatient, and all this waiting *really* impacts my iteration time, so let's
make it faster:
- Reduce warmup time to 0.5s.
- Reduce benchmark time to 2s, and only take 10 samples.
- Disable plots and html generation.
The first two are done from code:
#+caption: Initializing =criterion= with reduced warmup and measurement time.
#+begin_src rust
criterion_group!(
    name = group;
    config = Criterion::default()
        // Make sure that benchmarks are fast.
        .warm_up_time(Duration::from_millis(500))
        .measurement_time(Duration::from_millis(2000))
        .sample_size(10);
    targets = initial_runtime_comparison, count_comparisons_bench
);
#+end_src
The last is best done by adding the ~--plotting-backend disabled~ flag. For
convenience, we add this rule to the =justfile= so we can /just/ do =just
bench=. I'm also adding =quiet= to hide the comparison between runs to simplify presentation.
#+caption: Adding the =bench= rule to the =justfile=.
#+begin_src make
bench:
    cargo criterion --plotting-backend disabled --output-format quiet
#+end_src

When we now do =just bench=, we see this (TODO update):

#+begin_export html
<script src="https://asciinema.org/a/EQtJkYBEXYzHsEBnhrMLOp29l.js" id="asciicast-EQtJkYBEXYzHsEBnhrMLOp29l" async="true"></script>
#+end_export

*** A note on CPU frequency

Most consumer CPUs support turboboost to increase the clock frequency for short
periods of time. That's nice, but not good for stable measurements. Thus, I
always pin the frequency of my =i7-10750H= to the default ~2.6GHz~:
#+caption: Pinning CPU frequency to =2.6GHz=.
#+begin_src sh
sudo cpupower frequency-set --governor powersave -d 2.6GHz -u 2.6GHz
#+end_src
This usually results in very stable measurements.

Similarly, I have hyper threading disabled, so that each program has a full core
to itself, without interference with other programs.


** Runtime comparison with other implementations
Let's compare the runtime of our method so far with
two other Rust implementations:
- [[https://crates.io/crates/minimizer-iter][minimizer-iter]] is an implementation written by [[https://twitter.com/IgorMartayan][Igor Martayan]]. It returns an iterator over all
  /distinct/ minimizers and uses the queue algorithm.
- Daniel Liu's [[https://gist.github.com/Daniel-Liu-c0deb0t/7078ebca04569068f15507aa856be6e8][implementation]] of the rescan algorithm.

#+caption: Runtime comparison between our and other's implementations.
#+begin_src txt
g/naive                 time:   [64.399 ms 64.401 ms 64.404 ms]
g/buffered              time:   [55.154 ms 55.160 ms 55.173 ms]
g/queue                 time:   [22.305 ms 22.322 ms 22.335 ms]
g/jumping               time:   [7.0913 ms 7.0969 ms 7.1042 ms]
g/rescan                time:   [14.067 ms 14.071 ms 14.078 ms]
g/split                 time:   [14.021 ms 14.023 ms 14.027 ms]
g/queue_igor            time:   [26.680 ms 26.689 ms 26.700 ms]
g/rescan_daniel         time:   [9.0335 ms 9.0340 ms 9.0349 ms]
#+end_src

Some initial observations:
- Measurements between samples are very stable, varying less than $1\%$.
- Each method takes 7-55ms to process 1 million characters. That's 7-55ns per
  character, or 7s to 55s for 1Gbp.
- Our queue implementation performs similar to the one in =minimizer-iter=.
- Daniel's implementation of =Rescan= is around twice as fast as ours! Time to
  start looking deeper into our code.

Looking back at our expectations:
- As expected, =Buffered= is the slowest method by far, but even for $w=11$, it
  is only $2\times$ slower than the next-slowest method.
- =Split= is about as fast as =Rescan=, and even a bit slower. We were expecting
  a big speedup here, so something is weird.
- =RescanDaniel= is even much faster than our =Split=, so clearly there must be
  a lot of headroom.

** Deeper inspection using =perf stat=
Let's look some more at performance statistics using =perf stat=. We can reuse
the benchmarks for this, and call =criterion= with the =--profile-time 2= flag,
which simple runs a benchmark for $2$ seconds.

#+caption: =just stat= will run a benchmark for 2 seconds under =perf stat=.
#+begin_src just
stat test='':
    cargo build --profile bench --benches
    perf stat -d cargo criterion -- --profile-time 2 {{test}}
#+end_src

The metrics that are relevant to us now are summarized in the following table.

#+begin_src sh :exports none
for alg in naive buffered\$ queue\$ queue_igor jumping rescan\$ rescan_daniel split\$ ; just stat-selection $alg ; end
#+end_src

#+caption: =perf stat= results for all methods so far.
#+attr_html: :class small
| metric         | Runtime | instructions/cycle | branches/sec | branch misses |
|                | ns/char |                GHz |        M/sec |             % |
| =Naive=        |      64 |               4.13 |         1743 |         0.11% |
| =Buffered=     |      55 |               2.17 |          987 |         4.66% |
| =Queue=        |      22 |               2.29 |          810 |         4.29% |
| =QueueIgor=    |      27 |               2.33 |          662 |         4.29% |
| =Jumping=      |     7.1 |               3.89 |         1459 |         0.13% |
| =Rescan=       |      14 |               3.02 |         1116 |         2.26% |
| =RescanDaniel= |     9.0 |               3.21 |         1410 |         0.85% |
| =Split=        |      14 |               3.37 |         1116 |         1.55% |

Observe:
- =Naive= has an instructions per cycle (IPC) of over 4! I have never actually seen IPC that high.
  It's not measurement noise, so most likely it comes from the fact that some
  instructions such as compare followed by conditional jump are fused.
- =Naive= also has pretty much no branch misses.
- =Buffered= only has half the IPC of =Naive= but is still slightly faster since
  it only hashes each kmer once. For some reason, there are a lot of branch
  misses, which we'll have to investigate further.
- The two =Queue= variants also have
  relatively few instructions per cycle, and
  also relatively a lot of branch misses. Here, the branch-misses are expected,
  since the queue pushing and popping are inherently unpredictable.
- In fact, I argued that the last element of the =Queue= is popped with
  probability $50\%$. But even then, the branch predictor is able to get down to
  $4\%$ of branch misses. Most likely there are correlations that make these
  events somewhat predictable, and of course, there are also other easier
  branches that count towards the total.
- =Jumping= has an very good IPC of nearly $4$, as we somewhat hoped for, and
  correspondingly has almost no branch misses.
- The remaining methods all have a decent IPC just above $3$.
- =RescanDaniel= has much fewer branch misses than =Rescan=. We should investigate this.
- We argued that =Split= is completely predictable, but we see a good number of
  branch misses anyway. Another thing to investigate.

** A first optimization pass
It's finally time to do some actual optimizations!
We'll go over each of the algorithms we saw and optimize them.
This step is 'just' so we can make a fair benchmark between them, and in the end
we will only optimize one of them further, so I will try to keep it short.
The final optimized version of each algorithm can be found in the repository,
but intermediate diffs are only shown here.
*** Optimizing =Buffered=: reducing branch misses
Let's have a look at =perf record=:
#+caption: =just perf buffered= shows very inefficient and branchy code. Lines marked with a =*= have over /5%/ of time spent in them. This pattern occurs multiple times due to loop unrolling.
#+begin_src asm
  1.16 │       cmp    %r8,%rbp          ; compare two values
  0.92 │       setne  %r11b
  0.70 │    ┌──jb     976               ; conditionally jump down
 *6.15*│    │  mov    %r11b,%r13b       ; overwrite the minimum (I think)
  0.02 │    │  mov    %r13d,%r12d       ; overwrite the position (I think)
 *5.44*│976:└─→mov    0x8(%rdx),%r10    ; continue
#+end_src
Most of these branches come from the fact that computing the minimum of two
=Elem { val: usize, pos: usize }= is very inefficient, since two comparisons are
needed, and then the result is conditionally overwritten.

The only reason we are storing an =Elem= that includes the position is that we
must return the leftmost minimum. But since e.g. =position_min()= returns the
leftmost minimum anyway, we can try to avoid storing the position and always
iterate over values from left to right. We'll add a method to our =RingBuf= for this:
#+caption: Split the =RingBuf= into two slices, such that the element of the first returned slice come before the second part, and compute their minimum.
#+include: "code/blog/ringbuf.rs" src rust :lines "44-"

#+caption: =BufferedOpt= that only stores values and finds the minimum from left to right. Runs in =26ms=, down from =55ms=.
#+include: "code/blog/naive.rs" src rust :lines "49-"

The hot loop of the code looks much, much better now, using =cmov= instead of
branches to overwrite the minimum.

#+attr_shortcode: "New assembly"
#+begin_details
#+caption: New hot-loop, using =cmov=, and very few branches.
#+begin_src asm
  1.59 │898:┌─→lea        0x8(%r14),%r13
  4.85 │    │  mov        %rbx,0xe0(%rsp)
  2.72 │    │  mov        %r9,0xf8(%rsp)
  5.29 │    │  mov        %r13,0xf0(%rsp)
  1.85 │    │  lea        0x1(%rax),%rbp
  4.78 │    │  mov        %rbp,0x110(%rsp)
 11.20 │    │  mov        (%r14),%r14
  7.24 │    │  cmp        %rcx,%r14       ; compare
  3.41 │    │  cmovb      %rax,%r11       ; conditionally overwrite pos
  4.04 │    │  cmovb      %r14,%rcx       ; conditionally overwrite min
  2.50 │    │  mov        %rbp,%rax
  5.19 │    │  mov        %r13,%r14
  3.01 │8d4:│  test       %r14,%r14
  0.92 │    │↓ je         8de
  0.44 │    ├──cmp        %r9,%r14
  3.90 │    └──jne        898          
#+end_src
#+end_details

Also the =perf stat= is great now: =4.23 IPC= and =0.13%= branch misses, so
we'll leave it at this.

#+attr_shortcode: takeaway
#+begin_notice
- Compute minima from left to right to avoid comparing positions.
- Prefer =cmov= over branches to compute minima.
#+end_notice

*** =Queue= is hopelessly branchy
We can confirm that indeed a lot of time is spent on branch-misses around
the popping of the queue.
#+attr_shortcode: "Assembly code showing a badly predicted branch."
#+begin_details
#+caption: Each of the two directions after the branch takes =10%= of time.
#+begin_src asm
 1.20 │3a0:   lea        (%rdx,%rax,1),%rdi
 0.63 │ ↑     cmp        %rcx,%rdi
 1.34 │ │     mov        $0x0,%edi
 0.97 │ │     cmovae     %rcx,%rdi
 0.46 │ │     shl        $0x4,%rdi
 0.49 │ │     mov        %rsi,%r8
 1.58 │ │     sub        %rdi,%r8
 1.52 │ │  ┌──cmp        %rbp,(%r8)    ; compare the new element with the back
 1.97 │ │  ├──jb         3d2           ; if back is smaller, break loop
*9.85*│ │  │  dec        %rax          ; pop: decrease capacity
 3.70 │ │  │  mov        %rax,0x28(%rsp)
 0.41 │ │  │  add        $0xfffffffffffffff0,%rsi
 0.08 │ │  │  test       %rax,%rax     ; queue not yet empty
 0.37 │ └──│─ jne        3a0           ; try popping another element
 0.38 │    │  xor        %eax,%eax
*9.64*│3d2:└─→cmp        %rcx,%rax
#+end_src
#+end_details

Sadly, I don't really see much room for improvement here.

*** =Jumping= is already very efficient
If we do a =perf report= here, we see that only =25%= of the time is spent in
the =minimizer_positions= function:
#+caption: =just perf jumping= shows that most time is spent computing hashes.
#+begin_src txt
  63.69%  <map::Map<I,F> as Iterator>::fold
  24.25%  <JumpingMinimizer<H> as Minimizer>::minimizer_positions
#+end_src

The hot loop for finding the minimum of a window is very compact again.
#+attr_shortcode: "Assembly code"
#+begin_details
#+caption: Hot loop of =Jumping= that finds the minimum of a window, with plenty =cmov= instructions and no branches.
#+begin_src asm
  2.84 │170:┌─→lea     0x1(%rcx),%rsi
  1.58 │    │  mov     -0x18(%rdx,%rcx,8),%rdi
  4.17 │    │  mov     -0x10(%rdx,%rcx,8),%r8
  0.09 │    │  cmp     %rdi,%rax
  3.60 │    │  cmovb   %rax,%rdi
  9.34 │    │  cmovbe  %rbp,%rsi
       │    │  lea     0x2(%rcx),%rax
  0.94 │    │  cmp     %r8,%rdi
  2.59 │    │  cmovae  %r8,%rdi
  9.36 │    │  cmovbe  %rsi,%rax
  1.35 │    │  mov     -0x8(%rdx,%rcx,8),%rsi
  0.27 │    │  lea     0x3(%rcx),%rbp
  0.86 │    │  cmp     %rsi,%rdi
  5.69 │    │  cmovae  %rsi,%rdi
  5.94 │    │  cmovbe  %rax,%rbp
  1.48 │    │  mov     (%rdx,%rcx,8),%rsi
  0.72 │    │  add     $0x4,%rcx
  0.32 │    │  cmp     %rsi,%rdi
  4.98 │    │  mov     %rdi,%rax
  1.31 │    │  cmovae  %rsi,%rax
  5.76 │    │  cmova   %rcx,%rbp
       │    ├──cmp     %rbx,%rcx
  0.13 │    └──jne     170                                                 
#+end_src
#+end_details

More problematic is the =63%= of time spent on hashing kmers, but we will
postpone introducing a faster hash function a bit longer.

*** Optimizing =Rescan=
The original =Rescan= implementation suffers from the same issue as the
=Buffered= and compares positions, so let's reuse the =RingBuf::forward_min()=
function, and compare elements only by value.
#+attr_shortcode: "Code for RescanOpt."
#+begin_details
#+caption: Optimized =RescanOpt= that runs in =10.2ms= instead of =14.1ns=.
#+include: "code/blog/rescan.rs" src rust :lines "25-"
#+end_details

This has =3.55 IPC= up from =3.02=, and =0.91%= branch misses, down from
=2.26%=, which results in the runtime going down from =14.1ms= to =10.2ms=.

The hottest part of the remaining code is now the branch-miss when ~if pos -
min.pos == w~ is true, taking ~6%~ of time, since this is an unpredictable
branch. This seems rather unavoidable. Otherwise most time is spent hashing kmers.

*** Optimizing =Split=
Also here we start by avoiding position comparisons.
This time it is slightly more tricky though: the =RingBuf= /does/ have to store
positions, since after taking suffix-minima, the value at each position does not
have to correspond anymore to its original position.

The tricky part here is to ensure that the compiler generates a branchless
minimum, since usually it creates a branch to avoid overwriting two values with
their own value:

#+caption: Making sure the suffix minima use =cmov= instructions.
#+begin_src diff
-if ring_buf[i + 1].val <= ring_buf[i].val {
-    ring_buf[i] = ring_buf[i + 1];
-};
+ring_buf[i] = if ring_buf[i + 1].val <= ring_buf[i].val {
+    ring_buf[i + 1]
+} else {
+    ring_buf[i]
+};
#+end_src

#+attr_shortcode: "Code for SplitOpt."
#+begin_details
#+caption: Optimized =SplitOpt= that runs in =11.2ms= instead of =14.0ns=.
#+include: "code/blog/split.rs" src rust :lines "30-"
#+end_details

Now, it runs in =11.2ms= instead of =14.0ms=, and has =3.46 IPC= (up from =3.37
IPC=) and =0.63%= branch misses, down from =1.55%=.
The remaining branch misses seem to be related to the kmer hashing, although
this is somewhat surprising as =Jumping= doesn't have them.

** A new performance comparison
Now that we have our optimized versions, let's summarize the new results.

#+begin_src sh :exports none
for alg in naive buffered\$ buffered_opt queue\$ queue_igor jumping rescan\$ rescan_opt rescan_daniel split\$ split_opt; just stat-selection $alg ; end
#+end_src


#+caption: =perf stat= results for all methods so far, for /w=11/.
#+attr_html: :class small
| metric         | Runtime | instructions/cycle | branches/sec | branch misses |
|                | ns/char |                GHz |        M/sec |             % |
| =Naive=        |      64 |               4.13 |         1743 |         0.11% |
| =Buffered=     |      55 |               2.17 |          987 |         4.66% |
| =BufferedOpt=  |    *26* |             *4.19* |       *1499* |       *0.13%* |
| =Queue=        |      22 |               2.29 |          810 |         4.29% |
| =QueueIgor=    |      27 |               2.33 |          662 |         4.29% |
| =Jumping=      |     7.1 |               3.89 |         1459 |         0.13% |
| =Rescan=       |      14 |               3.02 |         1116 |         2.26% |
| *=RescanOpt=*  |    *10* |             *3.58* |       *1397* |       *0.90%* |
| =RescanDaniel= |     9.0 |               3.21 |         1410 |         0.85% |
| =Split=        |      14 |               3.37 |         1116 |         1.55% |
| =SplitOpt=     |    *11* |             *3.47* |       *1410* |       *0.63%* |

Note how =BufferedOpt= is now almost as fast as =Queue=, even though it's
$O(nw)$ instead of $O(n)$. Unpredictable branches really are terrible. Also,
=RescanOpt= is now nearly as fast as
=RescanDaniel=, although =Jumping= is still quite a bit faster.
For reasons I do not understand =SplitOpt= is still not faster than =RescanOpt=,
but we'll have to leave it at this.

* TODO Rolling[fn::Hah, get it?@@html:<br/>@@ There's at least *two* alternative interpretations ;)] our own hash
As we saw, =Jumping= spends two thirds of its time on hashing the kmers, and in
fact, =SplitOpt= and =RescanOpt= spend around half of their time on hashing.
Thus, it is finally time to investigate some different hash functions.

** FxHash
So far, we have been using [[https://crates.io/crates/fxhash][=FxHash=]], a [[https://docs.rs/fxhash/0.2.1/src/fxhash/lib.rs.html#64][very simple]] hash function originally used by FireFox and also used
in the Rust compiler. For each =u64= chunk of 8 bytes, it bit-rotates the hash
so far, =xor=s with the new words, and multiplies by a fixed constant. For
inputs less than 8 characters, it's only a single multiplication.
Even though this method iterates over the input length, all k-mers have the same
length, so that the branches involved are usually very predictable.
Nevertheless, hashing each k-mer involves quite some instructions, and
especially a lot of code is generated to to handle all lengths modulo $8$ efficiently.

*** WyHash
[[https://crates.io/crates/wyhash][WyHash]] is currently the fastest cryptographically secure hash function in the
SMHasher benchmark, but using it instead of FxHash slows down
the benchmarks by roughly =2ns= per character.
The reason is that FxHash itself is not secure, and hence can be simpler.


** NtHash: a rolling hash
We could avoid this for loop for the hashing of each k-mer by using a /rolling/
hash that only needs to update the hash based on the character being added and
removed. The classsic rolling hash is the Karp-Rabin hash, where the hash of a kmer $x$
is
$$
h(x) = \sum_{i=0}^{k-1} x_i \cdot p^i \pmod m,
$$
for some modulus $m$ (e.g. $2^{64}$) and coprime integer $p$. This can be computed in an incremental
way by multiplying the previous hash by $p$ and adding/subtracting terms for the
character being added and removed.

=NtHash= [cite:@nthash] is a more efficient variant[fn::The ntHash paper doesn't
actually provide a benchmark for this though. But the consensus seems to be that
buzhash is faster than classic multiplication based methods.] of this based on 'buzhash',
using multiplication and addition $(\times, +)$ uses bitwise rotation and xor $(\text{rot}, \oplus)$:
$$
h(x) = \bigoplus_{i=0}^{k-1} \text{rot}^i(f(x_i)),
$$
where $f$ is a static table lookup that maps each character to some fixed random
value. (See also [[file:../nthash.org][this post on ntHash]]).

Using the =nthash= [[https://crates.io/crates/nthash][crate]], the implementation of the =Hasher= trait that we've
implicitly been using so far is simple:
#+name: hasher
#+caption: Wrapping the =nthash= crate with the =Hasher= trait.
#+include: "code/blog/hash.rs" src rust :lines "-29"

Unfortunately, the results are mixed:
#+caption: Runtime in ns/kmer of methods with FxHash and ntHash.
#+attr_html: :class small
| alg \ hash:   | FxHash | ntHash |
| =BufferedOpt= |     26 |     30 |
| =Queue=       |     22 |     24 |
| =Jumping=     |    7.1 |    5.0 |
| =RescanOpt=   |     10 |     14 |
| =SplitOpt=    |     11 |     16 |
All methods become =2-4ns/ker= slower, apart from =Jumping=, which is actually
significantly faster. Remember that for =Jumping= ([[jumping-code]]) we first compute all hashes
and collect them into a vector, while all other methods compute hashes on the
fly. Thus, for some reason =ExtNtHasher= must be faster than FxHash when
simply streaming through all kmers, while it's slower when rolling the hash at
more irregular intervals.

*** Buffered hash values
Based on the difference above, let's investigate whether first collecting all
hashes to a vector speeds up the other methods as well. We provide a simple
wrapper type for this:
#+caption: =Buffer<H>= is a =Hasher= that first collects all hashes to a vector and then iterates over them.
#+include: "code/blog/hash.rs" src rust :lines "39-"

#+caption: Runtime in ns/kmer of methods with FxHash, ntHash, and buffered versions.
#+attr_html: :class small
| alg \ hash:   | FxHash | FxHash | ntHash | ntHash |
| buffered:     |     no |  *yes* |     no |  *yes* |
| =BufferedOpt= |     26 |     31 |     30 |     28 |
| =Queue=       |     22 |     24 |     24 |     22 |
| =Jumping=     |    7.1 |    7.1 |    5.0 |    5.0 |
| =RescanOpt=   |     10 |     13 |     14 |     11 |
| =SplitOpt=    |     11 |     14 |     16 |     12 |

#+attr_shortcode: takeaway
#+begin_notice
Buffering hashes in an intermediate vector helps for ntHash, but not for FxHash.
#+end_notice

** Making ntHash fast: going branchless
Still now, ntHash take over half the time of the =Jumping= algorithm, so we'll
try to make it a bit faster.
#+caption: =just perf jumping_nt= shows that most time is spent creating the =Vec= of hashes from the ntHash iterator.
#+begin_src txt
50.65%  <Vec<T> as SpecFromIter<T,I>>::from_iter
36.68%  <JumpingMinimizer<H> as Minimizer>::minimizer_positions
#+end_src

#+attr_shortcode: "Assembly for collecting ntHashes into a vector."
#+begin_details
#+caption: Assembly code for collecting all ntHashes into a vector. We see plenty of branches.
#+begin_src gas
1c5:   mov    %r14,(%rax,%r13,8)
       inc    %r13
       mov    %r13,0x40(%rsp)
     if self.current_idx == self.max_idx {
       mov    %rbx,%rcx
       add    %r13,%rcx
     ↑ je     153
     if self.current_idx != 0 {
1dd:   mov    %rbp,%rcx
       add    %r13,%rcx
     ↓ je     24d
     let seqi = self.seq[i];
       lea    -0x1(,%r13,1),%rdi
       add    %rbp,%rdi
       cmp    %r15,%rdi
     ↓ jae    2e2
       mov    0x20(%rsp),%rcx
     let seqk = self.seq[i + self.k];
       add    %r13,%rcx
       dec    %rcx
       cmp    %r15,%rcx
     ↓ jae    2cd
     let seqi = self.seq[i];
       movzbl -0x1(%r12,%r13,1),%ecx
       lea    anon.c37bfa7dce888be50e4a96f2eaf16e27.6.llvm.2840843246760040980+0x6f,%rsi
     let val = H_LOOKUP[c as usize];
       mov    (%rsi,%rcx,8),%rdx
     if val == 1 {
       cmp    $0x1,%rdx
     ↓ je     272
       mov    0x28(%rsp),%rcx
     let seqk = self.seq[i + self.k];
       movzbl -0x1(%rcx,%r13,1),%ecx
     let val = H_LOOKUP[c as usize];
       mov    (%rsi,%rcx,8),%rsi
     if val == 1 {
       cmp    $0x1,%rsi
     ↓ je     272
       rorx   $0x3f,%r14,%r14
       mov    0x18(%rsp),%rcx
       rol    %cl,%rdx
     self.fh = self.fh.rotate_left(1) ^ h(seqi).rotate_left(self.k as u32) ^ h(seqk);
       xor    %rsi,%r14
       xor    %rdx,%r14
24d:   cmp    0x30(%rsp),%r13
     ↑ jne    1c5
#+end_src
#+end_details

Even though we know from =perf stat= that there are basically no branch misses,
the assembly does contain a lot of branches, most of which correspond to
assertions/bound checks in the original code. To fix these, will copy [[https://docs.rs/nthash/0.5.1/src/nthash/lib.rs.html#281-306][the source]]
of the =nthash= crate and modify it locally.

To start, =ExtNtHash= takes in =2.9ns/kmer= to collect kmer hashes to a vector.

*** Drop sanity checks

First, we see that for each character it is checked whether it is =ACTG=. It
would be faster to do that up-front if needed, so let's remove the check.
#+caption: Skip checking that characters are =ACTG=. Runtime: =2.9ns/kmer= ↦ =2.6ns/kmer=.
#+begin_src diff
 fn h(c: u8) -> u64 {
     let val = H_LOOKUP[c as usize];
-    if val == 1 {
-        panic!("Non-ACGTN nucleotide encountered! {}", c as char)
-    }
     val
 }
#+end_src

*** Drop bound checks
Next, we sprinkle in some =get_unchecked=, to avoid unnecessary bound checks.
Both checks are provably impossible to fail, yet the compiler isn't able to
elide them. I suspect it has something to do with the
=NtHashForwardIterator::new()= function not being inlined, and hence the
=next()= function not being aware of invariants checked during initialization. I
also tried adding some additional assertions and using =unchecked_add= to
indicate that overflow is undefined behaviour, but even then one of the branches remained.
Even with full link time optimization enabled (~lto = true~ in =Cargo.toml=),
the branches are still there[fn::The runtime does go down a bit to =2.3ns/kmer=,
but we'll postpone looking into this more, since full lto is absolutely terrible
for compile times, and thin lto is not sufficient.].

#+caption: Avoiding bound checks: =2.6ns/kmer= ↦ =1.9ns/kmer=.
#+begin_src diff
 if self.current_idx != 0 {
     let i = self.current_idx - 1;
-    let seqi = self.seq[i];
-    let seqk = self.seq[i + self.k];
+    let seqi = unsafe { *self.seq.get_unchecked(i) };
+    let seqk = unsafe { *self.seq.get_unchecked(i + self.k) };

     ...
 }
#+end_src

#+attr_shortcode: takeaway
#+begin_notice
Not all provably redundant bound checks are always elided.
#+end_notice

*** Efficiently collecting to a vector

At this point there are three branches left:
1. Checking whether the end of the sequence was reached.
2. The ~self.current_idx != 0~ check that makes sure we don't remove a character
   in the first iteration.
3. A check that the vector we are accumulating into still has space.

Both the second and third should be redundant: we can handle the first
iteration separately, and pre-allocate a vector that is sufficiently large.
Let's fix the latter one first. Ideally, the compiler would check that the
=size_hint= provided by the =NtHashForwardIterator= is large enough and that the
initial =Vec::reserve()= has sufficient capacity that an overflow can not
happen, but alas, that's not the case (also not with full lto).

Generally, it seems that we can not get rid of the possible resize check while
using =.collect::<Vec<_>>()=. Thus, let's roll our own.

First, we can manually reserve the vector capacity and then fill it:
#+caption: Even with a manually reserved vector, there is an impossible-to-reach call to =grow_one=.
#+begin_src rust
fn hash_kmers(&self, k: usize, t: &[u8]) -> impl Iterator<Item = Self::Out> {
    let len = t.len() - k + 1;
    let mut v = Vec::with_capacity(len);
    let mut it = self.hasher.hash_kmers(k, t);
    for _ in 0..len {
        v.push(it.next().unwrap());
    }
    v.into_iter()
}
#+end_src

#+attr_shortcode: "Generated assembly"
#+begin_details
#+caption: The assembly still shows a call to =grow_one=, which should be impossible to reach.
#+begin_src gas
  0.04 │330:┌─→mov          0x10(%rsp),%rax
 19.91 │    │  mov          %r13,0x8(%rax,%rbx,8)
  0.01 │    │  add          $0x2,%rbx
  0.11 │    │  mov          %rbx,0x18(%rsp)
       │    │  mov          %r12,%rax
 19.65 │    │  mov          %rbp,%rbx
       │    │  add          %rbp,%rax
       │    │↓ je           38f                                    ; unwrap failed?
  0.06 │34e:│  lea          0x1(%rbx),%rbp
  0.01 │    │  movzbl       (%r15,%rbx,1),%eax
 18.40 │    │  rorx         $0x3f,%r13,%rdx
  0.06 │    │  mov          (%r14,%rax,8),%r13
  0.16 │    │  mov          0x20(%rsp),%rcx
 20.75 │    │  rol          %cl,%r13
  0.13 │    │  mov          0x60(%rsp),%rax
  0.36 │    │  movzbl       (%rax,%rbx,1),%eax
 19.85 │    │  xor          %rdx,%r13
  0.41 │    │  xor          (%r14,%rax,8),%r13
       │    ├──cmp          0x8(%rsp),%rbp
  0.06 │    └──jne          330                                    ; vector capacity was reached?
       │       lea          0x8(%rsp),%rdi
       │       vzeroupper
       │     → call         alloc::raw_vec::RawVec<T,A>::grow_one  ; grow the vec
       │     ↑ jmp          330
       │38f:   mov          0x8(%rsp),%r12
#+end_src
#+end_details

Well this doesn't work. I really trusted the compiler to optimize this away, but
it doesn't[fn::Maybe there is a reason for keeping it that I don't see? Let me know!]. Guess I'll have to adjust my trust level accordingly.

Anyway, I suppose we'll just make a default vector with the right
/size/ and then fill it, so we can completely drop the =push= call.
#+caption: Using a fixed-size vector drops the =grow= check! Runtime: =1.94ns/kmer= ↦ =1.71ns/kmer=.
#+begin_src rust
fn hash_kmers(&self, k: usize, t: &[u8]) -> impl Iterator<Item = Self::Out> {
    let len = t.len() - k + 1;
    let mut v = vec![H::Out::default(); len];
    let mut it = self.hasher.hash_kmers(k, t);
    for x in v.iter_mut() {
        *x = it.next().unwrap();
    }
    v.into_iter()
}
#+end_src

The current assembly looks like this.
#+caption: Current assembly code for collecting ntHashes into a vector. *Spend some time* to understand the comments here.
#+begin_src gas
300:┌─→mov          %r11,%rcx                ; read max_iterations
    │  add          %r8,%rcx                 ; check if last iteration
    │↓ je           362                      ; unwrap fails?
    │  movzbl       (%r12,%r8,1),%ecx        ; get seq[i]
    │  mov          (%r9,%rcx,8),%r10        ; lookup hash of seq[i]
    │  mov          %r13d,%ecx               ; read 32-bit k into %ecx
    │  rol          %cl,%r10                 ; rotate hash(seq[i]) by %cl = low 8 bits of %ecx
    │  rorx         $0x3f,%rsi,%rsi          ; rotate rolling hash %rsi
    │  xor          %r10,%rsi                ; xor with hash of seq[i]
    │  movzbl       (%rax,%r8,1),%ecx        ; get seq[i+k]
    │  xor          (%r9,%rcx,8),%rsi        ; xor with hash of seq[i+k]
    │  mov          %rsi,0x8(%r14,%r8,8)     ; write result to vec
    │  inc          %r8                      ; increase i
    ├──add          $0xfffffffffffffff8,%rdx ; decrease loop counter
    └──jne          300                      ; loop
#+end_src

#+attr_shortcode: takeaway
#+begin_notice
Avoid =push= to fill know-size arrays. (But maybe this doesn't apply in general.)
#+end_notice

*Aside: x64 intricacies.* This looks pretty great! The first three lines are somewhat redundant though,
since we know the unwrap can never fail[fn::Again, the compiler disappoints a
bit. Maybe there is some tricky reason why it's not actually redundant?].  And also on line 6, there is a 32-bit =mov= from =%r13d= to =%ecx= of
which the low 8 bits are used on line 7 as =%cl=[fn::[[https://wiki.osdev.org/CPU_Registers_x86-64][This article]] has a nice
summary table of the register naming.]. But then why not directly use the low 8
bits of =%r13=, =%r13b=? Maybe the compiler insists on using the /counter/
register =c= for this? After some more reading and discussing with [[https://twitter.com/zommiommy][Tommasso Fontana]], that does indeed seem to be the
case: [[https://c9x.me/x86/html/file_module_x86_id_273.html][this page]] lists the opcodes for =rol= instructions, and the only supported
variants are rotating by =1=, by =cl=, and by an immediate value.[fn::See also
[[https://en.wikibooks.org/wiki/X86_Assembly/X86_Architecture#General-Purpose_Registers_(GPR)_-_16-bit_naming_conventions][this wiki page]] and [[https://www.swansontec.com/sregisters.html][this nice article]] for some more information about special
purpose registers].

*Aside: native compilation.* I'm using =-march=native=. Without that, it generates a =rol $1,%rsi=
instead of a =rorx $0x3f,%rsi,%rsi=, and the runtime is =1.83ns/kmer= instead of
=1.71ns/kmer=. =rorx= is probably faster because the =x= suffix means it doesn't
set overflow/carry flags, and hence has fewer dependencies with other
instructions. But =rorx= is only available with [[https://en.wikipedia.org/wiki/X86_Bit_manipulation_instruction_set#BMI2_(Bit_Manipulation_Instruction_Set_2)][BMI2]], so this requires the
native compilation.

#+attr_shortcode: takeaway
#+begin_notice
Use =-march=native=.
#+end_notice

*Back to branches.* Either way, let's get rid of the last remaining branch:
#+caption: Using =unwrap_unchecked= to avoid a branch. Runtime: =1.71ns/kmer= ↦ =2.20ns/kmer=.
#+begin_src diff
 for x in v.iter_mut() {
-    *x = it.next().unwrap();
+    unsafe { *x = it.next().unwrap_unchecked() };
 }
#+end_src

Somehow this is worse. It seems two new branches came back when removing this
one![fn::Cf. [[https://en.wikipedia.org/wiki/Lernaean_Hydra][Hydra]] (Greek mythology).]

#+attr_shortcode: "Assembly, but worse."
#+begin_details
#+caption: There is now a branch again to check if the index is 0.
#+begin_src gas
       │2f0:┌─→cmp          %rbx,%r8
       │    │↑ je           2e0
 10.88 │    │  test         %r8,%r8
  0.01 │    │↓ je           323
  0.03 │    │  movzbl       -0x1(%r13,%r8,1),%ecx
  8.79 │    │  lea          (%r8,%rbp,1),%r9
       │    │  dec          %r9
 12.47 │    │  movzbl       0x0(%r13,%r9,1),%r9d
  0.06 │    │  rorx         $0x3f,%rdx,%r10
  9.37 │    │  mov          (%r11,%rcx,8),%rdx
       │    │  mov          %ebp,%ecx
 19.99 │    │  rol          %cl,%rdx
  0.09 │    │  xor          %r10,%rdx
 13.48 │    │  xor          (%r11,%r9,8),%rdx
       │323:│  inc          %r8
 16.40 │    │  mov          %rdx,(%r14,%rsi,1)
       │    │  add          $0x8,%rsi
       │    ├──cmp          %rsi,%r12
  8.44 │    └──jne          2f0
#+end_src
#+end_details

I've also tried adding an =assert_eq!(v.len(), len)=, but that doesn't help
either.[fn::Cf. [[https://en.wikipedia.org/wiki/Chekhov%27s_gun][Chekhov's gun]].]
#+caption: Asserting on the length of the vector doesn't change the runtime.
#+begin_src diff
+let mut v = vec![H::Out::default(); len];
+assert_eq!(v.len(), len);
#+end_src

*Addressing mode.*
Another thing we can do is improve the vector addressing mode. Instead of
writing to =vector_offset + index * 8=, it's sometimes faster to directly write
to a =*ptr= address. Let's try something:
#+name: ptr-offset
#+caption: Using a pointer offset instead of iterating the vector again makes things worse: =1.71ns/kmer= ↦ =1.82ns/kmer=.
#+begin_src diff
 let mut v = vec![H::Out::default(); len];
 assert_eq!(v.len(), len);
 let mut it = self.hasher.hash_kmers(k, t);
-for x in v.iter_mut() {
-    *x = it.next().unwrap();
-}
+for i in 0..len {
+    unsafe { v.as_mut_ptr().add(i).write(it.next().unwrap()) };
+}
#+end_src
I'm not quite sure why, but this causes the assembly to be one instruction
longer and hence slightly slower.

*Confusion, again!* Ok let's remove the assertion again.
#+caption: Removing the assertion gets us where we wanted to be all this time: =1.71ns/kmer= ↦ =1.59ns/kmer=.
#+begin_src diff
 let mut v = vec![H::Out::default(); len];
-assert_eq!(v.len(), len);
 let mut it = self.hasher.hash_kmers(k, t);
 for i in 0..len {
     unsafe { v.as_mut_ptr().add(i).write(it.next().unwrap()) };
 }
#+end_src

/Finally/ the assembly is how I like it:

#+caption: Look at it! Isn't it wonderful? No more redundant branches and some loop-unrolling.
#+begin_src gas
  0.02 │7b0:   movzbl       0x0(%rbp,%rsi,1),%ecx
 10.28 │       mov          (%r14,%rcx,8),%rdi
  3.67 │       mov          %r12d,%ecx
 11.31 │       rol          %cl,%rdi
  3.13 │       rorx         $0x3f,%rdx,%rdx
       │       xor          %rdi,%rdx
  0.02 │       movzbl       -0x1(%rbx,%rsi,1),%ecx
 10.94 │       xor          (%r14,%rcx,8),%rdx
  5.91 │       mov          %rdx,0x8(%rax,%rsi,8)
  0.02 │       movzbl       0x1(%rbp,%rsi,1),%ecx
  0.02 │       mov          (%r14,%rcx,8),%rdi
 10.70 │       mov          %r12d,%ecx
  4.53 │       rol          %cl,%rdi
 11.34 │       rorx         $0x3f,%rdx,%rdx
  5.42 │       xor          %rdi,%rdx
  0.58 │       movzbl       (%rbx,%rsi,1),%ecx
  0.17 │       xor          (%r14,%rcx,8),%rdx
 18.47 │       mov          %rdx,0x10(%rax,%rsi,8)
  3.48 │       add          $0x2,%rsi
       │       cmp          %rsi,%r8
       │     ↑ jne          7b0
#+end_src

At this point I am rather confused, but at least the code looks good:
1. Why does removing the assertion make things faster?
2. Why is the loop using a pointer offset ([[ptr-offset]]) faster/different at
   all? Shouldn't the compiler have the same information on both cases?

#+attr_shortcode: takeaway
#+begin_notice
Adding assertions to 'help' the compiler can be counterproductive.
#+end_notice

** Rolling a bit[fn::$k$ bits, actually] less
As we saw before, the rotation by $k$ positions takes two instructions: one to
move $k$ into =%cl=, and one for the rotation itself. But all rotations are over
exactly $k$, so in fact, we can precompute these:
#+caption: Precomputing the rotations by $k$ positions. Runtime: =1.59ns/kmer= ↦ =1.41ns/kmer=.
#+begin_src diff
 Some(NtHashForwardIterator {
     seq,
     k,
     fh,
     current_idx: 0,
     max_idx: seq.len() - k + 1,
+    rot_k: H_LOOKUP.map(|x| x.rotate_left(k as u32)),
 })

 ...

-self.fh = self.fh.rotate_left(1) ^ h(seqi).rotate_left(self.k as u32) ^ h(seqk);
+self.fh = self.fh.rotate_left(1) ^ self.rot_k[seqi as usize] ^ h(seqk);
#+end_src

#+caption: The corresponding assembly goes from 21 to just 15 instructions for 2 loops.
#+begin_src gas
       │570:   movzbl       0x0(%rbp,%rax,1),%ecx
 16.03 │       movzbl       -0x1(%r8,%rax,1),%edx
  0.05 │       rorx         $0x3f,%r12,%rsi            *
  0.34 │       xor          0xa58(%rsp,%rcx,8),%rsi    *
  0.05 │       xor          (%rbx,%rdx,8),%rsi         *
 32.71 │       mov          %rsi,0x8(%r15,%rax,8)
  0.02 │       movzbl       0x1(%rbp,%rax,1),%ecx
  0.02 │       rorx         $0x3f,%rsi,%r12            *
  0.34 │       xor          0xa58(%rsp,%rcx,8),%r12    *
 17.62 │       movzbl       (%r8,%rax,1),%ecx
  0.49 │       xor          (%rbx,%rcx,8),%r12         *
 32.02 │       mov          %r12,0x10(%r15,%rax,8)
  0.08 │       add          $0x2,%rax
       │       cmp          %rax,%rdi
  0.14 │     ↑ jne          570                         
#+end_src

** Analysing the assembly code

At this point, we cannot hope for much more. The 15 instructions fall apart as:
- 1: increase iteration count by 2,
- 2: compare and jump,
- Then for each of the two unrolled iterations:
  - 2: load two characters,
  - 1: rotate the rolling hash,
  - 2: xor in the two characters,
  - 1: write back the result.

At =1.41ns/kmer=, this loop should take =2.82ns= for the two iterations, or
=2.82ns/loop * 2.6GHz = 7.3 cycles/loop=, that corresponds to just over 2
instructions per cycle, and indeed =perf stat= shows =2.04 IPC=. Indeed, I have
marked the critical path of xor'ing the hashes with =*= s in the assembly code,
and we see 6 instructions that all depend on each others result, so an overhead
of =1.3= cycle on top of that is not terrible.

Still, instead of doing =x = (x^a)^b=, it would be more efficient to do =x =
x^(a^b)=. But just parenthesizing the code like that doesn't improve the
assembly. Probably because that would require an separate load instruction,
since xor'ing two addresses from memory is not a thing.



* TODO SIMD all the way
- unchecked indexing
- SIMD: processing 4/8/16(?) sequences in parallel.

* TODO Super-k-mers and canonical kmers

#+print_bibliography:
