#+title: [WIP] Beyond Global Alignment
#+filetags: @thesis pairwise-alignment wip
#+OPTIONS: ^:{} num: num:t
#+hugo_front_matter_key_replace: author>authors
#+toc: headlines 3
#+hugo_paired_shortcodes: %notice
#+date: <2025-03-24 Mon>

#+begin_export html
This is Chapter 5 of my thesis.

---
#+end_export

#+attr_shortcode: summary
#+begin_notice
So far, we have considered only algorithms for /global/ alignment.
In this chapter, we consider /semi-global/ alignment and its variants instead,
where a pattern (query) is searched in a longer string (reference).
There are many flavours of semi-global alignment, depending on the
(relative) sizes of the inputs. We list these variants, and introduce
some common approaches to solve this problem.

We then extend A*PA into A*map as a new tool to solve these problems.
#+end_notice

#+attr_shortcode: attribution
#+begin_notice
All text in this chapter is my own.
Some ideas on semi-global alignment are based on early discussions with Pesho Ivanov.
#+end_notice

$$
\renewcommand{\st}[2]{\langle #1, #2\rangle}
$$

In this chapter, we will look at the problem of /semi-global alignment/, as a next
step after /global alignment/ ([[../pairwise-alignment/pairwise-alignment.org][blog post]]).
In fact, we will quickly see that there is no such thing as ''just'' global
alignment.
Rather, there are many variants that have applications in different domains.

In [[#semi-global-variants]] we survey the different types of semi-global alignment.
Then, in [[#text-searching]], we adapt the bitpacking and SIMD method of A*PA2 for
$O(nm)$ text searching. In [[#mapping]], we extend the gap-chaining seed heuristic
of A*PA for semi-global alignment and hence mapping. We use this as the
/seeding/ and /chaining/ parts of the classic /seed-chain-extend/ framework.
Then we run a separate alignment to find the optimal alignment.


* Variants of semi-global alignment
:PROPERTIES:
:CUSTOM_ID: semi-global-variants
:END:
#+name: types
#+caption: Different types of pairwise alignment. Bold vertices and edges indicate where each type of alignment may start and end. Local-extension alignment can end anywhere, whereas local alignment can also start anywhere. Like clipping, these modes require a /score/ for matching characters, while in other cases, a /cost model/ (with cost-0 matches) suffices. This figure is based on an earlier version that was made in collaboration with Pesho Ivanov.
#+attr_html: :class inset large
[[file:fig/alignment-modes.svg]]

As we saw in Chapter 2 ([[../pairwise-alignment/pairwise-alignment.org][blog post]]), there are many different types of pairwise alignment ([[types]]).
Whereas /global/ alignment is used when two sequences are to be fully aligned,
when that is not the case, there are many different variants.
In /semi-global/ alignment, we align a sequence to a subset of a longer sequence.
While theoretically speaking this captures the full problem, in practice, there
are many variants of semi-global alignment that admit different algorithms.
In particular, solutions to the problem depend a lot on the absolute and
relative size of the two sequences being aligned.

#+caption: Depending on the absolute and relative size of the two sequences, instances of semi-global alignment fall into three categories: pairwise alignment, text searching, and mapping.
#+attr_html: :class inset :width 70%
[[file:fig/dimentions.svg]]

*Pairwise semi-global alignment.*
In this chapter, we will use /(pairwise) semi-global alignment/ to refer to
instances where the two sequences to be aligned have comparable length.
In this case, the number of skipped characters at the start and end of the
longer sequence should be small, and not much more than the edit distance
between the two sequences. In this case, we assume that the alignment is /one-off/, i.e., that the
both sequences are only aligned once, and that only a single best alignment is
needed.
Instances of this problem can have length anywhere from 100bp to 100kbp.
Pairwise semi-global alignment is especially useful when a pattern has already
been approximately located in a text, and a final alignment is done to obtain
the precise alignment.

Since the two sequences have similar length, the DP matrix is nearly square, and
it suffices to compute states near ''the'' diagonal.
Thus, semi-global alignment is similar to /ends-free/ and global alignment,
and the classic methods for global alignment can be applied.


*Approximate string matching (text searching).*
In the remaining cases, one of the two sequences (the /text/) is significantly longer than the
other (the /pattern/).
When additionally the pattern is short, with a length on
the order of the machine word size (64 classically, or 256 with SIMD), we
classify the instance as /approximate[fn::Here, /approximate/ means that we look
for /inexact/ matches with a number of mutations.] string matching/ or /text searching/.
Here, we are typically interested in not just one optimal alignment, but in
finding all sufficiently good alignments, with a cost below some (fixed) threshold.
One of the reasons we reasons we consider this a separate problem is that there
is a truly vast amount of literature on optimizing approximate string matching.
See e.g.
[cite:@alpern95;@baeza-yates-gonnet92;@wu92;@hyyro05-increased;@rognes11] for a
small selection[fn::See [[https://curiouscoding.nl/posts/approximate-string-matching]] for a longer overview of relevant papers.].

Initial solutions for this problem include computing the full matrix ($O(nm)$)
and using various types of bitpacking ($O(n\lceil m/w\rceil)$) and parallellization. Additionally,
when patterns with up to $k$ errors are to be found, only a subset of the matrix
has to be computed and $O(n\lceil k/w\rceil)$ expected time is sufficient
[cite:@myers99] ([[text-searching]]).

One characteristic of nearly all methods for text searching is that they scan
the entire text on each search, and thus require at least $\Omega(n)$ time.
Typical applications are searching for a text in a set of files (as can be done
by =grep=), and extracting tags and/or barcodes from multiplexed ONT (Oxford Nanopore Technologies) reads.

*Mapping.*
The primary characteristic of /mapping/ when compared to approximate string
matching is that for mapping, many strings are searched in a single text. Thus,
$\Omega(n)$ time per search is not sufficient, especially when the reference has
a size anywhere from megabytes to gigabytes.
Often, patterns are /long reads/ with a length on the order of 50kbp, but also
/short reads/ of length 100bp can be mapped.
To avoid the $\Omega(n)$ per search, the reference is /indexed/, so that regions
similar to the pattern can efficiently be identified.
Usually, this leads to /approximate/ methods, where the reported
alignments are not guaranteed to have a minimal cost.

As with approximate string searching, one way to use mapping is by asking for
all alignments with cost below some threshold.
Another option is to ask only for the few best alignments. In practice, another
mode is to first find the cost $s$ of the best alignment (assuming its cost is below some
threshold), and then find all alignments up to e.g. cost $s' = 1.1 \cdot s$.

The classic method applied here is /seed-chain-extend/ [cite:@minimap2].
This first finds matches between the sequences, then finds /chains/ of these
matches, and then fills the gaps in between consecutive matches using relatively
small alignments.

* Fast text searching
:PROPERTIES:
:CUSTOM_ID: text-searching
:END:

#+name: text-searching
#+caption: Text searching is the problem of finding a typically short (length $O(w)$) pattern in a longer text. The left shows how the classical Needleman-Wunsch algorithm fills the entire matrix column by column. On the right (adapted from [cite:@myers99]), we search for all alignments with cost $\leq k$, and states at distance $\leq k$ are highlighted. The block-based approach only computes blocks that contain at least one state at distance $\leq k$, and takes $O(n \lceil k/w\rceil)$ time in expectation on random strings.
#+attr_html: :class inset large
[[file:fig/search.svg]]

In A*PA2 ([[../astarpa2/astarpa2.org][blog post]]), we developed a block-based method for pairwise alignment.
At the core, these blocks are computed using a fast SIMD-based implementation of the
bitpacking algorithm of Myers [cite:@myers99].
So far, we have only used this as a building block for global alignment,
but now we will use this to directly support $O(n\lceil m/w\rceil)$ text searching.

In the basis, this requires two changes.
First, we ensure that the alignment can start anywhere in the text by changing the
horizontal differences along the top row of the matrix from $1$ (as used by
global alignment) to $0$, as indicated by the bold lines in [[text-searching]].

Secondly, the alignment may end anywhere, and the user may be interested more
than just a single best alignment.
To support this, we do not only report the score in the bottom right of the DP
matrix, but we return a list of all scores along the bottom row.
Based on this, the user can decide which scores are sufficiently low to find a
full alignment.

*Tracing.*
Once the user decides which scores at the bottom of the matrix are sufficiently
low, a traceback be started from those positions.
To save time and memory, the initial computation of the matrix only returns the output scores and does
not store all $nm$ values.
Thus, to find an alignment ending in column $i$, we recompute the matrix from column
$i-2m$ to column $i$ and store all values for each column.
We then do a usual trace through this matrix from $\st im$ until we reach the
top row ($j=0$).

** Skip-cost for overlap alignments

#+name: skip-cost
#+caption: By default, global alignment uses a cost of 1 along all edges of the matrix, while semi-global alignment and overlap/ends-free/extension variants have a cost of 0 along some edge.
#+caption: When a pattern only partially overlaps the text, as shown on the left, it may be preferable to have a /skip-cost/ $\alpha$ for each unmatched character that is in between $0$ and $1$. This can also be applied to global alignment (replacing ends-free alignment), and can be an alternative to local alignment.
#+attr_html: :class inset medium
[[file:fig/skip-cost.svg]]

In some applications, it may happen that the pattern is present, but cut off at
either its start or end, as shown on the left in [[skip-cost]].
For example when a read was cut short, or when aligning reads against an
incomplete assembly [cite:@Abramova_2024].
In a classical
semi-global alignment, the unmatched start of the pattern would incur a cost of
1 per unmatched character, but this may make the total cost of the pattern go
above the threshold. Instead, overlap alignment could be used
([[types]]), but this requires a bonus for matches, since otherwise the cheapest way
to align the pattern could be to skip nearly all of its characters. Ends-free
alignment solves this by only allowing a limited number of characters to be
skipped. Still, this is suboptimal: when the pattern matches once in full, and
once at the start of the sequence with $50\%$ overlap, the scores of these two
alignments are not directly comparable. In fact, the overlapping alignment
has a benefit because it only pays for mismatches in half its length.

To solve this, we introduce the skip cost[fn::I would not be surprised if this
has been done before. There are many tools applying similar techniques (either
via local alignment or a clipping cost), but as far as I am aware, the technique
as stated here has not been applied before.] $0\leq \alpha \leq 1$, which is the cost paid for each
character at the start and/or end of the pattern that is not aligned because it
extends outside the text.
This concept can also be applied to global-alignment variants such as ends-free
and overlap ([[skip-cost]], middle), so that skipping characters in both sequences
has a (not necessarily equal) cost.

In practice, it is not practical to handle fractional costs, especially in the
case of edit distance where the distance between adjacent states must be 0 or 1.
To avoid this, we can initialize the first and last column (and row, for global
alignment) with a mix of zeros and ones, so that the /fraction/ of ones is
approximately $\alpha$, as shown in [[skip-cost-example]] for $\alpha=0.5$.

#+name: skip-cost-example
#+caption: Example of computing a semi-global alignment with a skip-cost of $\alpha = 1/2$. In the first column the graph, edges of cost 1 and 0 alternate. On the bottom, the graph is extended with matches until a multiple of the block size is reached. On the right, the final score in row $j$ is increased by $\lceil \alpha(m-j)\rceil = \lceil (m-j)/2\rceil$ to obtain the score including skip-cost. Three alignments are highlighted and shown, with edits highlighted. Only half of the skipped characters (rounded up) incurs a cost.
#+attr_html: :class inset medium
[[file:fig/skip-cost-example.svg]]

*Applying the skip-cost.*
In [[skip-cost-plot]], we show an example output when using a skip-cost of
$\alpha\in\{0, 0.5, 1\}$ for the alignment as shown in [[skip-cost-setup]].
Using $\alpha = 1$ corresponds to classical semi-global alignment (green), and
we see that this correctly detects that the pattern matches in the middle of the
sequence, ending at position 300, with a cost around 20. However, the
occurrences overlapping the start and end of the text are completely missed.
Overlap alignment, which corresponds to $\alpha=0$ (blue) /does/ have local
minima at position 50 and 650 (indicating the pattern extends 50 characters
beyond the text). The drawback of these minima is that there are also
/global/ minima at positions 0 and 700 where the pattern is completely disjoint
from the text, so that some additional logic is needed to separate these cases.
We see that in regions where the pattern does not match, the alignment has a
score around 50, or $0.5$ per character. Thus, we choose $\alpha=0.5$ per
skipped character. Using this (orange), we recover clear local minima at
positions 50 and 650, while the cost converges back to 50 as the overlap shrinks
to 0.


#+name: skip-cost-plot
#+caption: Example of the output of the skip-cost alignment when aligning a length-100 pattern onto a length-600 text (as shown in [[skip-cost-setup]]). Graphs are shown for $\alpha=1$, corresponding to classical semi-global alignment, $\alpha=0.5$, corresponding to the skip-cost introduced here, and $\alpha=0$, corresponding to an overlap alignment. Vertical lines indicate the region inside of which the pattern fully matches within the text, and where the cost of the alignment does not depend on the skip-cost $\alpha$.
#+attr_html: :class inset medium
[[file:skip-cost/skip-cost-plot.svg]]

#+name: skip-cost-setup
#+caption: The setup of the alignment results shown in [[skip-cost-plot]]. A random pattern of length 100 is generated and overlaid on a length 600 text 3 times:
#+caption: once in the middle, and twice with an 50 base overlap at the start/end of the sequence. Before inserting the pattern into the text, a different number of mutations is applied to the full length-100 pattern.
#+attr_html: :class inset medium
[[file:fig/skip-cost-setup.svg]]

* A*Map:
:PROPERTIES:
:CUSTOM_ID: mapping
:END:

#+caption: seeding the map algorightm with k-mer mathces
#+attr_html: :class inset medium
[[file:fig/seed-chain-extend.svg]]

** The cost of chaining
#+caption: gap-cost: lower bound on distance between diagonals
#+attr_html: :class inset medium :width 80%
[[file:fig/chaining.svg]]


Say we have a match ending in $(i_1, j_1)$ and another match starting in $(i_2,
j_2)$.
Set $\Delta_i = i_2-i_1\geq0$ and $\Delta_j=j_2-j_1\geq 0$.
*** max: Anchored edit distance
Here we pessimistically have to pay for every character not supported by a match:
$\max(\Delta_i, \Delta_j)$.
*** diff: gap-cost
Here we only pay a lower bound on the cost: $|\Delta_i - \Delta_j|$.
*** dist: seed heuristic
If we are guaranteed to find all seeds of length at least $k$, then we cross $\Delta_i/k$
seeds without finding a single match, so that there must be at least
$\Delta_i/k$ errors. For simplicity, we can only consider matches that are
aligned to $i$ being a multiple of $k$ [cite:@astarpa;@astarix-2].

(A /seed/ here is a chunk of $k$ characters of the text/reference, while a
/match/ is a /seed/ with a matching occurence in the pattern.)

In a way, a match implies that ''alignments that starts here have relative cost strictly below $n'/k$.''

*** minimap
$w/100 \cdot |\Delta_i - \Delta_j| + 0.5\cdot \log_2 |\Delta_i - \Delta_j|$,
- $w$ is the average length of the seeds/matches.
- small cost of $w/100$ per char
- logarithmic cost for some additional concave penalty for small gaps
- Why $w/100$? Why not $1/w$ which is more equivalent to what the seed heuristic does???

*** GCSH: gap-chaining seed heuristic
- Max of diff and dist
- transform-theorem:
  - only chain when cgap<=cseed
  - only chain when ... formula
* New: A*Map
** Text searching
- Do the full $nm$ matrix
- Return the bottom row and right column scores, so user can make a decision
  what to do with this
- new: $0\leq \alpha\leq 1$ soft-clip cost, generalizing ends-free.
- new: output format
- traceback from specific positions on request
** Mapping
- build hashmap on chunked k-mers of reference
- find matches for each pattern
- transform, radix sort, and then chain using LCP algo
- say $k=20$, then we have guaranteed matches if divergence $\leq 5\%$.
- But we want to avoid processing random one-off matches
- So require at least $10\%$ of the possible matches to be present, for a max
  divergence of $4.5\%$.
- Track /dominant/ matches that start a chain of at least length $10\% \cdot
  m/k$.
- For each of them, do a semi-global alignment of a slightly buffered region of
  the text (around length $m + 2\cdot 4.5\%\cdot m$).
- The alignment can be done using $O(nm)$
- TODO: Better methods:
  - $O(ms)$, adapted to semi-global (currently the code only does global)
  - semi-global version of A*PA
  - semi-global version of A*PA2
  - bottom-up match-merging

* Early idea: Bottom-up match-merging (aka BUMMer?)
One thing that becomes clear with mapping is that we don't quite
know where exactly to start the semi-global alignments.
This can be fixed by adding some buffer/padding, but this remains slightly ugly
and iffy.

Instead, I'm going to attempt to explain a new approach here.
Some details are still a bit unclear to me on how exactly they'd work, but I
have good hope it can all be worked out.

** Some previous ideas

Instead, we can use the following approach, which is a natural
evolution/convergence of a few previous ideas:
- /pre-pruning/ (or /local-pruning/; I haven't been very consistent with the
  name)

  The idea here is that a k-mer match gives us information that this seed can be
  traversed for free. The lack of a match implies cost at least 1.
  When a match is followed by noise, and thus can not be extended into an
  alignment of two seeds with cost $<2$, we can discard it, because the promise
  that there would be a good alignment (ie, relative cost $<1/k$) is not held.
  - see A*PA2 paper [cite:@astarpa2] ([[file:../../static/papers/astarpa2.pdf][PDF]]) or [[../astarpa2/astarpa2.org][blogpost]]
- /path-pruning/ ([[file:../speeding-up-astar/speeding-up-astar.org][blogpost]]): if we already know /some/ alignment, which is not
  necessarily optimal, we can use that to either find a better one or prove
  optimality:
  we can find all places at the start of a match where the heuristic is smaller
  than the actual remaining distance, and remove those matches. Again, these
  matches ''promise'' that the remainder of the alignment can be done in cost
  $<1/k$, but we should avoid to over-promise.

  After /path-pruning/ some matches, we run the alignment as usual, until the
  end of the original path is reached. Either the guessed path is then optimal,
  or the optimal path will have been found.

- /local-doubling/ ([[../local-doubling/local-doubling.org][blogpost]]): a drawback of path-pruning is that first we must find a
  path somehow, and then we must run the alignment again with the improved heuristic.
  /Local-doubling/ attempts to fix this by increasing the band of the alignment
  locally as needed.

  It gives nice figures, but I never quite got it to work reliably.

** Divide & conquer
Another common technique for pairwise alignment is Hirschberg's divide & conquer
approach [cite:@hirschberg75]. This find the distance to the middle column from
the left and right. There, a splitting point on the optimal alignment is found,
and we recurse into the two half-sized sub problems.

** Bottom-up match merging (BUMMer)
Initially, we have a set of many matches, including some spurious ones.
As we already saw with pre-pruning and path-pruning, if a match covering 1 seed does not into
an alignment of cost $<2$ covering $2$ seeds, we might as well discard it.
Then, if it does not extend into an alignment of cost $<4$ covering 4 seeds, we
can again discard it.

A slightly more principled approach is as follows:
1. Consider a binary tree on the seeds.
2. Initially the leafs correspond to a k-mer (seed) of the text, and the matches
   for that seed.
3. Then, we go up one level and see if we can merge adjacent matches. If so, we
   get a new match spanning two seeds, with /margin/ $2$ (because the two
   matches have cost $0$, which is $2$ below the number of seeds covered).

   Otherwise, it may be possible to extend a match of the left seed to also
   cover the right seed for cost $1$, creating a match covering the two seeds
   with margin $1$.
   Similarly, a right-match might be extended into the left seed.
4. Because an alignment of $2^{k+1}$ seeds with cost $<2^{k+1}$ must have cost
   $<2^k$ in either the left or right half, this procedure finds all such
   $2^{k+1}$-matches by only starting with single k-mer matches.
5. Eventually we extend our matches into a full alignment of the pattern and
   we're done.

One core idea here is this: if you have a long run of matches, these build up a
bunch of margin $a$, that can then be spend by aligning through a region with up
to $a$ noise. In the end, the complexity will be something like $\sum_a a^2$.

In fact, maybe this ends up exactly similar to A*PA, but faster because it
doesn't actually do the relatively slow A* bit. But I'm not sure yet; we'll see.

*Tricky bits.* What I haven't figured out yet:
- We need to efficiently merge matches for consecutive seeds. Maybe a simple
  lower bound like the seed heuristic (that ignores the $j$ coordinate) is good
  enough, but it would be interesting to see if we can design some
  algo/datastructure for efficiently merging matches.
- Reconstructing traces from output costs: suppose we take a semi global
  alignment and run it once top-to-bottom and once bottom-to-top. Can we infer
  from this information the start and end points of all locally-optimal
  alignment traces?


# ** TODO Semi-global highlight
# - [cite:@landau-vishkin89]
# - [cite:@myers99]
# - [cite:@chang92]: shows that ukkonens idea (Finding approximate patterns in
#   strings, also '85) runs in $O(nk)$
#   expected time for $k$-approximate string matching, when the reference is a
#   random string.
# - [cite:@wu96]: Efficient four russians in combination with 'ukkonens zone'
#   $O(kn/\lg s)$ when $O(s)$ space is available for lookup.
# - Baeza-Yates Gonnet 92
# - Baeza-Yates Navarro 96
# - LEAP: https://www.biorxiv.org/content/10.1101/133157v3


* TODO Benchmarks of simple methods

#+print_bibliography:
