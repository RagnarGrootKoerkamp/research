#+TITLE: Diamond optimisation for diagonal transition
#+HUGO_BASE_DIR: ../..
#+HUGO_TAGS: pairwise-alignment diagonal-transition optimisation
#+HUGO_LEVEL_OFFSET: 1
#+OPTIONS: ^:{}
#+hugo_auto_set_lastmod: nil
#+date: <2022-08-01>
#+author: Mykola Akulov
#+author: Ragnar Groot Koerkamp
#+hugo_front_matter_key_replace: author>authors
#+bibliography: local-bib.bib
#+cite_export: csl ../../chicago-author-date.csl
#+toc: headlines 3

* Diamond transition or how technicalities can break concepts

/We assume the reader has some basic knowledge about pairwise alignment
and in particular the WFA algorithm./

In this post we dive into a potential 2x speedup of WFA --- one that turns out not to work.

Let’s take a look at one of the most important and efficient algorithms for
pairwise alignment --- WFA [cite:@wfa]. It already looks good, and is pretty efficient. In
[[wfa]], which copies the style of Figure 1 in [cite/t:@wfalm], rows are wavefronts, and columns are diagonals. Light-blue
states are stored in memory. Green shows the current state being computed, and dark-blue
shows the cells the green cell depends on.

If a path is not required, we can use the linear-memory optimisation, as shown
in the right figure.[fn::The animated SVG figures and the [[https://github.com/RagnarGrootKoerkamp/research/blob/master/posts/diamond-optimization/generate_wfa_svg.c][C-code]]
used to create them are available in [[https://github.com/RagnarGrootKoerkamp/research/tree/master/posts/diamond-optimization][the repository]].]

#+caption: WFA algorithm, without and with linear-memory optimisation. 
#+name: wfa
| [[file:WFA.svg]] | [[file:WFA_mem_save.svg]] |

WFA is especially good for parallel calculations (which are extremely fast using
SIMD), [[wfa-parallel]].
The parallel wavefront computation also can use the linear memory optimisation,
as shown on the right.

#+caption: Parallel WFA algorithm, without and with linear-memory optimisation.
#+name: wfa-parallel
| [[file:WFA_parallel.svg]] | [[file:WFA_parallel_mem_save.svg]] |


The following idea for improvement of this algorithm may cross your mind: In the very
beginning, we already know in which diagonal (column) the answer will appear:
\begin{equation}
answer\_diagonal = length\_of\_first\_seq - length\_of\_second\_seq.
\end{equation}
This "answer" diagonal is highlighted in red in the figures. So why do we
calculate whole triangle? We never use the bottom left and right corners, but we
do waste time on that. Let’s
try to calculate only cells needed to explore new cells on the answer diagonal.
Knowing that every cell depends on at most three cells in the previous wavefront, we can
realize we do not need to store whole triangle, only part of it.
[[diamond]] shows this.

#+caption: Diamond optimisation
#+name: diamond
#+attr_html: :class inset
[[file:diamond.svg]]

The form of the covered area looks like a diamond, so let’s call it a Diamond
optimisation. (This name was first coined by Santiago and his team.)

Less cells to store means memory optimisation; less cells to calculate - runtime
optimisation! It should be at least two times faster! Looks like it can be
parallelized just as WFA ([[diamond-parallel]]).

We also do no not need to store all states if we do not need to trace back path,
so we can make a linear memory optimisation.

#+caption: Parallel diamond optimisation, without and with linear-memory optimisation.
#+name: diamond-parallel
| [[file:diamond_parallel.svg]] | [[file:diamond_parallel_mem_save.svg]] |

Looks like an amazing alternative to WFA!

** But let’s take a closer look

Let’s take a look at [[diamond]] again.

In one layer, we need to first cover one side of the diamond, then the other one. Stunts like
this make the implementation harder and more error-prone.

The second detail is that cells in one layer /do/ depend on previous cells in the
SAME layer. What does it mean? It means we can say “Auf Wiedersehen” to parallel
calculations. On the contrary, in usual WFA the current layer depends ONLY on
previous layers, so you can compute all cells in the layer at the same time (in parallel).

And finally, for WFA the formula is very simple – take the maximum of the cells
in the previous layer on the same diagonal and the diagonals left and right of
it, and then extend.
The author is simplifying here a bit, for you usually also add constant
to these diagonals, but this formula is still straightforward.

On the other hand, in the Diamond
optimisation you will need to work with three layers at the same time and have
some special edge cases for cells near the answer diagonal. So,
both figures in [[diamond-parallel]] are completely wrong and give hope only at the
first sight.

Furthermore, in the diamond optimisation we can only extend one diagonal at a
time (or possibly two --- one at each side), while in WFA we can extend all
diagonals in the same layer at the same time since they do not depend on each
other and can be calculated simultaneously.  This becomes a bottleneck: the
for-loop over each side of the diamond now has iterations that are dependent on
each other, so the next cell can only be computed after the previous one has
been computed. Also, each iteration contains an extend step resulting in
hard-to-predict memory access pattern.

Overall, this is simply too complex to optimise well.

** Conclusion

So this double speed-up is just fading comparing to what hurdles and obstacles
this “improvement” brings. But one more important fact is that you can get
approximately the same effect by using BiWFA [cite:@biwfa] (in two words, running WFA from
both sides), which keeps all the benefits of usual WFA, allowing to reduce space
and time usage.

* References
#+print_bibliography:
