#+TITLE: A review of exact global pairwise alignment
#+hugo_aliases: /posts/pairwise-alignment
#+HUGO_TAGS: method pairwise-alignment
#+HUGO_LEVEL_OFFSET: 1
#+OPTIONS: ^:{}
#+date: <2022-04-01 Thu>
#+author: Ragnar Groot Koerkamp
#+author: Pesho Ivanov
#+hugo_front_matter_key_replace: author>authors

#+toc: headlines 3

Note: This is a living document, and will likely remain so for a while. Feel
free to suggest missing papers or make a [[https://github.com/RagnarGrootKoerkamp/research/blob/master/posts/pairwise-alignment/pairwise-alignment.org][pull request]].

This post explains the many variants of pairwise alignment, and covers papers
defining and exploring the topic.

---

* Variants of pairwise alignment

/Pairwise alignment/ is the problem of finding the lowest cost way to transform a
string $A=a_1\cdots a_n$ of length $n$ into a string $B=b_1\cdots b_m$ of length $m$ using some set of
operations, each with their own cost.

Terminology and differences between similar terms is also discussed in
[[https://pesho-ivanov.github.io/#Alignment%20terminology][this post]] by Pesho.

** Cost models

TODO: Explain edit graph

#+caption: Overview of different cost models. (CC0; [[https://github.com/RagnarGrootKoerkamp/research/blob/master/posts/pairwise-alignment/drawings/cost-models.drawio.svg][source]])
#+name: cost-models
#+attr_html: :class large
[[file:drawings/cost-models.drawio.svg]]


There are different models to specify the cost of each edit operation. We list
some of them here. See also figure [[cost-models]].
Note that as presented each model minimizes the cost. Some papers change the
sign and maximize the score instead.

- LCS [[[https://en.wikipedia.org/wiki/Longest_common_subsequence_problem][wikipedia]]] ::
  Count minimum number of /indels/, that is, insertions or deletions, to transform
  $A$ into $B$. This is equivalent to finding the /longest common subsequence/.
- Unit cost edit distance / Levenshtein distance [[[https://en.wikipedia.org/wiki/Levenshtein_distance][wikipedia]]] ::
  Minimum number of indels and/or substitutions needed. All
  of cost $1$.
- Edit distance [[[https://en.wikipedia.org/wiki/Edit_distance][wikipedia]]] :: Matches/mismatches between characters $a_i$ and $b_j$ have cost $\delta(a_i, b_j)$.
  Inserting/deleting a character has cost $\delta(-, b_j)>0$ and $\delta(a_i, -)>0$ respectively.
  Usually $\delta(a,a) \leq 0$ and $\delta(a,b)>0$ for $a\neq b$.

  Edit distance is also used to mean Levenshtein distance occasionally.
- Affine cost :: Variant of edit distance, where
  gap costs are affine, i.e. linear with an offset.
  There are costs $o$ (/open/) and $e$ (/extend/), and the cost
  of a gap of length $k$ is $w_k = o + k\cdot e$.

  It's possible to have different parameters $(o_{\mathrm{ins}},
  e_{\mathrm{ins}})$ and $(o_{\mathrm{del}}, e_{\mathrm{del}})$ for insertions
  and deletions.

- Dual affine /(rare)/ :: Introduce two gap scores based on $(o_1, e_1)$ and
  $(o_2, e_2)$. The cost of a gap of length $k$ is $w_k = \min(o_1 + k\cdot e_1, o_2 +
  k\cdot e_2)$.
- Convex, concave /(rare)/ :: Gaps of length $k$ have a cost $w_k$, where $w_k$ is a
  convex/convex function of $k$, where longer gaps are relatively
  more/less expensive. Affine costs are an example of a concave gap cost.
- Arbitrary /(outdated)/ :: Generalization of edit distance where a gap of length $k$ has an
  arbitrary cost $w_k$. Arbitrary substitution cost $\delta(a, b)$ for matches/mismatches
  between characters $a$ and $b$, and arbitrary cost $w_k$ for a gap of length
  $k$. Not used anymore since it requires cubic algorithms.

In practice, most methods use a match cost $\delta(a,a) = 0$, fixed mismatch
cost $\delta(a,b) = X>0$ for $a\neq b$, and fixed indel cost
$\delta(a,-) = \delta(-,b) = I$. Algorithms that do not explore the entire
$n\cdot m$ edit graph only work for non-negative costs and /require/ $\delta(a,a) = 0$.

** Alignment types

#+caption: Overview of different alignment types. (CC0 by Pesho Ivanov; [[https://github.com/RagnarGrootKoerkamp/research/blob/master/posts/pairwise-alignment/drawings/alignment-types.drawio.svg][source]])
#+caption: TODO: re-scale image
#+name: alignment-types
#+attr_html: :class large
[[file:drawings/alignment-types.drawio.svg]]

Besides /global/ pairwise alignment, there are also other alignment types where
only a substring of the pattern and text is aligned to each other.

Figure [[alignment-types]] depicts the alignment types described below.
- Global (end-to-end) :: Align both sequences fully, end-to-end.
- Ends-free :: Indels/gaps at the end are free. This comes in several variants.
  - Semi-global (mapping, glocal, infix) :: Align a full sequence to a substring of a reference.
  - Global-extension / prefix /(unstable name)/ :: Align one sequence to a prefix of the other.
  - Overlap (dovetail) :: Align two partially overlapping reads against each other.
- Local :: Aligns a substring of $A$ to a substring of $B$. Like ends-free, but
  now we may skip the and start of both sequences.
  - Extension / prefix /(unstable name)/ :: Align a prefix of the two sequences. Similar to
    local, but anchored at the start.

Some alignment types, in particular local and overlap, typically use a different
cost model where matching characters get a bonus.

*A note on naming:*
Not all names are standard yet and especially global-extension and extension are
still unclear, as discussed on [[https://mobile.twitter.com/lh3lh3/status/1488580381091770371][this tweet]].  There is a suggestion to rename
extension to prefix, but it is unclear to me how global-extension should be named in
that case. Feel free to discuss in the comments at the bottom.


---


* A chronological overview of global pairwise alignment

Here is a chronological summary, assuming finite alphabets where needed.
$n\geq m$.  Time/space
improvements and new ideas are bold.  Unless mentioned otherwise, all these
methods are *exact* (i.e. provably correct) and do *global alignment*.

The following parameters are used here:
- $n \geq m$: sequence lengths. Note that some papers assume the opposite.
- $s$: alignment cost, given some cost model;
- $p$: length of LCS;
- $r$: the number of pairs of matching characters between the two sequences;
- $|\Sigma|$: alphabet size.

Methods link to the detailed explanation further down this page.

Not mentioned in the table are two review papers, [cite/text:@kruskal83] and [cite/text:@navarro01].

TODO: https://link.springer.com/article/10.1186/1471-2105-10-S1-S10

TODO: https://arxiv.org/abs/1501.07053

TODO: https://doi.org/10.1016/S0092-8240(05)80216-2

TODO: https://doi.org/10.1145/135239.135244

TODO: [cite:@papamichail2009] and [cite:@wu-O-np]

TODO: Mention Dijkstra algorithm of Ukkonen'85.

TODO: on topo-sorted A*: Spouge 1989 (Speeding up dynamic programming algorithms for finding
optimal lattice paths, Fast optimal alignment I, Fast optimal alignment II), Ficket 1984 (Fast optimal alignment)

TODO: on chaining: Myers Miller 1985: Chaining multiple-alignment fragments in sub-quadratic
time. Wilbur Lipman 1983: Rapid similarity searches of nucleic acid and protein
data banks, Wilbur Lipman 1984: The context dependent comparison of biological
sequences (This is basically LCS and LCSk, but 30 years earlier), Eppstein et al 1992: Sparse dynamic programming..., Myers Huang
1992: An O(n^2 log n) restriction map comparison and search algorithm.

TODO: Smith-Waterman-Fitch 1981: Comparative Biosequence Metrics

TODO: KSW2: https://github.com/lh3/ksw2

TODO: read mapping review: https://link.springer.com/article/10.1186/s13059-021-02443-7

TODO: fastest exact algorithm: Szymon Grabowski. New tabulation and sparse dynamic programming based techniques for
sequence similarity problems. Discrete Applied Mathematics, 212:96â€“103, 2016.
$O(n^2 log log n/ log^2 n)$

TODO: BSAlign [cite:@bsalign]

#+caption: Chronological overview of papers related to exact global pairwise alignment.
#+caption: If you use this for a paper, please cite this post.
#+name: table
#+attr_html: :class full-width
| Paper                                                | Cost model                                 | Time                                      | Space                                  | Method                                                            | Remarks                                                                          |
|------------------------------------------------------+--------------------------------------------+-------------------------------------------+----------------------------------------+-------------------------------------------------------------------+----------------------------------------------------------------------------------|
| [cite/text/cf:@vintsyuk68]                           | no deletions                               | $O(nm)$                                   | $O(nm)$                                | [[dp-history][DP]]                                                                | different formulation in a different domain, but conceptually similar            |
| [cite/text/cf:@nw]                                   | *edit distance* [fn::also arbitrary gaps?] | $O(n^2m)$                                 | $O(nm)$                                | [[#cubic-dp][DP]]                                                                | solves pairwise alignment in polynomial time                                     |
| [cite/text/cf:@sankoff]                              | LCS                                        | $\boldsymbol{O(nm)}$                      | $O(nm)$                                | [[dp-history][DP]]                                                                | the first quadratic algorithm                                                    |
| [cite/text/cf:@sellers] and [cite/text/cf:@wagner74] | edit distance                              | $O(nm)$                                   | $O(nm)$                                | [[#quadratic-dp][DP]]                                                                | the quadratic algorithm [[dp-history][now known as Needleman-Wunch]]                             |
| [cite/text/cf:@hirschberg75]                         | LCS                                        | $O(nm)$                                   | $\boldsymbol{O(n)}$                    | [[#divide-and-conquer][*divide-and-conquer*]]                                              | introduces linear memory backtracking                                            |
| [cite/text/cf:@hunt77]                               | LCS                                        | $\boldsymbol{O((r+n)\lg n)}$              | $O(r+n)$                               | [[#thresholds][*thresholds*]]                                                      | distance only                                                                    |
| [cite/text/cf:@hirschberg77]                         | LCS                                        | $\boldsymbol{O(p(m-p)\lg n)}$             | $\boldsymbol{O(n+(m-p)^2)}$            | [[#thresholds][*contours*]] + band                                                 | for similar sequences                                                            |
| [cite/text/cf:@four-russians-ed]                     | edit distance^{[[[discrete-scores]]]}          | $\boldsymbol{O(n\cdot \max(1, m/\lg n))}$ | $O(n^2/\lg n)$^{[[[score-only]]]}          | [[#four-russians][*four Russians*]]                                                   | best known complexity                                                            |
| [cite/text/cf:@gotoh]^{[[[bugfix]]]}                     | *affine*                                   | $O(nm)$                                   | $O(nm)$^{[[[score-only]]]}                 | [[#affine-costs][DP]]                                                                | extends [cite/text:@sellers] to affine                                           |
| [cite/text/cf:@nakatsu82]                            | LCS                                        | $\boldsymbol{O(n(m-p))}$                  | $O(n(m-p))$                            | *DP on thresholds*                                                | improves [cite/text:@hirschberg77], subsumed by [cite/text:@myers86]             |
| [cite/text/cf:@ukkonen85]^{[[[multiple]]]}               | edit distance                              | $\boldsymbol{O(ms)}$                      | $O(ns)$^{[[[score-only]]]}                 | [[#exponential-band][*exponential search on band*]]                                      | first $O(ns)$ algorithm for edit distance                                        |
| [cite/text/cf:@ukkonen85]^{[[[multiple]]]}               | edit distance^{[[[fixed-indel-cost]]]}         | $O(ns)$^{[[[expected-runtime]]]}              | $\boldsymbol{O(n+s^2)}$^{[[[score-only]]]} | [[#diagonal-transition][*diagonal transition*]]^{[[[diagonal-transition]]]}                     | introduces diagonal transition method                                            |
| [cite/text/cf:@myers86]^{[[[multiple]]]}                 | LCS                                        | $O(ns)$^{[[[expected-runtime]]]}              | $O(s)$ working memory                  | [[#diagonal-transition][*diagonal transition*]]^{[[[diagonal-transition]]]}, divide-and-conquer | introduces diagonal transition method for LCS, $O(n+s^2)$ expected time          |
| [cite/text/cf:@myers86]^{[[[multiple]]]}                 | LCS                                        | $\boldsymbol{O(n +s^2)}$                  | $O(n)$                                 | + [[#ns2][*suffix-tree*]]                                                   | better worst case complexity, but slower in practice                             |
| [cite/text/cf:@myers88]                              | affine                                     | $O(nm)$                                   | $O(m + \lg n)$                         | divide-and-conquer                                                | applies [cite/text:@hirschberg75] to [cite/text:@gotoh] to get linear space      |
| [cite/text/cf:@lcsk-overview]                        | LCS$k$^{[[[lcsk]]]}                            | $O(n + r \log l)$                         | $O(n + \min(r, nl))$                   | thresholds                                                        | modifies [cite/text:@hunt77] for LCS$k$                                          |
| Edlib: [cite/text/cf:@edlib]                         | unit costs                                 | $O(ns/w)$^{[[[word-size]]]}                   | $O(n)$                                 | exponential search, bit-parallel                                  | extends bit-parallel [cite:@myers99] to global alignment                         |
| WFA: [cite/text/cf:@wfa]                             | affine                                     | $O(ns)$^{[[[expected-runtime]]]}              | $O(n+s^2)$^{[[[score-only]]]}              | diagonal-transition                                               | extends diagonal transition to gap affine [cite/text:@gotoh]                     |
| WFALM: [cite/text/cf:@wfalm]                         | affine                                     | $O(n+s^2)$                                | $O(n+s^{3/2})$^{[[[score-only]]]}          | diagonal-transition, square-root-decomposition                    | reduces memory usage of WFA by only storing $1/\sqrt n$ of fronts                |
| BiWFA: [cite/text/cf:@biwfa]                         | affine                                     | $O(ns)$^{[[[expected-runtime]],[[https://github.com/smarco/BiWFA-paper/issues/2][?]]]}            | $O(s)$ working memory                  | diagonal-transition, divide-and-conquer                           | applies [cite/text:@hirschberg75] to WFA to get linear space                     |
| A* pairwise aligner [unpublished]                    | unit costs                                 | $O(n)$ expected                           | $O(n)$                                 | *A**, *seed heuristic*, *pruning*                                 | only for random strings with random errors, with $n\ll\vert \Sigma\vert  ^{1/e}$ |

1. <<multiple>> Multiple algorithms in a single paper.
2. <<discrete-scores>> The four Russians algorithm of [cite/text:@four-russians-ed] needs discrete scores and a finite alphabet.
3. <<score-only>> When only the score is needed, and not an alignment, these
   methods only need $O(n)$ memory, and for some $O(m)$ additional memory is sufficient.
4. <<bugfix>> [cite/text/c:@altschul] fixes a bug in the backtracking algorithm of [cite/text:@gotoh].
5. <<fixed-indel-cost>> Needs all indel costs $\delta(a, -)$ and $\delta(-,b)$
   to be equal.
6. <<diagonal-transition>> [cite/text/c:@ukkonen85] and [cite/text:@myers86]
   independently introduced the diagonal transition method in parallel.
7. <<expected-runtime>> These methods run in $O(n+s^2)$ expected time, even
   though not all authors note this. However, the proof of [cite/text:@myers86]
   applies for all of them. [[#ns2][Details here]].
8. <<word-size>> $w=64$ is the word size.
9. <<lcsk>> LCS$k$ is a variant of LCS where only runs of exactly $k$ consecutive equal characters can be matched.

---

* Algorithms in detail

We will go over some of the more important results here. Papers differ in the
notation they use, which will be homogenized here.

- We use $D(i,j)$ at the distance/cost to be minimized, and $S(i,j)$ as a
  score to be maximized. However, we use $\delta(a,b)$ both for costs and
  scores. [TODO: Change to $s(a,b)$ for scores?]
- The DP goes from the top left $(0,0)$ to the bottom right $(n,m)$.
- The lengths of $A$ and $B$ are $n$ and $m$, with $n\geq m$.
- We use $0$-based indexing for $A$ and $B$, so at match at $(i,j)$ is for
  characters $a_{i-1}$ and $b_{j-1}$.
- $A$ is at the top of the grid, and $B$ at the left. $0\leq i\leq n$ indicates
  a column, and $0\leq j\leq m$ a row.

** Classic DP algorithms

*** Cubic algorithm of [cite/text:@nw]
:PROPERTIES:
:CUSTOM_ID: cubic-dp
:END:
#+caption: The cubic algorithm of [cite/text:@nw].
#+name: fig:nw
[[file:screenshots/nw.png]]

TODO: max instead of min formulation

This algorithm ([[https://en.wikipedia.org/wiki/Needleman%E2%80%93Wunsch_algorithm#Historical_notes_and_algorithm_development][wikipedia]]) defines $S(i,j)$ as the score of the best path ending with a
(mis)match in $(i,j)$. The recursion uses that before matching $a_{i-1}$ and $b_{j-1}$,
either $a_{i-2}$ and $b_{j-2}$ are matched to each other, or one of them is
matched to some other character:
\begin{align}
    S(0,0) &= S(i,0) = S(0,j) := 0\\
    S(i,j) &:= \delta(a_{i{-}1}, b_{j{-}1})&& \text{cost of match}\\
&\phantom{:=} + \max\big( \max_{0\leq i' < i} S(i', j{-}1) + w_{i{-}i'{-}1},&&\text{cost of matching $b_{j-2}$}\\
&\phantom{:=+\max\big(} \max_{0\leq j'<j} S(i{-}1, j')+w_{j{-}j'{-}1}\big).&&\text{cost of matching $a_{i-2}$}
\end{align}
The value of $S(n,m)$ is the score of the alignment.

Note that the original paper uses $MAT_{ij}$ notation and goes backwards instead of
forwards. The example they provide is where $\delta(a_i, b_j)$ is $1$ when
$a_i=b_j$, and thus computes the length of the LCS.
Figure [[fig:nw]] shows the dependencies in the evaluation of a single
cell. The total runtime is $O(nm \cdot (n+m)) = O(n^2m)$ since each cell needs
$O(n+m)$ work.

*** A quadratic DP
:PROPERTIES:
:CUSTOM_ID: quadratic-dp
:END:

#+caption: An example of the edit distance computation between two sequences as shown in [cite/text/cf:@sellers], using unit costs.
#+caption: /1/ is a special character indicating the start.
[[file:screenshots/sellers.png]]

[cite/text/cf:@sellers] and [cite/text/f:@wagner74] both provide the following
quadratic recursion for edit distance. The improvement here compared to the
previous cubic algorithm comes from dropping the requirement that $D(i,j)$ has a
(mis)match between $a_i$ and $b_j$, and dropping the gap cost $w_k$.
\begin{align}
    D(i, 0) &:= \sum_{0\leq i' < i} \delta(a_i, -) \\
    D(0, j) &:= \sum_{0\leq j' < j} \delta(-, b_j)\\
    D(i, j) &:= \min\big(D(i{-}1,j{-}1) + \delta(a_i, b_j), &&\text{(mis)match}\\
            &\phantom{:=\min\big(}\, D(i{-}1,j) + \delta(a_i, -), && \text{deletion}\\
            &\phantom{:=\min\big(}\, D(i,j{-}1) + \delta(-, b_j)\big). && \text{insertion}.
\end{align}

This algorithm takes $O(nm)$ time since it does constant work per DP cell.

<<dp-history>> *History and naming:*
This algorithm is now called the Needleman-Wunsch (NW) algorithm ([[https://en.wikipedia.org/wiki/Needleman%E2%80%93Wunsch_algorithm][wikipedia]]).
[cite/text/c:@gotoh] refers to it as Needleman-Wunsch-Sellers' algorithm, to
highlight the speedup that [cite/text:@sellers] contributed.
Apparently Gotoh was not aware of the identical formulation in [cite/text:@wagner74].

[cite/text/c:@vintsyuk68] is a quadratic algorithm published already before
[cite/text:@nw], but in the context of speech recognition, where instead of
characters there is some cost $d(i,j)$ to match two states. It does not allow
deletions, and costs are associated with a state $(i,j)$, instead of the
transitions between them:
\begin{align}
    D(i, j) &:= \min\big(D(i{-}1,j{-}1), D(i{-}1, j)\big) + \delta(i,j).
\end{align}


The quadratic recursion of [cite/text:@sankoff] is similar to the one by
[cite/text:@sellers], but similar to [cite/text:@nw] this is a maximizing
formulation. In particular they set $\delta(a_i, b_j)=1$ when $a_i = b_j$ and
$0$ otherwise, so that they compute the length of the LCS. This leads to the recursion
\begin{align}
    S(i, j) &:= \max\big(S(i{-}1,j{-}1) + \delta(a_i, b_j),\, D(i{-}1, j), D(i, j{-}1)\big).
\end{align}

The wiki pages on [[https://en.wikipedia.org/wiki/Wagner%E2%80%93Fischer_algorithm][Wagner-Fisher]] and [[https://en.wikipedia.org/wiki/Needleman%E2%80%93Wunsch_algorithm#Historical_notes_and_algorithm_development][Needleman-Wunsch]] have some more historical context.


*** Local alignment
[cite/text/cf:@sw] introduces local alignment ([[https://en.wikipedia.org/wiki/Smith%E2%80%93Waterman_algorithm][wikipedia]]). This is formulated as a maximization
problem where matching characters give positive score $\delta(a,b)$.
The maximum includes $0$ to allow starting a new alignment anywhere in the DP
table, 'discarding' parts that give a negative score.
The best local alignment corresponds to the larges value $S(i,j)$ in the table.
\begin{align}
    S(0, 0) &:= S(i, 0) = S(0, j) := 0 \\
    S(i,j)  &:= \max\big(0, &&\text{start a new local alignment}\\
    &\phantom{:=\max\big(} S(i-1, j-1) + \delta(a_{i{-}1}, b_{j{-}1}), &&\text{(mis)math}\\
    &\phantom{:=\max\big(} \max_{0\leq i' < i} S(i', j) - w_{i{-}i'}, &&\text{deletion}\\
    &\phantom{:=\max\big(} \max_{0\leq j'<j} S(i, j')-w_{j{-}j'}\big).&&\text{insertion}
\end{align}
This algorithm uses arbitrary gap costs $w_k$, as first mentioned
in [cite/text:@nw] and formally introduced by [cite/text:@waterman].
Because of this, it runs in $O(n^2m)$.

*History and naming:*
The quadratic algorithm for local alignment is now usually referred to as the
Smith-Waterman-Gotoh (SWG) algorithm, since the ideas in [cite/text:@gotoh] can
be used to reduce the runtime from cubic by assuming affine costs,
just like to how [cite/text:@sellers] sped up [cite/text:@nw] for global alignment
costs by assuming linear gap costs.
Note though that [cite/text:@gotoh] only mentions this speedup in passing, and
that [cite/author/b:@sw] could have directly based their idea on the quadratic
algorithm of [cite/text:@sellers] instead.

*** Affine costs
:PROPERTIES:
:CUSTOM_ID: affine-costs
:END:
In their discussion, [cite/text/c:@smith81] make the first mention of affine
costs that I am aware of.
[cite/text/cf:@gotoh] generalized the quadratic recursion to these affine costs
$w_k = o + k\cdot e$, to circumvent the cubic runtime needed for the arbitrary
gap costs of [cite/text:@waterman]. He introduces two additional matrices
$P(i,j)$ and $Q(i,j)$ that contain the minimal cost to get to $(i,j)$ where the
last step is required to be an insertion/deletion respectively.
\begin{align}
    D(i, 0) &= P(i, 0) = I(i, 0) := 0 \\
    D(0, j) &= P(0, j) = I(0, j) := 0 \\
    P(i, j) &:= \min\big(D(i-1, j) + o+e, &&\text{new gap}\\
    &\phantom{:= \min\big(}\ P(i-1, j) + e\big)&&\text{extend gap}\\
    Q(i, j) &:= \min\big(D(i, j-1) + o+e, &&\text{new gap}\\
    &\phantom{:= \min\big(}\ Q(i, j-1) + e\big)&&\text{extend gap}\\
    D(i, j) &:= \min\big(D(i-1, j-1) + \delta(a_{i-1}, b_{j-1}),\, P(i, j),\, Q(i, j)\big).
\end{align}
This algorithm run in $O(nm)$ time.

Gotoh mentions that this method can be modified to also solve the local
alignment of [cite/text:@sw] in quadratic time.

** Minimizing vs. maximizing duality

While the DP formulas for minimizing cost and maximizing score are very
similar, there are some interesting conceptual differences.

When maximizing the score, this is a conceptually similar to computing the LCS: each pair of matching
characters increases the score. [cite/text:@nw] gives an example of this.
As we will see, these algorithms usually consider all pairs of matching
characters between $A$ and $B$.

Algorithms that minimize the cost instead look at the problem as finding the shortest
path in a graph, usually with non-negative weights and cost $0$ for matching characters.
The structure of the corresponding DP matrix turns out to be more complex, but
can also be exploited for algorithms faster than $O(nm)$.

Maximizing score is typically used for local alignment, since it needs an
explicit bonus for each matches character. Most modern aligners are based on
finding the shortest path, and hence minimize cost.

*LCS:*
For the problem of LCS in particular there is a duality. When $p$ is the
length of the LCS, and $s$ is the cost of aligning the two sequences via
the LCS cost model where indels cost $1$ and mismatches are not allowed, we have
\begin{align}
    2\cdot p + s = n+m.
\end{align}

*Parameter correspondence:*
More generally for global alignment, [cite/text:@wfalm] show that there is a
direct correspondence between parameters for maximizing score and minimizing cost,
under the assumption that each type of operation has a fixed cost[fn:: Is
it possible to extend this to arbitrary mismatch costs $\delta(a,b)$? Probably
not since the proof relies on the LCS duality.].
In the affine scoring model, let $\delta(a, a) = l_p$, $\delta(a,b) = x_p$,
and $w_k = o_c + e_c \cdot k$. Then the maximal score satisfies
\begin{align}
    p = l_p \cdot L - x_p \cdot X - o_p \cdot O - e_c \cdot E,
\end{align}
where $L$ is the number of matches in the optimal alignment, $X$ the number of
mismatches, $O$ the number of gaps, and $E$ the total length of the gaps.
From this, they derive an equivalent cost model for minimizing scores:
\begin{align}
    l_s &= 0\\
    x_s &= 2l_p + 2x_p\\
    o_s &= 2o_p\\
    e_s &= 2e_p + l_p.
\end{align}
Using that $2L+2X+E=M+N$, this results in
\begin{align}
    s &= 0\cdot  L + x_s  \cdot X + o_s \cdot O+e_s \cdot E\\
    &= (2l_p-2l_p) L+ (2l_p+2x_p) X + 2o_p  O + (2e_p+l_p) E\\
    &= l_p(2L+2X+E) - 2(l_p L - x_p X - o_p  O - e_p E)\\
    &= l_p\cdot (N+M) - 2p.
\end{align}
This shows that any global alignment maximizing $p$ at the same time minimizes $s$ and
vice versa.

** Four Russians method
:PROPERTIES:
:CUSTOM_ID: four-russians
:END:

#+caption: In the four Russians method, the $n\times m$ grid is divided into blocks of size $r\times r$.
#+caption: For each block, differences between DP table cells along the top row $R$ and left column $S$ are the /input/, together with the corresponding substrings of $A$ and $B$.
#+caption: The /output/ are the differences along the bottom row $R'$ and right column $S'$.
#+caption: For each possible input of a block, the corresponding /output/ is precomputed, so that the DP table can be filled by using lookups only.
#+caption: Red shaded states are not visited.
#+caption: (CC0; [[https://github.com/RagnarGrootKoerkamp/research/blob/master/posts/pairwise-alignment/drawings/four-russians.drawio.svg][source]])
#+name: fig:four-russians
#+attr_html: :class large
[[file:drawings/four-russians.drawio.svg]]

The so called /four Russians method/ ([[https://en.wikipedia.org/wiki/Method_of_Four_Russians][wikipedia]]) was introduced by
[cite/text/f:@four-russians], and who all
worked in Moscow at the time of publication. It is a general method to speed up
DP algorithm from $n^2$ to $n^2 / \lg n$ provided that entries are integers and
all operations are 'local'.

[cite/text/c:@four-russians-ed] apply this idea to pairwise alignment, resulting
in the first subquadratic algorithm for edit distance. They partition
the $n\times m$ matrix in blocks of size $r\times r$, for some $r=\log_k n$, as
shown in figure [[fig:four-russians]]. Consider the differences $R_i$ and $S_i$ between
adjacent DP table cells along the top row ($R_i$) and left column ($S_i$) of
the block. The core observation is that the differences $R'_i$ and $S'_i$ along
the bottom row and right column of the block only depend on $R_i$, $S_i$, and
the substrings $a_i\cdots a_{i+r}$ and $b_j\cdots b_{j+r}$. This means that for
some value of $k$, $r=\log_k n$ is small enough so that we can precompute the
values of $R'$ and $S'$ for all possibilities of $(R, S, a_i\cdots a_{i+r},
b_j\cdots b_{j+r})$ in $O(n^2/r)$ time.

Note that $k$ depends on the size of the alphabet, $|\Sigma|$. In practice this
needs to be quite small.

Using this precomputation, the DP can be sped up by doing a single $O(r)$
lookup for each of the $O(n^2/r^2)$ blocks, for a total runtime of $O(n^2/\lg
n)$.

[cite/text/c:@wu96] present a practical implementation of the four Russians
method for approximate string matching. They suggest a block size of $1\times
r$, for $r=5$ or $r=6$, and provide efficient ways of transitioning from one
block to the next.

Nowadays, the bit-parallel technique (e.g. [cite/text:@myers99]) seems to have
replaced four Russians, since it can compute up to 64 cells in a single step,
while not having to wait for (comparatively) slow lookups of the precomputed data.

** TODO $O(ns)$ methods
TODO: Diagonal transition only works for fixed indel cost (but may have variable
mismatch cost)
*** TODO Exponential search on band
:PROPERTIES:
:CUSTOM_ID: exponential-band
:END:
*** TODO LCS: thresholds, $k$-candidates and contours
:PROPERTIES:
:CUSTOM_ID: thresholds
:END:
  #+caption: Contours as shown in [cite/text:@hirschberg77]
  #+name: contours
  [[file:screenshots/contours.png]]
- [cite/text/cf:@hunt77] [[[https://en.wikipedia.org/wiki/Hunt%E2%80%93Szymanski_algorithm][wikipedia]]] :: An $O((r+n) \lg n)$ algorithm for LCS, for $r$ ordered pairs
  of positions where the two sequences match, using an array of /threshold
  values/ $T_{i,k}$: the smallest $j$ such that the prefixes of length $i$ and
  $j$ have an LCS of length $k$. Faster than quadratic for large alphabets (e.g.
  lines of code).
- [cite/text/cf:@hirschberg77] :: Defines /$k$-candidates/ (already introduced in Hirschberg's
  thesis two years before) as matches where a LCS of length $k$ ends. /Minimal/
  (also called /essential/ elsewhere) $k$-candidates are those for which there
  are no other /smaller/ $k$-candidates.  This leads to /contours/: the border
  between regions of equal $L$-value, and an $O(pn+n\lg n)$ algorithm.  His $O(p
  (m-p) \lg n)$ algorithm is based on using a band of width $m-p$ when the LCS
  has length at least $p$.

*** TODO Diagonal transition: furthest reaching and wavefronts
:PROPERTIES:
:CUSTOM_ID: diagonal-transition
:END:



- Ukkonen [cite/text/cf:@ukkonen83 conference;@ukkonen85 paper] ::
  Introduces the diagonal transition method for edit costs, using $O(s\cdot
  \min(m,n))$ time and $O(s^2)$ space, and if only the score is needed, $O(s)$
  space.

  Concepts introduced:
  * $D(i,j)$ is non-decreasing on diagonals, and has bounded increments.
  * *Furthest reaching point*: Instead of storing $D$, we can store increments
    only: $f_{kp}$ is the largest $i$ s.t. $D(i,j)=p$ on diagonal $k$ ($j-i=k$).
    [TODO: they only generalize it from LCS elsewhere]
  * A recursion on $f_{kp}$ for unit costs, computing /wavefront/ $f_{\bullet,p}$ from
    the previous front $f_{\bullet, p-1}$, by first taking a maximum over
    insert/deletion/substitution options, and then increasing $f$ as long as
    characters on the diagonal are matching.

    Only $O(s^2)$ values of $f$ are computed, and if the alignment is not
    needed, only the last /front/ $f_{\bullet, p}$ is needed at each step.
  * *Gap heuristic*: The distance from $d_{ij}$ to the end $d_{nm}$ is at least
    $|(i-n)-(j-m)|\cdot \Delta$ when $\Delta$ is the cost of an indel.
    This allows pruning of some diagonals.

  Additionally, this paper introduces an algorithm that does exponential search
  on the band with, leading to an $O(ns)$ algorithm for general costs but using
  $O(ns)$ space.

  Mentions $O(n+s^2)$ best case and that $O(ns)$ is a pessimistic worst case,
  but no expected case.
- [cite/text/cf:@myers86], submitted '85 ::
  Independent of [cite/text:@ukkonen85], this
  introduces the concept of furthest reaching point and the
  recursion, but for LCS. Dijkstra's algorithm is used to evaluate DP states in
  order of increasing distance. $O(ns)$. For random strings, they show it runs in
  $O(n+s^2)$ expected time.
  #+caption: Furthest reaching points for LCS by [cite/text:@myers86].
  #+name: furthest-reaching
  [[file:screenshots/furthest-reaching.png]]

  Uses divide-and-conquer to achieve $O(n)$ space; see below.
- [cite/text/cf:@landau-vishkin89], submitted '86 :: Extends [cite/text:@ukkonen85]
  to $k$-approximate string matching, the problem of finding /all/ matches of a
  pattern in a text with at most $k$ errors, in
  $O(nm)$ time. They improve this to $O(nk)$ by using a suffix tree with LCA
  queries to extend matching diagonals in $O(1)$ instead of checking one
  character at a time.
  #+caption: Example of [cite/text:@landau-vishkin89]. Note that values increase along diagonals.
  #+name: lv-example
  [[file:screenshots/lv-example.png]]

  #+caption: Furthest reaching points for the above example.
  #+name: lv-furthest-reaching
  [[file:screenshots/lv-furthest-reaching.png]]
** TODO Suffixtree for $O(n+s^2)$ expected runtime
:PROPERTIES:
:CUSTOM_ID: ns2
:END:

** Using less memory
*** Computing the score in linear space
[cite/text/cf:@gotoh] was the first to remark that if only the final alignment
score is needed, and not the alignment itself, linear memory is often sufficient.
Both the quadratic algorithms presented above can use this technique.
Since each column $D(i, \cdot)$ of the matrix $D$ (and $P$ and $Q$) only depends on
the previous column $D(i-1, \cdot)$ (and $P(i-1, \cdot)$ and $Q(i-1, \cdot)$),
it suffices to only keep those in memory while computing column $i$.

*** Divide-and-conquer
:PROPERTIES:
:CUSTOM_ID: divide-and-conquer
:END:
#+caption: Divide-and-conquer as shown in [cite/text/cf:@myers88].
#+caption: Unlike the text here, they choose i* to be the middle row instead of the middle column.
#+name: myers88
[[file:screenshots/myers88.png]]

[cite/text/cf:@hirschberg75] introduces a divide-and-conquer algorithm to
compute the LCS of two sequences in linear space.  This technique was applied
multiple times to reduce the space complexity of other algorithms as well:
[cite/text:@myers86] applies it to their $O(ns)$ LCS algorithm,
[cite/text:@myers88] reduces the $O(nm)$ algorithm by [cite/text:@gotoh] to
linear memory, and biWFA [unpublished] improves WFA.

*Method:*
Instead of computing the alignment from
$(0,0)$ to $(n,m)$, we fix $i^\star = \lfloor n/2\rfloor$ and split the problem
into two halves: We compute the /forward/ DP matrix $D(i, j)$ for all $i\leq
i^\star$, and introduce a /backward/ DP $D'(i, j)$ that is computed for all
$i\geq i^\star$. Here, $D'(i,j)$ is the minimal cost for aligning suffixes
of length $n-i$ and $m-j$ of $A$ and $B$.  A theorem of Hirschberg shows that
there must exist a $j^\star$ such that $D(i^\star, j^\star) + D'(i^\star,
j^\star) = D(n, m)$, and we can find $j^\star$ as the $j$ that minimizes
$D(i^\star, j) + D'(i^\star, j)$.

This means that the point $(i^\star, j^\star)$ is part of the optimal alignment.
The two resulting subproblems of aligning $A[0, i^\star]$ to $B[0, j^\star]$ and
$A[i^\star, n]$ to $B[j^\star, m]$ can now be solved recursively using the same
technique, where again we find the midpoint of the alignment. This recursive
process is shown in figure [[myers88]].
The recursion stops as soon as the alignment problem becomes trivial.

*Space complexity:*
At each step we can use the linear-space variant described
above to compute $D(i^\star, j)$ and $D'(i^\star, j)$ for all $j$.  Since we only do one step at a
time and the alignment itself (all the pairs $(i^\star, j^\star)$) only takes
linear space as well, the overall space needed is linear.

*Time complexity:*
This closely follows [cite/text:@myers88].
The time taken in the body of each step (excluding the recursive calls) is
bounded by $C\cdot mn$ for some constant $C$. From figure [[myers88]] it can be seen
that the total time spent in the two sub-problems is $\frac 12 C\cdot mn$, as
the corresponding shaded area is half the of the total area. The four
sub-sub-problems again take half of that time, and a quarter of the total time,
$\frac 14 C\cdot mn$. Summing over all layers, we get a total run time
bounded by
\begin{equation}
C\cdot mn + \frac 12 C\cdot mn + \frac 14 C\cdot mn + \frac 18C\cdot mn + \dots \leq 2C\cdot mn = O(mn).
\end{equation}
In practice, this algorithm indeed takes around twice as long to find an
alignment as the non-recursive algorithm takes to find just the score.

** TODO LCSk[++] algorithms

- [cite:@lcsk] introduces LCSk: LCS where all matches come $k$ characters at a
  time. Presents a quadratic NW-like algorithm.
- [cite:@lcsk++] introduces LCSk++, and an algorithm for it with expected time
  $O(n + r \lg r)$, where $r$ is the number of $k$-match pairs, similar to
  [cite:@hunt77].
- [cite:@lcsk-overview] Overview of LCS algorithms -- nothing new here.
- [cite:@lcsk-fast] more incremental improvements -- not so interesting

** Theoretical lower bound
[cite/text/cf:@no-subquadratic-ed] show that Levenshtein distance can not be solved in
time $O(n^{2-\delta})$ for any $\delta > 0$, on the condition that the /Strong
Exponential Time Hypothesis/ (SETH) is true.
Note that the $n^2/\lg n$ runtime of the four Russians method is not
$O(n^{2-\delta})$ for any $\delta>0$, and hence does not contradict this.

They use a reduction from the /Orthogonal Vectors Problem/ (OVP): given two sets
$A, B\subseteq \{0,1\}^d$ with $|A|=|B|=n$, determine whether there exists $x\in
A$ and $y\in B$ such that $x\cdot y=\sum_{j=1}^d x_j y_j$ equals $0$. Their
reduction involved constructing a string (/gadget/) $VG'_1(a)$ for each $a\in A$
and $VG'_2(b)$ for each $b\in B$, such that $EDIT(VG'_1(a), VG'_2(b))$ equals $C_0$
if $a\cdot b=0$ and equals $C_1>C_0$ otherwise. Then they construct strings
\begin{align}
    P_1 &= VG'_1(a_1) \cdots VG'_1(a_n)\\
    P_2 &= \big[VG'_2(f)\big]^{n-1} \cdot \big[VG'_2(b_1) \cdots VG'_2(b_n)\big] \cdot \big[VG'_2(f)\big]^{n-1}
\end{align}
for some fixed element $f$, and conclude that the cost of a
semi-global alignment of $P_1$ to $P_2$ is some constant $X$ if $a\cdot b=0$ is
not possible, and at most $X-2$ otherwise.

If edit distance can be computed in strongly subquadratic time, then so can
semi-global alignment. Using the reduction above that would imply a subquadratic
solution for OVP, contradicting SETH.


** TODO A note on DP (toposort) vs Dijkstra vs A*
TODO: Who uses/introduces gap heuristic?

TODO: ukkonen'85 (first?) states the link between DP and shortest path (in edit graph)

TODO: Include Fickett 84 paper for O(ns) variant of dijkstra

TODO: https://link.springer.com/article/10.1186/1471-2105-10-S1-S10

---
* TODO Tools
Note: From 1990 to 2010 there is a gap without much theoretical progress on
exact alignment.
During this time, speedups were achieved by [TODO: citations]:
- more efficient implementations on available hardware;
- heuristic approaches such as banded alignment and $x$-drop.

There are many implementations of exact and inexact aligners. Here I will only
list current competitive aligners.

[TODO: This is very incomplete for now]

- Greedy matching :: todo
- Myers bit-parallel algorithm :: todo
- SeqAN :: $O(nm)$ NW implementation, or $O(nm/w)$ using bit-parallel
  [cite:@myers99] for unit cost edit distance: [[https://docs.seqan.de/seqan/3.0.3/group__pairwise__alignment.html#gab6ff083328a700c26c90fea870d63491][docs]]
- Parasail :: todo
- KSW2 :: todo
- Edlib :: diagonal transition [cite:@ukkonen85] and bit-parallel [cite:@myers99]
- WFA :: exact, diagonal transition method

  States the recurrence for gap-affine costs for the diagonal transition
  algorithm, and provides a fast implementation. It is unclear to me why it took
  30+ years to merge the existing gap-affine recursion and more efficient
  diagonal-transition method.
- WFA2 :: Extends WFA to more cost models, more alignment types, and introduces
  low-memory variants
- WFALM :: *L*ow *M*emory variant of WFA.

  Uses a square-root decomposition to do backtracking in $O(s^{3/2})$

  *Additional speedup:*
  The extension/greedy matching can be done using a precomputed suffixtree and LCA queries.
  This results in $O(n+m+s^2)$ complexity but is not faster in practice.
  [TODO: original place that does this]
- biWFA [WIP, unpublished] :: Meet-in-the-middle/divide-and-conquer variant of WFA, applying the ideas in
  [cite/text:@hirschberg75] to WFA to reconstruct the alignment in linear space.
- lh3/lv89 :: Similar to biWFA (but non-recursive) and WFALM (but with a fixed
  edit-distance between checkpoints, instead of dynamically storing every
  $2^{i}$ /th/ wavefront).


---
* TODO Notes for other posts

** Semi-global alignment papers
- [cite:@landau-vishkin89]
- [cite:@myers99]
- [cite:@chang92]: shows that ukkonens idea (Finding approximate patterns in
  strings, also '85) runs in $O(nk)$
  expected time for $k$-approximate string matching, when the reference is a
  random string.
- [cite:@wu96]: Efficient four russians in combination with 'ukkonens zone'
  $O(kn/\lg s)$ when $O(s)$ space is available for lookup.
- Baeza-Yates Gonnet 92
- Baeza-Yates Navarro 96
- https://www.biorxiv.org/content/10.1101/133157v3

** Approximate pairwise aligners

- Block aligner

** Old vs new papers

There's a big dichotomy between the old and the new papers:

- old
  - short intro
  - to the point
  - little context; more theory
  - short about utility: Gotoh has literally 1 sentence on this: 'can be executed on a small pc with limited memory'
  - Examples: [cite:@sw], the original four russians paper

- new
  - at least 1 A4 of blahblah
  - needs to talk about other tools, types of data available (length and error rate of pacbio...)
  - spends 3 pages on speed compared to others


---
#+print_bibliography:
