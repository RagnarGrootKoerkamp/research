#+title: [WIP]  Binary search variants and the effects of batching
#+filetags: @walkthrough binary-search wip
#+OPTIONS: ^:{} num:t
#+hugo_front_matter_key_replace: author>authors
#+toc: headlines 3
#+hugo_level_offset: 1
#+author: František Kmječ
#+author: Ragnar Groot Koerkamp
#+date: <2025-02-12 Wed>

* Optimizing Binary Search And Interpolation Search
:PROPERTIES:
:CUSTOM_ID: optimizing-binary-search-and-interpolation-search
:END:
This blogpost is a preliminary of the
[[https://curiouscoding.nl/posts/static-search-tree/][post on static
search trees]]. We will be looking into binary search and how it can be
optimized using different memory layouts (Eytzinger), branchless
techniques and careful use of prefetching. In addition, we will explore
batching of these schemes. Our language of choice will be Rust.

The goal of this text is mainly educational, as we'll mostly be
replicating research that has already been done. Looking at performance plots is fun!

The source code can be found at [[https://github.com/RagnarGrootKoerkamp/suffix-array-searching]].

** Problem statement
:PROPERTIES:
:CUSTOM_ID: problem-statement
:END:
*Input*: we take a sorted array of /n/ unsigned 32-bit integers.

*Output*: for a query q, find the smallest element ~>= q~ or =u32::MAX=
if no such element exists.

*Metric*: we optimize throughput. In plots, we usually report amortized
time per query, i.e. not queries/time, but time/query.

Optimizing for throughput is relevant because for many use-cases,
e.g. in bioinformatics, the queries are often independent and the
latency of one is not critical. It also allows for the usage of
batching, which helps us to hide memory latency.

To easily check the implementations against each other,
we always have the largest possible value in the input array, so that the result always exists.

** Inspiration and background
This article is inspired by a post in
[[https://en.algorithmica.org/hpc/data-structures/binary-search/][Algorithmica
on binary search]]. We start by reimplementing its experiments and
expand upon it by not optimizing for latency, but throughput. We also
took inspiration from the notable paper
[[https://arxiv.org/pdf/1509.05053][Array Layouts For Comparison-Based
Searching]] by Paul-Virak Khuong and Pat Morin, which goes in-depth on
more-or-less successful array layouts.

The paper is great because it goes step by step and introduces nice models
for why different schemes behave like they behave. It is worth a read! It changed
my mind on how a good scientific article should look.

** Benchmarking setup
The benchmarking was done using =rustc 1.85.0-nightly=. The CPU is an AMD
Ryzen 4750U, and the DRAM is DDR4-2666. For reproducibility, clock speed
was locked at 1.7GHz and turbo boost was disabled.

This post will be built around plenty of performance plots. These all show inverse throughput,
e.g. average time per one query. Therefore, in all plots, lower is better. We will benchmark
in arrays up to a size of 1GB, well beyond the size of L3 cache, to see effects of going to main memory.

All measurements are done 5 times. The lines in plots follow the median, and we
show the spread of the 2nd to 4th value (i.e., after discarding the
minimum and maximum). Note that the results are often very deterministic and the spread is therefore not very visible.
The measurements are done at \(1.17^k\) so as to be similar to
Algorithmica. This also mitigates some artefacts of limited cache set
associativity which we describe in a later section.

We also allocate the datastructures on 2MB hugepages. This is done so as to reduce
pressure on the TLB.

One thing that I am sorry about are the colors in the plots. In some plots, there will be two lines with the same color;
usually it is in the noncritical ones. I made it so that algorithms would keep their color across plots, and that
means that sometimes, this happened. It only happens in the non-critical plots though.

* Binary search
We start with a textbook binary search implementation,
and we see if we can replicate the results from Algorithmica. Here's the
code:

#+begin_src rust
pub fn binary_search(&self, q: u32) -> u32 {
    let mut l = 0;
    let mut r = self.vals.len();
    while l < r {
        let m = (l + r) / 2;
        if self.get(m) < q {
            l = m + 1;
        } else {
            r = m;
        }
    }
    self.get(l)
}
#+end_src

Here, =self= is a =SortedVec= class, which is a wrapper that allocates the sorted =vals= array
on hugepages.

We note that by default, Rust wraps all accesses to vectors with bounds
checks. Since we are comparing against C++ implementations and there is
no risk of accessing unallocated memory, we implement the =get=
function, which performs a Rust unsafe =get_unchecked= to get rid of the
bounds checks. This will also be the case for all the further
experiments.

So, how does it perform? Let's compare to the Rust standard library
version (denoted here by =binary_search_std=):

#+attr_html: :class inset
[[file:plots/binsearch-std-vs-binsearch.svg]]

We can see that our naive version is slower, except for large array sizes. Checking out the
standard library source code, we can see that their implementation already
has some optimizations in place. In the =binary_search_by= function, we
see the following:

#+begin_src rust
#[stable(feature = "rust1", since = "1.0.0")]
pub fn binary_search(&self, x: &T) -> Result<usize, usize>
where
    T: Ord,
{
    self.binary_search_by(|p| p.cmp(x))
}

pub fn binary_search_by<'a, F>(&'a self, mut f: F) -> Result<usize, usize>
where
    F: FnMut(&'a T) -> Ordering,
{
    let mut base = 0usize;

    // This loop intentionally doesn't have an early exit if the comparison
    // returns Equal. We want the number of loop iterations to depend *only*
    // on the size of the input slice so that the CPU can reliably predict
    // the loop count.
    while size > 1 {
        let half = size / 2;
        let mid = base + half;

        let cmp = f(unsafe { self.get_unchecked(mid) });

        // Binary search interacts poorly with branch prediction, so force
        // the compiler to use conditional moves if supported by the target
        // architecture.
        base = select_unpredictable(cmp == Greater, base, mid);

        // This is imprecise in the case where `size` is odd and the
        // comparison returns Greater: the mid element still gets included
        // by `size` even though it's known to be larger than the element
        // being searched for.
        //
        // This is fine though: we gain more performance by keeping the
        // loop iteration count invariant (and thus predictable) than we
        // lose from considering one additional element.
        size -= half;
    }

    let cmp = f(unsafe { self.get_unchecked(base) });
    if cmp == Equal {
        unsafe { hint::assert_unchecked(base < self.len()) };
        Ok(base)
    } else {
        let result = base + (cmp == Less) as usize;
        unsafe { hint::assert_unchecked(result <= self.len()) };
        Err(result)
    }
}
#+end_src

So they do a constant number of iterations instead of early stopping
when the value is found and they try to use the =cmov= instruction if it
is available in the [[https://doc.rust-lang.org/std/intrinsics/fn.select_unpredictable.html][=select_unpredictable=]] function. Both these optimizations are done so that the branch
predictor has an easier time (as mispredictions are expensive). The =cmov=
instruction is useful when the result of the comparison can't be
reliably predicted (which here it really can't).[fn::Here's [[https://yarchive.net/comp/linux/cmov.html][Linus talking about it]]] They are also both
recommended by the Algorithmica post, and make the code effectively branchless.

In addition, they do their accesses to the array without bounds checks, in the same way we do.

It now makes sense that our naive version is faster on large array sizes.
Algorithmica explains this by the fact that with =cmov=, the branch predictor can't
start to speculatively prefetch data from main memory (as there is no branch). The =cmov=-optimized version
therefore suffers more memory latency, as it can't be hidden by prefetching.

Note that originally, I intended to write here that I did not see this effect of missing speculation and prefetching.
I then found out that when testing on arrays of power-of-two size can give skewed results; but more on
that soon when we talk about batching.

** Branchless search
Now let us implement these branchless optimizations as well and see how we do then.
Here's the code:

#+begin_src rust
pub fn binary_search_branchless(&self, q: u32) -> u32 {
    let mut base: u64 = 0;
    let mut len: u64 = self.vals.len() as u64;
    while len > 1 {
        let half = len / 2;
        let cmp = self.get((base + half - 1) as usize) < q;
        base = select_unpredictable(cmp, base + half, base);
        len = len - half;
    }
    self.get(base as usize)
}
#+end_src

When first implementing this, me, being a Rust newbie, immediately went
for the =cmov= crate, as I was unable to make the compiler generate
the =cmov= on its own just with an if expression. Trying this, I found
out that it is still plenty slower than the =select_unpredictable=
function that is used in =binary_search_by=, so I followed the approach
of the standard library.

#+attr_html: :class inset
[[file:plots/binsearch-std-vs-branchless.svg]]

We can see that we now match the performance of the library version, even surpassing it. We
assume this is due to our function being specialized and not having an
error condition at the end, leading to it being better by a small
fraction. On large sizes, we're still worse off than the naive version, though.

Since we do a fixed number of iterations, we can clearly see "stairs" in the graph with every
power of two, as the runtime is now pretty much a deterministic function of the input size.

** Explicit prefetching
To speed the search up for large array sizes, the Algorithmica post recommends explicit prefetching.
This negates the CPU's inability to prefetch when we use the =cmov= instruction, and trades a bit
of added memory traffic for more performance.
We use the following construction:

#+begin_src rust
pub fn prefetch_index<T>(s: &[T], index: usize) {
    let ptr = unsafe { s.as_ptr().add(index) as *const u64 };
    prefetch_ptr(ptr);
}
#+end_src

And using this function, we explicitly prefetch both the locations where
the binary search could lead us in a given iteration:

#+begin_src rust
prefetch_index(&self.vals, (base + half / 2 - 1) as usize);
prefetch_index(&self.vals, (base + half + half / 2 - 1) as usize);
#+end_src

#+attr_html: :class inset
[[file:plots/binsearch-std-vs-branchless-prefetch.svg]]

The prefetching does its part, giving us a nice small ~10-15% speedup.
You see there is a small slowdown at small sizes, which is to be expected.

** Batching
So far we have been replicating the work Algorithmica has done. Now it is time to use the fact that we only care about throughput,
and talk about batching.

In this context, batching is just what it sounds like: we will take
several requests at once, and we will handle them concurrently within a
single function. In every loop iteration, we do a comparison for each of
the queries, and we move the =base= index for the query accordingly.

We start with the branchless version with no prefetching:

#+begin_src rust
pub fn batch_impl_binary_search_branchless<const P: usize>(
    &self,
    qb: &[u32; P],
) -> [u32; P] {
    let mut bases = [0u64; P];
    let mut len = self.vals.len() as u64;
    while len > 1 {
        let half = len / 2;
        len = len - half;
        for i in 0..P {
            let cmp = self.get((bases[i] + half - 1) as usize) < qb[i];
            bases[i] = select_unpredictable(cmp, bases[i] + half, bases[i]);
        }
    }

    bases.map(|x| self.get(x as usize))
}
#+end_src

The reason this is advantageous is that it allows us to "amortize" or "hide"
memory latency; while comparing and computing the next relevant address
for the search, we can already query the memory for the next value. Since we don't care
about latency but only throughput, we can do this at essentially no cost! And since
the search is a memory-bottlenecked operation, we can speed it up many times.



#+attr_html: :class inset
[[file:plots/binsearch-branchless-batched.svg]]

When I first thought about this, I figured that explicit prefetching should not be needed.
But in the S+-tree post, Ragnar found that explicitly prefetching memory that
was going to be accessed at the next interval size was also helpful. We
therefore add it as well, just to compare:

#+attr_html: :class inset
[[file:plots/binsearch-branchless-prefetched-batched.svg]]

We compare the two best variants to see their differences:

#+attr_html: :class inset
[[file:plots/binsearch-batched-vs-batched-prefetch.svg]]

We see that the prefetching is helping a bit at large array sizes, so we'll keep it. It does not add too much memory traffic
(the CPU would have to fetch the data anyway) and provides a nice hint when there is time to prefetch ahead. It makes things a bit worse
at small array sizes, but that is to be expected.

** A note on power-of-two array sizes
In the bechmarking setup section, we wrote about not doing the
benchmarks on power-of two-sized arrays. Now is the time to talk about
why. Let us repeat the previous experiment with multiple batch sizes
with arrays of size \(2^k\), \(5/4 \cdot 2^k\) , \(3/2 \cdot 2^k\) and
\(7/4 \cdot 2^k\).

#+attr_html: :class inset
[[file:plots/binsearch-branchless-batched-comparison-pow2.svg]]

Notice the sawtooth pattern on the right side of the plot. We see that when the size of the searched
array is a power of two, the time per query jumps higher. This effect
also gets more pronounced with more batching. Why is this?

After consulting the array layouts paper and the Algorithmica post, we
find that the answer is poor cache utilization. The CPU cache sets have
limited associativity, and when our memory accesses are regularly spaced
(a multiple of cache size apart from each other), they will tend to kick
each other out of the cache, leading to more loading from main memory.
The article
[[http://pvk.ca/Blog/2012/07/30/binary-search-is-a-pathological-case-for-caches/][Binary
Search is a Pathological Case for Caches]] goes more in-depth on this,
if you are interested. I personally was puzzled by this at first and had
to think hard about why the program is faster for batch size of 4 at
large sizes, only to find it actually is not.

* Eytzinger
:PROPERTIES:
:CUSTOM_ID: alternative-memory-layout
:END:
An issue with the standard array layout is that caches are not optimally
exploited. When you think about it, the first few queries in the array are really
far apart from each other, and for each of them, we need to fetch a whole cacheline,
but we only use one element from that cacheline. We can only exploit spatial locality
in the bottom layers of the search. The Eytzinger layout can fix this, while also being
friendly to efficient prefetching.

First, as a personal note: when I first encountered the layout, I had no idea it actually had this
name. It was for a university programming homework and the task was to
code a binary heap. To not have to deal with pointers, the heap layout
was specified by indices in arrays. When at position \(i\), the left
descendant is at position \(2i\) and the right one is at position
\(2i + 1\). I think it is a very common exercise, so maybe you have encountered it in the same way.
An illustration of the layout is shown below:

#+caption: A picture of the Eytzinger layout (taken from Algorithmica)
#+attr_html: :class inset
[[file:plots/eytzinger-layout-picture.png]]

As for how to build the layout from a sorted array, there is a simple recursive algorithm
which is well described in Algorithmica, so we will not waste space here
and will refer the reader there if interested.

So, why should Eytzinger be better?
The whole problem of array searching is very memory bound; it is about how fast can we query memory
and how many levels of the search can we fit into caches so that we don't have to do many main memory requests.
In many ways, a normal sorted array and Eytzinger are similar. Eytzinger is very efficient at caching values
at the top of the tree (one query of 64B can help us go forward by 4 layers in the search) while sorted array
is efficient in the same manner at the bottom of the tree. In addition, Eytzinger will allow us to more efficiently prefetch
all the possible paths up to 4 steps into the future.

Algorithmica finds that in the end, it is the efficient prefetching that leads to good performance. We shall see
that for a batched and a multithreaded implementation, it the better efficiency at the top of the tree is crucial.
When conducting many queries, caching the top is better, because it can be better reused and leads to less main
memory traffic overall. But we shall see that in due time.

** Naive implementation
The API stays the same as for normal binary search; we get a query and
we return the lower bound or =u32::MAX= when the lower bound does not
exist.

Notice that indexing starts from one. This makes the layout a bit easier
to implement, is a bit more pleasant to caches (layers of the tree will be aligned to multiples of cache size), and allows us to easily
handle the case where the lower bound does not exist, as the way we calculate the final index will result in zero.

#+begin_src rust
let mut idx = 1;
while idx < self.vals.len() {
    idx = 2 * idx + (q > self.get(idx)) as usize;
}
idx = search_result_to_index(idx);
self.get(idx)
#+end_src

The first while loop looks through the array, but the index it generates
in the end will be out of bounds. How do we get the index of the lower
bound?

I needed some time to grok this from the Algorithmica post, so I will
write it here in my own words. Essentially, each iteration of the
=while= loop resembles either going to the left or to the right in the
binary tree represented by the layout. By the end of the loop, the index
will resemble our trajectory through the tree in a bitwise format; each
bit will represent whether we went right (1) or left (0) in the tree,
with the most significant bit representing the decision on the top of
the tree.

Now, let's think about how the trajectory finding the lower bound will
look. Either we will not find it, so the trajectory will be all ones,
since =q= was always greater than each element of the array. Then we want
to return the default value, which we have stored at index 0 of the
=self.vals= array.

In the case the lower bound was found, we infer that we compared =q=
against it once in the trajectory, went left and then only went right
afterwards (because it is the smallest value ~>= q~, all values smaller
than it are smaller than q). Therefore, we have to strip all the right
turns (ones) at the end of the trajectory and then one bit.

Putting this together, what we want to do is this (hidden in the function
=search_result_to_index=):

#+begin_src rust
idx >> (idx.trailing_ones() + 1)
#+end_src

Okay, let us see how it performs!

#+attr_html: :class inset
[[file:plots/eytzinger-vs-binsearches.svg]]

Okay, so we see the layout is a bit slower at the smaller sizes and not too great at the large
array sizes. So far, not too good. Notice the bumps; these are not random, they are actually
branch mispredictions in the final loop iteration. We'll fix them later.

** Prefetching
The great thing about Eytzinger is that
prefetching can be super effective. This is due to the fact that if we
are at index \(i\), the next index is going to be at \(2i\) or
\(2i + 1\). That means that if we prefetch, we can actually prefetch
both of the possible options within the same cacheline!

We can make use of this effect up to the effective cache line size. A usual cache line size
is 64 bytes, meaning that the cache line can fit 16 =u32= values.
If we prefetch 4 Eytzinger iterations ahead, e.g. to position \(16i\),
we can get all the possible options at that search level in a single
cache line! So, let's implement this:

#+begin_src rust
/// L: number of levels ahead to prefetch.
pub fn search_prefetch<const L: usize>(&self, q: u32) -> u32 {
    let mut idx = 1;
    while (1 << L) * idx < self.vals.len() {
        idx = 2 * idx + (q > self.get(idx)) as usize;
        prefetch_index(&self.vals, (1 << L) * idx);
    }
    while idx < self.vals.len() {
        idx = 2 * idx + (q > self.get(idx)) as usize;
    }
    idx = search_result_to_index(idx);
    self.get(idx)
}
#+end_src

As for the performance, it gets a lot better at large sizes:

#+attr_html: :class inset
[[file:plots/eytzinger-prefetching.svg]]

And we can see that prefetching 4 iterations ahead is really best,
which makes sense, because we're not really doing more work, we're just utilizing the fetched cachelines better.

** Branchless Eytzinger
Now, we go on to fixing the bumpiness in the Eytzinger graph. This is
caused by branch mispredictions on when to end the loop; if the array
size is close to a power of two, the ending is easy to predict, but
otherwise, it is difficult for the CPU. We proceed as Algorithmica
suggests, doing a fixed number of iterations and then doing one
conditional move if still needed. We also still do prefetching:

#+begin_src rust
pub fn search_branchless_prefetch<const L: usize>(&self, q: u32) -> u32 {
    let mut idx = 1;
    let prefetch_until = self.num_iters as isize - L as isize;
    for _ in 0..prefetch_until {
        let jump_to = (q > self.get(idx)) as usize;
        idx = 2 * idx + jump_to;
        // the extra prefetch is apparently very slow here; why?
        prefetch_index(&self.vals, (1 << L) * idx);
    }

    for _ in prefetch_until..(self.num_iters as isize) {
        let jump_to = (q > self.get(idx)) as usize;
        idx = 2 * idx + jump_to;
    }

    idx = self.get_next_index_branchless(idx, q);
    idx = search_result_to_index(idx);
    self.get(idx)
}
#+end_src

Where the =get_next_index_branchless= uses an explicit =cmov= from the
=cmov= crate. It was surprisingly difficult to get the compiler to
accept this optimization, as =select_unpredictable= did not quite work.

#+attr_html: :class inset
[[file:plots/eytzinger-branchless-prefetching.svg]]

On the performance plot, we see that this helps remove the bumps and also slightly helps
the performance when the array size is big.

** Batched Eytzinger
Now, let us do batching the same way we did with binary search. We will
consider two variants, prefetched and not prefetched. The prefetching shouldn't really be needed; the
batching should properly overlay memory requests anyway. But modern computers
are strange beasts, so we'll try it and we'll see. See the source code below.

*** Non-prefetched

#+begin_src rust
pub fn batch_impl<const P: usize>(&self, qb: &[u32; P]) -> [u32; P] {
    let mut k = [1; P]; // current indices

    for _ in 0..self.num_iters {
        for i in 0..P {
            let jump_to = (self.get(k[i]) < qb[i]) as usize;
            k[i] = 2 * k[i] + jump_to;
        }
    }
    for i in 0..P {
        k[i] = self.get_next_index_branchless(k[i], qb[i]);
        k[i] = search_result_to_index(k[i]);
    }

    k.map(|x| self.get(x))
}
#+end_src

#+attr_html: :class inset
[[file:plots/eytzinger-batched-comparison.svg]]

*** Prefetched

#+begin_src rust
pub fn batch_impl_prefetched<const P: usize, const L: usize>(&self, qb: &[u32; P]) -> [u32; P] {
    let mut k = [1; P]; // current indices
    let prefetch_until = self.num_iters as isize - L as isize;

    for _ in 0..prefetch_until {
        for i in 0..P {
            let jump_to = (self.get(k[i]) < qb[i]) as usize;
            k[i] = 2 * k[i] + jump_to;
            prefetch_index(&self.vals, (1 << L) * k[i]);
        }
    }

    for _ in prefetch_until..(self.num_iters as isize) {
        for i in 0..P {
            let jump_to = (self.get(k[i]) < qb[i]) as usize;
            k[i] = 2 * k[i] + jump_to;
        }
    }

    for i in 0..P {
        k[i] = self.get_next_index_branchless(k[i], qb[i]);
        k[i] = search_result_to_index(k[i]);
    }
    // println!("{:?}", k);
    k.map(|x| self.get(x))
}
#+end_src

#+attr_html: :class inset
[[file:plots/eytzinger-batched-prefetched-comparison.svg]]


We compare the two graphs and compare the two best options, one from prefetched and
non-prefetched:

#+attr_html: :class inset
[[file:plots/eytzinger-best-batching-comparison.svg]]

We see that the prefetched version is a few percent faster on large input sizes.
Therefore, we select it as our best eytzinger version.

* Eytzinger or BinSearch?
Now, to compare batched Eytzinger to batched binary search:

#+attr_html: :class inset
[[file:plots/binsearch-eytzinger-conclusion.svg]]

We see the approaches are very similar, especially at larger array sizes.
If we compare the two layouts, we know that Eytzinger provides better locality at the top of the search
while the normal sorted array layout for binary search provides better locality at the bottom of the search.
Both of these effects are largely offset by batching (because it hides the latency of memory accesses quite well).

To investigate this further, I wrote a small Python script simulating the behaviour of both algorithms with respect to caches.
The setup was a single-layer, fixed-size, direct-mapped cache. What I found
was that when it comes to memory throughput, batched Eytzinger is more advantageous. This is because the more-accessed top levels of the
tree are more efficiently cached and can be reused between queries. This leads to less cache lines fetched from main memory overall
compared to binary search. However, we do not see this effect here, probably because the better efficiency is offset by batching anyway.

* Memory efficiency -- parallel search and comparison to B-trees
Now let us push memory to its limits and compare the layouts when we are allowed to use multiple threads to query.
For this test, I have turned off hyperthreading and locked the CPU to 8 cores.
The first interesting aspect of this is whether prefetching will help now. Let's first look at binary search:

#+attr_html: :class inset
[[file:plots/binsearch-batched-vs-batched-prefetch-multithreaded.svg]]

We see that as the prefetching increases pressure on memory, it is again mostly counterproductive in the multithreaded setting.
In the next plot we see that batching is helpful up to roughly size 32, and then it levels out.

#+attr_html: :class inset
[[file:plots/binsearch-branchless-prefetched-batched-multithreaded.svg]]

We will use batch size 32 as a reference.

As far as Eytzinger goes:

#+attr_html: :class inset
[[file:plots/eytzinger-best-batching-comparison-multithreaded.svg]]
Here we see that prefetching does makes it slightly better. We keep it for the comparison.

#+attr_html: :class inset
[[file:plots/eytzinger-batched-prefetched-comparison-multithreaded.svg]]

Here we see that increasing batch size too much hurts performance on small array sizes, and does not improve
performance much beyond batch size 16. We therefore use batch size 16 as a reference for Eytzinger, as it nicely combines speedup at small and large sizes.
So for the final comparison:

#+attr_html: :class inset
[[file:plots/binsearch-eytzinger-conclusion-multithreaded.svg]]

In the plot, we also include one of the fast S-tree versions to have a direct comparison on my machine. We can see that S-trees have a slower rise to the curve,
but are quite a bit slower at small array sizes. This is suprising because their layout should treat the caches optimally, giving us the best utilization.
A possible reason for this is that they do more operations at smaller sizes (for every 4 levels, Eytzinger/binsearch does 4 operations, but the SIMD variants of S-trees
do 16 comparisons).

We see that Eytzinger and binsearch are almost equal, leading us to believe that their advantages effectively align; Eytzinger is better at the top while binary search is better
at the bottom of the search.

We can also look at a direct comparison of single and multi-threaded variants:

#+attr_html: :class inset
[[file:plots/single-vs-multithreaded.svg]]

Overall, the speedup was roughly 4 (at array size 1GB) when using 8 threads. This clearly indicates that we're memory bound. If we wanted to go for more speed and more cache utilization, we could start the first $\lg(n)/2$ layers with the Eytzinger layout and the bottom $\lg(n)/2$ layers
with a standard sorted array. However, we won't delve into this here, as there is more efficient stuff one can do with S-trees!
Otherwise, if you are curious, the array layouts paper goes quite in-depth on this.

* Interpolation search
In the static search tree post, Ragnar suggested looking at [[https://en.wikipedia.org/wiki/Interpolation_search][interpolation search]] as an option to do less accesses to main memory.
For completeness, we will implement it here as well to check out how it performs.

The idea behind interpolation search based on the fact if data is drawn from a random uniform distribution, then when we sort it
and plot the indices on the x-axis and values on the y-axis, we should roughly get a straight line. Using that, when we have the query,
we can efficiently interpolate ("guess") where values corresponding to the query should be.

When the input data is nicely evenly distributed, the complexity is $O(\lg \lg n)$ iterations, rather than $O(\lg n)$ for binary search.
When the data is not well distributed, the worst case complexity is $O(n)$, which is illustrated by the following example. Imagine we're searching for
2 in the following array of 10000 elements:


#+begin_src rust
1111111111111111111111112 9999
*.  --------------------     *
 *.                          *
  *                          *
   *                         *
                        *    *
#+end_src

Every time we do the interpolation, we suspect that the 2 is on the second position of the array. It is therefore very easy to construct pathological
examples for interpolation search. Even in non-adversarial settings, like with the human genome, we could get into trouble with non-uniform distribution
of input data. But let's try it out anyway and see how it goes.


#+begin_src rust
pub fn interpolation_search(&self, q: u32) -> u32 {
    let mut l: usize = 0;
    let mut r: usize = self.vals.len() - 1;
    let mut l_val: usize = self.get(l).try_into().unwrap();
    let mut r_val: usize = self.get(r).try_into().unwrap();
    let q_val = q.try_into().unwrap();
    if q_val <= l_val {
        return self.get(l);
    }
    assert!(
        r_val.checked_mul(r).is_some(),
        "Too large K causes integer overflow."
    );
    while l < r {
        // The +1 and +2 ensure l<m<r.
        let mut m: usize = l + (r - l) * (q_val - l_val + 1) / (r_val - l_val + 2);
        let low = l + (r - l) / 16;
        let high = l + 15 * (r - l) / 16;
        m = m.clamp(low, high);
        let m_val: usize = self.get(m).try_into().unwrap();
        if m_val < q_val {
            l = m + 1;
            l_val = m_val;
        } else {
            r = m;
            r_val = m_val;
        }
    }
    self.get(l)
}
#+end_src

For the following plots, please notice that compared to the previous section, the scale changed quite drastically, so the results
are quite a bit worse for the algorithm.

#+attr_html: :class inset
[[file:plots/interp-vs-binsearch.svg]]

We see that the performance is mostly terrible, multiple times slower than even standard library binary search, even though it beats it at large array sizes. Looking at =perf= outputs,
we see that the issue is two-fold. Firstly, there is a data hazard on the if condition in each iteration. But secondly, integer division
is just very slow.

We try if batching can hide some of this, as it did before:

#+begin_src rust
    pub fn interp_search_batched<const P: usize>(&self, qs: &[u32; P]) -> [u32; P] {
        let mut ls = [0usize; P];
        let mut rs = [self.vals.len() - 1; P];
        let mut l_vals: [usize; P] = ls.map(|i| self.get(i).try_into().unwrap());
        let mut r_vals: [usize; P] = rs.map(|i| self.get(i).try_into().unwrap());
        let mut retvals = [0u32; P];
        let mut done = [false; P];
        let mut done_count = 0;

        // trick to avoid negative values
        for i in 0..P {
            let q_val: usize = qs[i].try_into().unwrap();
            if q_val <= l_vals[i] {
                retvals[i] = self.get(ls[i]);
                done_count += 1;
                done[i] = true;
            }
        }

        while done_count < P {
            for i in 0..P {
                if done[i] {
                    continue;
                }

                let q_val = qs[i].try_into().unwrap();
                let l = ls[i];
                let r = rs[i];
                let l_val = l_vals[i];
                let r_val = r_vals[i];

                if l >= r {
                    retvals[i] = self.get(l);
                    done_count += 1;
                    done[i] = true;
                    continue;
                }

                let mut m: usize = l + (r - l) * (q_val - l_val + 1) / (r_val - l_val + 2);
                let low = l + (r - l) / 16;
                let high = l + 15 * (r - l) / 16;
                m = m.clamp(low, high);
                let m_val = self.get(m).try_into().unwrap();
                if m_val < q_val {
                    ls[i] = m + 1;
                    l_vals[i] = m_val;
                } else {
                    rs[i] = m;
                    r_vals[i] = m_val;
                }
            }
        }

        retvals
    }
#+end_src

#+attr_html: :class inset
[[file:plots/interp-vs-binsearch-batched.svg]]

The performance improves a bit and is decent for large array sizes, but still nowhere close to the level of performance of previous schemes.
The division is a bottleneck and it is hard to optimize it away. I tried to go around it with SIMD, but there, efficient integer division
instructions don't really exist either, and the performance gains are minimal.[fn::When reimplementing the batched version with SIMD, I burned myself by thinking that the Rust portable SIMD =clamp()= function would do an element-wise clamp. Watch out, [[https://github.com/rust-lang/rust/issues/94682][it doesn't]], at least not at this time.]

An interesting factor for interpolation search is also how it performs well on non-random data. Therefore, we download a part of the human genome
from [[https://s3-us-west-2.amazonaws.com/human-pangenomics/T2T/CHM13/assemblies/analysis_set/chm13v2.0.fa.gz][here]].
and compute 32-bit prefixes of all the suffixes. We then search in a subset of them and measure performance. This should be slower, as the data
is not going to be exactly uniformly distributed.

I tried at first with just taking the first X 16-mers, sorting them and then conducting a search on them. I ended up with a really strange result
where the time per query would at first increase sharply, then fall and then take off again. The reason for this strange result is that the human data is strongly non-uniform. As interpolation search
performs badly with increasing non-uniformity, we can assume that the start of the genome is really, really badly distributed
and the distribution goes back to something resembling a uniform one as we increase the size of the sample we're searching.

I fixed this by not always starting from the beginning, but taking a random starting index in the unsorted array of 16-mers
and taking a continuous segment from it. That way, the results will be quite realistic (it makes sense to search through a continuous segment
of the genome) but we will avoid the skewed start.

#+attr_html: :class inset
[[file:plots/interp-vs-binsearch-batched-human-final.svg]]

We see that the result is noisy, but more as expected. The results are not really too bad; the data seems to be "random enough". But overall, it isn't really enough to make the scheme worthwhile against the other ones.
We can see the comparison to Eytzinger in the plot.
For completeness we also show the graph for the multithreaded case:

#+attr_html: :class inset
[[file:plots/interp-vs-binsearch-batched-human-final-multithreaded.svg]]

Overall, I did not see this as a priority and did not spend too much time at optimizing it, as it seems like a bit of a dead end. I would appreciate
ideas; if you have them, please let me know.

* Conclusion and takeaways
Overall, we found that the conclusions from the Algorithmica article and from the array layouts paper mostly hold even for batched settings. Eytzinger
is the best choice for a simple algorithm that is also very fast. It beats standard binary search due to its better cache use characteristics
and ease of prefetching. The other major takeaway is of course that batching is essentially free performance and if you can, you should always do it.

For interpolation search, I did not believe the scheme to be too worthwhile; it is difficult to optimize and relies on the characteristics of the data
for performance. Given there are schemes like Eytzinger or S-trees that are well suited for modern hardware optimizations, I think you should mostly
use those even though the asymptotics are worse.

When writing this, I was suprised to see that the Rust standard library has some optimizations for binary search already implemented, but not all that are recommended
by our sources, namely, prefetching is missing. This is suprising, because prefetching arguably does not cost almost anything. Is it due to unavailability of prefetch instructions
on some platforms?

Anyway, it was a lot of fun to go a bit into the world of performance engineering. Thanks to Ragnar for the idea & the opportunity!
